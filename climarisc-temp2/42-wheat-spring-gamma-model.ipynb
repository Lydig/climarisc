{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b76663",
   "metadata": {},
   "source": [
    "# EDA for wheat_spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f666b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import scipy.stats as stats\n",
    "from patsy import dmatrices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeaa1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis - Full Correlation Matrix\n",
    "\n",
    "print(\"--- EDA: Correlation Analysis of All Monthly Stressors ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# As confirmed, this file is already specific to wheat_spring and its growing season.\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_spring = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Select Only the Monthly Stressor Variables ---\n",
    "    # We will select all columns that have a month name in them, which is a robust\n",
    "    # way to grab all the monthly predictors we want to investigate.\n",
    "    monthly_stressors = [col for col in df_wheat_spring.columns if '_' in col and 'yield' not in col]\n",
    "    df_corr = df_wheat_spring[monthly_stressors]\n",
    "    \n",
    "    print(f\"\\nSelected {len(df_corr.columns)} monthly stressor variables for correlation analysis.\")\n",
    "\n",
    "    # --- 3. Calculate and Print the Correlation Matrix ---\n",
    "    correlation_matrix = df_corr.corr()\n",
    "    \n",
    "    # Optional: If you want to see the full numerical matrix, uncomment the next line\n",
    "    # print(\"\\n--- Full Pairwise Correlation Matrix ---\")\n",
    "    # print(correlation_matrix)\n",
    "\n",
    "    # --- 4. Visualize the Matrix with a Heatmap ---\n",
    "    # A heatmap is the best way to see the broad patterns of collinearity.\n",
    "    print(\"\\nGenerating correlation heatmap...\")\n",
    "    \n",
    "    plt.figure(figsize=(18, 15))\n",
    "    heatmap = sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        cmap='coolwarm',  # Use a diverging colormap (red=positive, blue=negative)\n",
    "        center=0,         # Center the colormap at zero\n",
    "        vmin=-1,          # Set the color scale limits to the theoretical min/max\n",
    "        vmax=1,\n",
    "        linewidths=.5,\n",
    "        annot=False       # Annotations are turned off as the matrix is too large to be readable\n",
    "    )\n",
    "    \n",
    "    plt.title('Pairwise Correlation Matrix of All Monthly Stressors for wheat_spring', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800897af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Extended EDA for wheat_spring Yield Analysis ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_spring = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- Task 1: Examine the Distribution of the Dependent Variable (yield_wheat_spring) ---\n",
    "    print(\"--- Task 1: Analyzing the distribution of the dependent variable 'yield_wheat_spring' ---\")\n",
    "    \n",
    "    # Create a figure with two subplots side-by-side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Distribution Analysis for yield_wheat_springe', fontsize=16)\n",
    "\n",
    "    # a) Histogram with a Kernel Density Estimate (KDE)\n",
    "    # This helps us visually assess the shape, center, and spread of the yield data.\n",
    "    # We are checking for positive skewness, which is characteristic of data modeled by a Gamma distribution.\n",
    "    sns.histplot(df_wheat_spring['yield_wheat_spring'], kde=True, ax=axes[0])\n",
    "    axes[0].set_title('Histogram of wheat_spring Yield')\n",
    "    axes[0].set_xlabel('Yield (tonnes per hectare)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    # b) Q-Q (Quantile-Quantile) Plot against a theoretical normal distribution\n",
    "    # This plot helps us assess if the data's distribution follows a specific theoretical distribution.\n",
    "    # Deviations from the red line suggest skewness or heavy tails.\n",
    "    # While our target is a Gamma GLM, a Q-Q plot vs. Normal is a standard first step to detect non-normality.\n",
    "    stats.probplot(df_wheat_spring['yield_wheat_spring'], dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot of wheat_spring Yield vs. Normal Distribution')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Distribution plots generated. Check for positive skew, which supports our choice of a Gamma GLM.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 2: Bivariate Scatter Plots of Yield vs. Key Stressors ---\n",
    "    print(\"--- Task 2: Visualizing relationships between yield and key climate stressors ---\")\n",
    "    \n",
    "    # Select a few key stressors based on agronomic theory for wheat_spring\n",
    "    key_stressors = ['temperature_Jun', 'temperature_Jul', 'precipitation_Apr', \"temperature_May\" , \"temperature_Mar\" , \"potential_evaporation_Apr\"]\n",
    "    \n",
    "    # Create a figure to hold the scatter plots\n",
    "    fig, axes = plt.subplots(1, len(key_stressors), figsize=(21, 6))\n",
    "    fig.suptitle('Bivariate Relationships: wheat_spring Yield vs. Key Stressors', fontsize=16)\n",
    "\n",
    "    for i, stressor in enumerate(key_stressors):\n",
    "        # We use a regression plot with a LOWESS (Locally Weighted Scatterplot Smoothing) curve.\n",
    "        # This is a non-parametric way to see the underlying trend without assuming a linear relationship.\n",
    "        # It's excellent for spotting potential non-linearities (like an inverted 'U' shape).\n",
    "        sns.regplot(\n",
    "            x=stressor,\n",
    "            y='yield_wheat_spring',\n",
    "            data=df_wheat_spring,\n",
    "            ax=axes[i],\n",
    "            lowess=True, # Use LOWESS smoother to detect non-linear patterns\n",
    "            scatter_kws={'alpha': 0.1, 'color': 'gray'}, # De-emphasize individual points\n",
    "            line_kws={'color': 'blue'} # Emphasize the trend line\n",
    "        )\n",
    "        axes[i].set_title(f'Yield vs. {stressor}')\n",
    "        axes[i].set_xlabel(f'{stressor}')\n",
    "        axes[i].set_ylabel('Yield (tonnes per hectare)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Scatter plots generated. Look for non-linear patterns that might inform our final model.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 3: Plot Key Variables Over Time ---\n",
    "    print(\"--- Task 3: Examining long-term trends in yield and a key climate variable ---\")\n",
    "    \n",
    "    # Calculate the mean of yield and a key stressor for each year\n",
    "    yearly_data = df_wheat_spring.groupby('year')[['yield_wheat_spring', 'temperature_Jun']].mean().reset_index()\n",
    "\n",
    "    # Create a plot with a primary and secondary y-axis to show both trends together.\n",
    "    # This confirms the necessity of including 'year' as a control variable to capture trends\n",
    "    # likely related to technology, while also checking for climate trends.\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plotting average yield on the primary (left) y-axis\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Average Yield (tonnes per hectare)', color=color)\n",
    "    ax1.plot(yearly_data['year'], yearly_data['yield_wheat_spring'], color=color, marker='o', label='Average Yield')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Create a second y-axis that shares the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plotting average temperature on the secondary (right) y-axis\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Average June Temperature (°C)', color=color)\n",
    "    ax2.plot(yearly_data['year'], yearly_data['temperature_Jun'], color=color, linestyle='--', marker='x', label='Avg. July Temp')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('Long-Term Trend of Spring Wheat Yield and June Temperature (1982-2016)', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    # Adding a single legend for both lines\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"Time-series plot generated. Note the clear upward trend in yield, confirming the need for a 'year' control variable.\\n\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: A required column was not found in the dataset: {e}. Please check the CSV file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44b618",
   "metadata": {},
   "source": [
    "# regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_spring = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Define the Full Model Formula ---\n",
    "    # Programmatically get all monthly stressor column names\n",
    "    monthly_stressors = [col for col in df_wheat_spring.columns if '_' in col and 'yield' not in col]\n",
    "    \n",
    "    # Join them with '+' to create the predictor part of the formula\n",
    "    stressor_formula_part = ' + '.join(monthly_stressors)\n",
    "    \n",
    "    # Construct the complete R-style formula string.\n",
    "    # We include our controls (year, spatial splines) and all potential predictors.\n",
    "    # Note: patsy's bs() function creates the basis spline columns.\n",
    "    formula = f\"yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + {stressor_formula_part}\"\n",
    "    \n",
    "    print(\"\\nGenerated model formula for patsy:\")\n",
    "    print(formula) # Uncomment to see the full, very long formula string\n",
    "\n",
    "    # --- 3. Create the Design Matrix (X) and Response Vector (y) ---\n",
    "    # patsy processes the formula and the dataframe to create the matrices needed for modeling.\n",
    "    # 'y' will be our dependent variable, 'X' will be the full set of predictors.\n",
    "    # The intercept is automatically included in 'X' by patsy.\n",
    "    print(\"\\nCreating design matrix (X) and response vector (y) using patsy...\")\n",
    "    y, X = dmatrices(formula, data=df_wheat_spring, return_type='dataframe')\n",
    "    \n",
    "    print(f\"Successfully created response vector y with shape: {y.shape}\")\n",
    "    print(f\"Successfully created design matrix X with shape: {X.shape}\")\n",
    "    print(f\"The {X.shape[1]} columns in X include the intercept, year, 8 spline bases (4 for lat, 4 for lon), and {len(monthly_stressors)} climate stressors.\")\n",
    "\n",
    "    # --- 4. Standardize the Predictor Matrix (X) ---\n",
    "    # We scale ALL predictors to have a mean of 0 and a standard deviation of 1.\n",
    "    # This ensures the regularization penalty is applied fairly to all variables.\n",
    "    # We do NOT scale the response variable y.\n",
    "    print(\"\\nStandardizing the design matrix X...\")\n",
    "    \n",
    "    # We remove the Intercept column before scaling, as it should not be regularized or scaled.\n",
    "    # We will add it back later if needed, but scikit-learn's models handle it by default.\n",
    "    X_no_intercept = X.drop('Intercept', axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_values = scaler.fit_transform(X_no_intercept)\n",
    "    \n",
    "    # Convert the scaled array back to a pandas DataFrame with the original column names\n",
    "    X_scaled = pd.DataFrame(X_scaled_values, columns=X_no_intercept.columns, index=X.index)\n",
    "    \n",
    "    print(\"Standardization complete.\")\n",
    "    \n",
    "    # Verification: Check the mean and standard deviation of a few scaled columns\n",
    "    print(\"\\n--- Verification of Standardization ---\")\n",
    "    verification_cols = ['year', 'bs(lat, df=4)[0]', 'temperature_Jul']\n",
    "    for col in verification_cols:\n",
    "        mean_val = X_scaled[col].mean()\n",
    "        std_val = X_scaled[col].std()\n",
    "        print(f\"Column '{col}': Mean = {mean_val:.4f}, Std Dev = {std_val:.4f}\")\n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume 'y' and 'X_scaled' are already in memory from the previous step.\n",
    "# If not, you would need to re-run the data preparation script.\n",
    "\n",
    "try:\n",
    "    # --- 1. Define the GLM Model ---\n",
    "    # We specify our model family (Gamma) and the link function (log) as per our project plan.\n",
    "    # We pass the prepared y and the fully scaled X matrix.\n",
    "    # Note: statsmodels requires the intercept to be in the X matrix, which patsy provided.\n",
    "    \n",
    "    # We need to add the intercept back to the scaled data for statsmodels GLM\n",
    "    X_scaled_with_intercept = X.copy() # Start with the original X to preserve intercept and structure\n",
    "    X_scaled_with_intercept[X_no_intercept.columns] = X_scaled # Replace non-intercept columns with scaled versions\n",
    "\n",
    "    gl_gamma = sm.GLM(y, X_scaled_with_intercept, family=sm.families.Gamma(link=sm.families.links.log()))\n",
    "    print(\"Successfully initialized Gamma GLM with a log link.\")\n",
    "\n",
    "    # --- 2. Set up the Regularization Path ---\n",
    "    # We need to test a series of alpha values (penalty strengths).\n",
    "    # A logarithmic scale is best for this, from a weak penalty to a strong one.\n",
    "    n_alphas = 100\n",
    "    alphas = np.logspace(-3, 0.5, n_alphas) # From 0.001 to ~3.16\n",
    "\n",
    "    # The L1_wt parameter controls the Elastic Net mix (0=Ridge, 1=Lasso). \n",
    "    # 0.5 is a balanced choice.\n",
    "    elastic_net_l1_wt = 0.5 \n",
    "    \n",
    "    print(f\"Will fit the model for {n_alphas} alpha values with L1_wt (l1_ratio) = {elastic_net_l1_wt}\")\n",
    "\n",
    "    # --- 3. Fit the Model for Each Alpha and Store Coefficients ---\n",
    "    # We will loop through our alphas and save the coefficients from each model fit.\n",
    "    coefficients = []\n",
    "    \n",
    "    for alpha_val in alphas:\n",
    "        # The fit_regularized method performs the Elastic Net estimation.\n",
    "        # We set refit=False because we want to see the shrunken coefficients for this analysis.\n",
    "        results = gl_gamma.fit_regularized(\n",
    "            method='elastic_net', \n",
    "            alpha=alpha_val, \n",
    "            L1_wt=elastic_net_l1_wt,\n",
    "            refit=False \n",
    "        )\n",
    "        coefficients.append(results.params)\n",
    "    \n",
    "    # Convert the list of coefficient series into a DataFrame for easy plotting\n",
    "    coef_df = pd.DataFrame(coefficients, index=alphas)\n",
    "    coef_df.index.name = \"alpha\"\n",
    "    \n",
    "    # Exclude the Intercept for plotting, as it's not regularized and has a different scale.\n",
    "    coef_df_no_intercept = coef_df.drop('Intercept', axis=1)\n",
    "    \n",
    "    print(\"\\nCompleted fitting models along the regularization path.\")\n",
    "\n",
    "    # --- 4. Visualize the Regularization Path ---\n",
    "    print(\"Generating the regularization path plot...\")\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    ax.plot(coef_df_no_intercept)\n",
    "    ax.set_xscale('log') # The alpha path is best viewed on a log scale\n",
    "    \n",
    "    # Add a vertical line at zero\n",
    "    ax.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    \n",
    "    ax.set_title('Regularization Path for spring Wheat Model Coefficients (Elastic Net)', fontsize=18)\n",
    "    ax.set_xlabel('Alpha (Penalty Strength)', fontsize=14)\n",
    "    ax.set_ylabel('Standardized Coefficient Value', fontsize=14)\n",
    "    \n",
    "    # To avoid a cluttered legend, we don't add one here. The goal is to see the general pattern.\n",
    "    # Alternatively, for fewer variables, a legend could be useful:\n",
    "    # ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"ERROR: Make sure that 'y' and 'X_scaled' DataFrames from the previous step are available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ddb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Corrected code to identify the most robust variables ---\n",
    "# We will inspect the coefficients at a moderately high alpha value\n",
    "# This tells us which variables \"survived\" the penalty the longest.\n",
    "alpha_to_inspect = 0.03\n",
    "\n",
    "try:\n",
    "    # Find the alpha in our index that is closest to our target\n",
    "    # CORRECTED LINE: The operation works directly on the index without .flat\n",
    "    closest_alpha = coef_df.index[np.abs(coef_df.index - alpha_to_inspect).argmin()]\n",
    "\n",
    "    print(f\"--- Coefficients at alpha ≈ {closest_alpha:.4f} ---\")\n",
    "\n",
    "    # Get the coefficients at this alpha and sort them by absolute value\n",
    "    robust_coeffs = coef_df.loc[closest_alpha].copy()\n",
    "    robust_coeffs_sorted = robust_coeffs.abs().sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nVariables sorted by the absolute magnitude of their shrunken coefficient:\")\n",
    "    # We display more variables to get a fuller picture\n",
    "    print(robust_coeffs_sorted.head(15))\n",
    "\n",
    "    # Let's also see their actual values (positive or negative) for the top variables\n",
    "    print(\"\\n--- Actual coefficient values for the most robust variables ---\")\n",
    "    print(coef_df.loc[closest_alpha, robust_coeffs_sorted.index].head(15))\n",
    "\n",
    "except NameError:\n",
    "     print(\"ERROR: Make sure that 'coef_df' DataFrame from the previous step is available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dec827",
   "metadata": {},
   "source": [
    "# modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac93dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Base Champion Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_spring = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the base Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    base_formula = \"yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + temperature_Jul + temperature_May + temperature_Mar + potential_evaporation_Apr + precipitation_Apr\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    base_model = smf.glm(\n",
    "        formula=base_formula,\n",
    "        data=df_wheat_spring,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    base_model_results = base_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(base_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Future Comparison ---\n",
    "    # The AIC is a key metric for comparing different model formulations. Lower is better.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Base Champion Model: {base_model_results.aic:.2f}\")\n",
    "    print(\"This will be our benchmark for comparison.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38304cae",
   "metadata": {},
   "source": [
    "# quadratic terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Quadratic Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_spring = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the base Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    quadratic_formula = \"yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + temperature_Jul + I(temperature_Jul**2) + temperature_May + I(temperature_May**2) + temperature_Mar + potential_evaporation_Apr + precipitation_Apr + I(precipitation_Apr**2)\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    quadratic_model = smf.glm(\n",
    "        formula=quadratic_formula,\n",
    "        data=df_wheat_spring,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    quadratic_model_results = quadratic_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(quadratic_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion's AIC (3516.86).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Quadratic Model: {quadratic_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the Base Model's AIC (3518.87).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d333b",
   "metadata": {},
   "source": [
    "# interractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Quadratic Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_spring = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the base Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    interaction_formula = \"yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + temperature_May:temperature_Jul + temperature_Jul + I(temperature_Jul**2) + temperature_May + I(temperature_May**2) + temperature_Mar + potential_evaporation_Apr + precipitation_Apr + I(precipitation_Apr**2)\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    interaction_model = smf.glm(\n",
    "        formula=interaction_formula,\n",
    "        data=df_wheat_spring,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    interaction_model_results = interaction_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the interaction Champion Model ---\")\n",
    "    print(interaction_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion's AIC (3516.86).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Interaction Model: {interaction_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the quadratic Model's AIC (3481.75).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277612df",
   "metadata": {},
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Generating Final Yield Response Curves for Spring Wheat ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "df_wheat = pd.read_csv(file_path)\n",
    "df_wheat = df_wheat[df_wheat['yield_wheat_spring'] > 0].copy()\n",
    "print(\"Data prepared successfully.\")\n",
    "\n",
    "# --- 2. Fit Our Final Champion Model ---\n",
    "final_champion_formula = \"\"\"\n",
    "    yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                         temperature_Jul + I(temperature_Jul**2) + \n",
    "                         temperature_May + I(temperature_May**2) + \n",
    "                         temperature_Mar + potential_evaporation_Apr + \n",
    "                         precipitation_Apr + I(precipitation_Apr**2) +\n",
    "                         temperature_May:temperature_Jul\n",
    "\"\"\"\n",
    "print(\"Fitting Final Champion model for Spring Wheat...\")\n",
    "final_model = smf.glm(\n",
    "    formula=final_champion_formula,\n",
    "    data=df_wheat,\n",
    "    family=sm.families.Gamma(link=sm.families.links.log())\n",
    ").fit()\n",
    "print(f\"Model fitted successfully. AIC: {final_model.aic:.2f}\")\n",
    "\n",
    "\n",
    "# --- 3. Prepare a Base Prediction Dictionary with Median Values ---\n",
    "median_values = {\n",
    "    'year': df_wheat['year'].median(),\n",
    "    'lat': df_wheat['lat'].median(),\n",
    "    'lon': df_wheat['lon'].median(),\n",
    "    'temperature_Jul': df_wheat['temperature_Jul'].median(),\n",
    "    'temperature_May': df_wheat['temperature_May'].median(),\n",
    "    'temperature_Mar': df_wheat['temperature_Mar'].median(),\n",
    "    'potential_evaporation_Apr': df_wheat['potential_evaporation_Apr'].median(),\n",
    "    'precipitation_Apr': df_wheat['precipitation_Apr'].median()\n",
    "}\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "# --- PLOTS A, B, C: The Three Non-Linear Main Effects ---\n",
    "# We create a dictionary to loop through, making the code cleaner\n",
    "plots_to_make = {\n",
    "    'temperature_Jul': {'color': 'red', 'xlabel': 'Average July Temperature (°C)'},\n",
    "    'temperature_May': {'color': 'orange', 'xlabel': 'Average May Temperature (°C)'},\n",
    "    'precipitation_Apr': {'color': 'blue', 'xlabel': 'April Precipitation (mm)'}\n",
    "}\n",
    "\n",
    "for var, props in plots_to_make.items():\n",
    "    print(f\"\\nGenerating plot: Non-Linear Yield Response to {var}...\")\n",
    "    \n",
    "    x_range = np.linspace(df_wheat[var].min(), df_wheat[var].max(), 100)\n",
    "    pred_df = pd.DataFrame(median_values, index=range(100))\n",
    "    pred_df[var] = x_range\n",
    "    \n",
    "    preds = final_model.get_prediction(pred_df).summary_frame(alpha=0.05)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(pred_df[var], preds['mean'], color=props['color'], linewidth=3, label='Predicted Yield')\n",
    "    plt.fill_between(pred_df[var], preds['mean_ci_lower'], preds['mean_ci_upper'], color=props['color'], alpha=0.2, label='95% Confidence Interval')\n",
    "    \n",
    "    plt.title(f'Non-Linear Yield Response of Spring Wheat to {var}', fontsize=18)\n",
    "    plt.xlabel(props['xlabel'], fontsize=14)\n",
    "    plt.ylabel('Predicted Spring Wheat Yield (t/ha)', fontsize=14)\n",
    "    plt.legend(); plt.ylim(bottom=0); plt.show()\n",
    "\n",
    "\n",
    "# --- PLOT D: The \"Money Plot\" - Compounding Heat Stress Interaction ---\n",
    "print('\\nGenerating Plot D: The \"Compounding Heat Stress\" Interaction Effect...')\n",
    "\n",
    "# Define the scenarios for May temperature\n",
    "temp_may_quantiles = df_wheat['temperature_May'].quantile([0.25, 0.5, 0.75])\n",
    "scenarios = {\n",
    "    'Cool May (25th Pct)': {'value': temp_may_quantiles[0.25], 'color': 'blue'},\n",
    "    'Average May (50th Pct)': {'value': temp_may_quantiles[0.50], 'color': 'purple'},\n",
    "    'Hot May (75th Pct)': {'value': temp_may_quantiles[0.75], 'color': 'red'}\n",
    "}\n",
    "\n",
    "x_range_jul = np.linspace(df_wheat['temperature_Jul'].min(), df_wheat['temperature_Jul'].max(), 100)\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "for scenario_name, props in scenarios.items():\n",
    "    pred_df_interact = pd.DataFrame(median_values, index=range(100))\n",
    "    pred_df_interact['temperature_Jul'] = x_range_jul  # Vary July temp on x-axis\n",
    "    pred_df_interact['temperature_May'] = props['value'] # Set the May temp for this scenario\n",
    "    \n",
    "    preds_interact = final_model.get_prediction(pred_df_interact).summary_frame(alpha=0.05)\n",
    "    \n",
    "    plt.plot(pred_df_interact['temperature_Jul'], preds_interact['mean'], color=props['color'], linewidth=3, label=scenario_name)\n",
    "    plt.fill_between(pred_df_interact['temperature_Jul'], preds_interact['mean_ci_lower'], preds_interact['mean_ci_upper'], color=props['color'], alpha=0.15)\n",
    "\n",
    "plt.title('Vulnerability to July Heat is Compounded by a Hot May', fontsize=18)\n",
    "plt.xlabel('Average July Temperature (°C)', fontsize=14)\n",
    "plt.ylabel('Predicted Spring Wheat Yield (t/ha)', fontsize=14)\n",
    "plt.legend(title='May Temperature Scenario'); plt.ylim(bottom=0); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af7205",
   "metadata": {},
   "source": [
    "### **Analysis of Spring Wheat Yield Response Curves**\n",
    "\n",
    "These plots visualize the key relationships from our final, complex statistical model. They show how predicted spring wheat yield responds to specific climate factors, holding all others at their typical values.\n",
    "\n",
    "#### **Plots 1 & 2: The Dominance of Heat Stress (`temperature_Jul` & `temperature_May`)**\n",
    "\n",
    "*   **Primary Finding:** Higher temperatures in both May and July are consistently and significantly associated with lower yields.\n",
    "*   **Interpretation:** Both plots show a similar pattern: a steep decline in yield as temperatures rise. The slight curve reveals a subtle inverted U-shape, but the peak occurs at very cool temperatures. This indicates that for the vast majority of observed conditions, the primary effect of temperature in May and July is **heat stress**.\n",
    "\n",
    "#### **Plot 3: The Optimal Level of Early-Season Rain (`precipitation_Apr`)**\n",
    "\n",
    "*   **Primary Finding:** The relationship with April precipitation is a distinct **inverted U-shape**.\n",
    "*   **Interpretation:** The model shows that yield is maximized at a moderate level of spring rainfall (approx. 200-250 mm). This demonstrates a \"too much of a good thing\" effect, where both drought conditions (too little rain) and waterlogging/oversaturation (too much rain) are detrimental to crop establishment.\n",
    "\n",
    "#### **Plot 4: The Core Insight - Compounding Heat Stress**\n",
    "\n",
    "*   **Primary Finding:** The plant's vulnerability to heat stress in July is **compounded by heat stress experienced in May.**\n",
    "*   **Interpretation:** This plot visualizes our most sophisticated finding. It shows the yield response to July heat under three different May temperature scenarios.\n",
    "    *   The **red line (\"Hot May\")** has the steepest downward slope. This is the visual proof that a plant \"pre-stressed\" by a hot May suffers the most severe yield losses when it also faces a hot July.\n",
    "    *   Conversely, the **blue line (\"Cool May\")** is the flattest. This demonstrates that a plant that enjoys a mild May is more **resilient**, suffering a much smaller yield penalty when faced with the same level of July heat. This quantifies the critical, compounding nature of heat stress across different growth stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcaa785",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Generating Final Vulnerability Curves for Spring Wheat ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat = pd.read_csv(file_path)\n",
    "    df_wheat = df_wheat[df_wheat['yield_wheat_spring'] > 0].copy()\n",
    "    print(\"Data prepared successfully.\")\n",
    "\n",
    "    # --- 2. Fit Our Final Champion Model ---\n",
    "    final_champion_formula = \"\"\"\n",
    "        yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                             temperature_Jul + I(temperature_Jul**2) + \n",
    "                             temperature_May + I(temperature_May**2) + \n",
    "                             temperature_Mar + potential_evaporation_Apr + \n",
    "                             precipitation_Apr + I(precipitation_Apr**2) +\n",
    "                             temperature_May:temperature_Jul\n",
    "    \"\"\"\n",
    "    final_model = smf.glm(\n",
    "        formula=final_champion_formula,\n",
    "        data=df_wheat,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    ).fit()\n",
    "    print(f\"Final champion model for Spring Wheat fitted successfully. AIC: {final_model.aic:.2f}\\n\")\n",
    "\n",
    "    # --- 3. Define Scenarios and the Baseline ---\n",
    "    # We define our \"compounding heat stress\" scenarios based on May's temperature\n",
    "    temp_may_quantiles = df_wheat['temperature_May'].quantile([0.25, 0.5, 0.75])\n",
    "    scenarios = {\n",
    "        'Cool May (25th Pct)': {'value': temp_may_quantiles[0.25], 'color': 'blue'},\n",
    "        'Average May (50th Pct)': {'value': temp_may_quantiles[0.50], 'color': 'purple'},\n",
    "        'Hot May (75th Pct)': {'value': temp_may_quantiles[0.75], 'color': 'red'}\n",
    "    }\n",
    "    \n",
    "    # Our \"typical year\" is a year with average conditions in both May and July\n",
    "    median_values = {\n",
    "        'year': df_wheat['year'].median(),\n",
    "        'lat': df_wheat['lat'].median(),\n",
    "        'lon': df_wheat['lon'].median(),\n",
    "        'temperature_Jul': df_wheat['temperature_Jul'].median(),  # Median July heat\n",
    "        'temperature_May': df_wheat['temperature_May'].median(),  # Median May heat\n",
    "        'temperature_Mar': df_wheat['temperature_Mar'].median(),\n",
    "        'potential_evaporation_Apr': df_wheat['potential_evaporation_Apr'].median(),\n",
    "        'precipitation_Apr': df_wheat['precipitation_Apr'].median()\n",
    "    }\n",
    "    X_baseline = pd.DataFrame(median_values, index=[0])\n",
    "    yield_baseline = final_model.get_prediction(X_baseline).summary_frame()['mean'].iloc[0]\n",
    "    print(f\"Predicted baseline yield for a typical case (median May, median July): {yield_baseline:.2f} t/ha\")\n",
    "\n",
    "    # --- 4. Generate and Plot Vulnerability for Each Scenario ---\n",
    "    print(\"Generating the vulnerability curve plot...\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(14, 9))\n",
    "    \n",
    "    temp_jul_range = np.linspace(df_wheat['temperature_Jul'].min(), df_wheat['temperature_Jul'].max(), 100)\n",
    "\n",
    "    for scenario_name, props in scenarios.items():\n",
    "        # Create the prediction grid for this scenario\n",
    "        pred_df_scenario = pd.DataFrame(median_values, index=range(100))\n",
    "        pred_df_scenario['temperature_Jul'] = temp_jul_range # Vary July heat on x-axis\n",
    "        pred_df_scenario['temperature_May'] = props['value'] # Set the May heat level for this scenario\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = final_model.get_prediction(pred_df_scenario).summary_frame(alpha=0.05)\n",
    "        yield_predicted = preds['mean']\n",
    "        \n",
    "        # Calculate percentage change from the single baseline\n",
    "        yield_change_pct = ((yield_predicted - yield_baseline) / yield_baseline) * 100\n",
    "        \n",
    "        # Plot the vulnerability curve for this scenario\n",
    "        ax.plot(temp_jul_range, yield_change_pct, color=props['color'], linewidth=3, label=scenario_name)\n",
    "\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_title('Vulnerability of Spring Wheat to July Heat, Compounded by May Temperature', fontsize=18)\n",
    "    ax.set_xlabel('Average July Temperature (°C)', fontsize=14)\n",
    "    ax.set_ylabel('Yield Change vs. Typical Year (%)', fontsize=14)\n",
    "    ax.legend(title='May Temperature Scenario')\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842908cd",
   "metadata": {},
   "source": [
    "### **Analysis of the Final Spring Wheat Vulnerability Curve**\n",
    "\n",
    "This vulnerability curve is the ultimate synthesis of our model's most important finding: the compounding nature of heat stress. It translates the complex statistical interaction into a clear, quantitative measure of risk and opportunity.\n",
    "\n",
    "#### **How to Read This Plot**\n",
    "\n",
    "*   The **X-axis** represents the range of average July temperatures, from cool to extremely hot.\n",
    "*   The **Y-axis** shows the predicted percentage change in yield compared to a baseline \"typical year\" (which has both average May and average July temperatures).\n",
    "*   The **three colored lines** represent the vulnerability to July's temperature under three different May temperature scenarios: Cool, Average, and Hot.\n",
    "\n",
    "#### **Primary Findings**\n",
    "\n",
    "1.  **Compounding Vulnerability to Heat:**\n",
    "    *   The most critical finding is the difference in the slopes of the curves on the right side of the plot. The **red line (\"Hot May\")** is the steepest, showing the most dramatic yield loss as July gets hotter. For example, at 25°C in July, a plant that also experienced a hot May suffers a yield loss of approximately **-15%**.\n",
    "    *   In contrast, the **blue line (\"Cool May\")** is the flattest. A plant that had a mild start to the season is more resilient; it suffers a much smaller yield loss of only about **-5%** at the same 25°C July temperature.\n",
    "\n",
    "2.  **A Complex \"Head Start\" Effect:**\n",
    "    *   Interestingly, the plot reveals a crossover effect. In the rare scenario of a **very cool July** (below ~18°C), a \"Hot May\" (red line) actually leads to the highest yield *gain*. This suggests that an initial burst of warmth can be beneficial if it is not followed by subsequent heat stress, possibly by accelerating early development.\n",
    "\n",
    "3.  **The Dominance of Heat Stress:**\n",
    "    *   For any July temperature above ~21°C, all scenarios result in a **yield loss** compared to a typical year. This confirms that late-season heat stress is the dominant threat to spring wheat in this region, and a \"Hot May\" consistently and significantly worsens this vulnerability.\n",
    "\n",
    "#### **Overall Conclusion**\n",
    "\n",
    "This plot powerfully visualizes the model's core insight: **vulnerability is not static.** A hot May \"pre-stresses\" the crop, making it substantially more vulnerable to the damaging effects of a hot July. Conversely, a cool May builds resilience, helping to buffer the crop against subsequent heat stress. This quantifies the critical, compounding nature of climate risk for spring wheat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa80d1",
   "metadata": {},
   "source": [
    "### **Final Spring Wheat Model: Interpretation and Conclusions**\n",
    "\n",
    "This section summarizes the final champion model developed to explain the relationship between monthly climate stressors and spring wheat yield in Northern Italy. The model is the result of a multi-step workflow designed to be statistically robust, parsimonious, and highly insightful.\n",
    "\n",
    "#### **The Final Champion Model**\n",
    "\n",
    "After a data-driven process of variable selection and extensive iterative refinement, the final, best-performing model was determined to be a Gamma GLM with a complex interactive and non-linear structure:\n",
    "\n",
    "**Final Model Formula:**\n",
    "```\n",
    "yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                     temperature_Jul + I(temperature_Jul**2) + \n",
    "                     temperature_May + I(temperature_May**2) + \n",
    "                     temperature_Mar + potential_evaporation_Apr + \n",
    "                     precipitation_Apr + I(precipitation_Apr**2) +\n",
    "                     temperature_May:temperature_Jul\n",
    "```\n",
    "\n",
    "**Key Performance Metrics:**\n",
    "*   **Akaike Information Criterion (AIC):** `3467.84` (The lowest of all tested models)\n",
    "*   **Pseudo R-squared (CS):** `0.5762` (Explains approx. **58%** of the variation in yield)\n",
    "\n",
    "#### **The Modeling Journey: How We Arrived Here**\n",
    "\n",
    "The final model was the product of a systematic, evidence-based process:\n",
    "\n",
    "1.  **Variable Selection:** An **Elastic Net regularization** revealed that spring wheat yield is influenced by a complex mix of factors throughout its earlier growing season, identifying a broad set of robust predictors.\n",
    "\n",
    "2.  **Model Refinement (Parsimony):** An initial base model was fitted. Unlike other crops, all selected climate predictors proved to be statistically significant, indicating a more complex set of initial drivers.\n",
    "\n",
    "3.  **Testing for Non-Linearity:** Guided by strong evidence from our EDA, we systematically tested for non-linear effects. Adding quadratic terms for `temperature_Jul`, `temperature_May`, and `precipitation_Apr` all resulted in **massive, successive drops in the AIC**, confirming that multiple, distinct \"optimal\" conditions are critical for spring wheat.\n",
    "\n",
    "4.  **Testing for Interactions:** We tested the most plausible interaction hypotheses. While a `Heat x Water` interaction was not significant, the **`temperature_May:temperature_Jul`** (compounding heat stress) interaction proved **highly significant and dramatically improved the AIC**, becoming the core insight of the final model.\n",
    "\n",
    "This structured process ensures our final model is not overfit and that its complexity is justified by strong statistical evidence.\n",
    "\n",
    "#### **Detailed Interpretation of the Final Model**\n",
    "\n",
    "*   **Control Variables:**\n",
    "    *   `year`: The positive, significant coefficient confirms a strong **technological trend**, with yields consistently increasing over time.\n",
    "    *   `bs(lat, df=4)` & `bs(lon, df=4)`: The high significance of the spatial splines confirms that **geography is a dominant driver** of yield.\n",
    "\n",
    "*   **Key Climate Drivers:**\n",
    "    *   **Multiple Non-Linear Effects:** The model identified three distinct \"optimal\" conditions. `temperature_Jul`, `temperature_May`, and `precipitation_Apr` all have significant **inverted U-shaped effects**. This is a major finding, indicating that spring wheat is highly sensitive to \"just right\" conditions, and yields suffer if it is too hot/cold or too wet/dry during these critical months.\n",
    "    *   **The Compounding Heat Stress Interaction:** This is the model's most important finding. The significant `temperature_May:temperature_Jul` interaction reveals that heat stress is not simply additive; it's compounding. A hot May \"pre-stresses\" the crop, making it significantly more vulnerable to the damaging effects of a subsequent hot July.\n",
    "\n",
    "#### **Insights from Visualization**\n",
    "\n",
    "*   **The Non-Linear Response Curves:** The yield response plots for the three temperature and precipitation variables visually confirmed the **inverted U-shapes**, allowing us to see the optimal conditions for each month and the penalty for deviating from them.\n",
    "\n",
    "*   **The Final Vulnerability Curve (Interaction Plot):** This plot synthesizes the core finding into a quantitative measure of risk. By showing the vulnerability to July heat under different May temperature scenarios, it tells a clear story:\n",
    "    *   **Resilience:** A crop that experiences a \"Cool May\" (blue line) is more resilient, suffering a much smaller percentage yield loss from a hot July.\n",
    "    *   **Vulnerability:** A crop \"pre-stressed\" by a \"Hot May\" (red line) is far more vulnerable, suffering a dramatically more severe yield loss when faced with the same July heat.\n",
    "\n",
    "#### **Overall Conclusion**\n",
    "\n",
    "The model provides a powerful and nuanced explanation of spring wheat vulnerability. Unlike the other summer crops, spring wheat's success is not tied to one or two dominant factors but is highly dependent on achieving **\"optimal\" conditions across multiple, distinct phases** of its growth (April establishment, May growth, July grain-fill). The model's core insight is the discovery of **compounding heat stress**, where early-season heat in May dramatically amplifies the crop's vulnerability to the primary threat of a hot July. This highlights a complex, time-dependent climate risk profile that is unique to spring wheat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
