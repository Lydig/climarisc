{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59fc82f3",
   "metadata": {},
   "source": [
    "# EDA for Winter Wheat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c22e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import scipy.stats as stats\n",
    "from patsy import dmatrices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c840638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis - Full Correlation Matrix\n",
    "\n",
    "print(\"--- EDA: Correlation Analysis of All Monthly Stressors ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# As confirmed, this file is already specific to wheat_winter and its growing season.\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_winter = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Select Only the Monthly Stressor Variables ---\n",
    "    # We will select all columns that have a month name in them, which is a robust\n",
    "    # way to grab all the monthly predictors we want to investigate.\n",
    "    monthly_stressors = [col for col in df_wheat_winter.columns if '_' in col and 'yield' not in col]\n",
    "    df_corr = df_wheat_winter[monthly_stressors]\n",
    "    \n",
    "    print(f\"\\nSelected {len(df_corr.columns)} monthly stressor variables for correlation analysis.\")\n",
    "\n",
    "    # --- 3. Calculate and Print the Correlation Matrix ---\n",
    "    correlation_matrix = df_corr.corr()\n",
    "    \n",
    "    # Optional: If you want to see the full numerical matrix, uncomment the next line\n",
    "    # print(\"\\n--- Full Pairwise Correlation Matrix ---\")\n",
    "    # print(correlation_matrix)\n",
    "\n",
    "    # --- 4. Visualize the Matrix with a Heatmap ---\n",
    "    # A heatmap is the best way to see the broad patterns of collinearity.\n",
    "    print(\"\\nGenerating correlation heatmap...\")\n",
    "    \n",
    "    plt.figure(figsize=(18, 15))\n",
    "    heatmap = sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        cmap='coolwarm',  # Use a diverging colormap (red=positive, blue=negative)\n",
    "        center=0,         # Center the colormap at zero\n",
    "        vmin=-1,          # Set the color scale limits to the theoretical min/max\n",
    "        vmax=1,\n",
    "        linewidths=.5,\n",
    "        annot=False       # Annotations are turned off as the matrix is too large to be readable\n",
    "    )\n",
    "    \n",
    "    plt.title('Pairwise Correlation Matrix of All Monthly Stressors for wheat_winter', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Extended EDA for wheat_winter Yield Analysis ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_winter = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- Task 1: Examine the Distribution of the Dependent Variable (yield_wheat_winter) ---\n",
    "    print(\"--- Task 1: Analyzing the distribution of the dependent variable 'yield_wheat_winter' ---\")\n",
    "    \n",
    "    # Create a figure with two subplots side-by-side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Distribution Analysis for yield_wheat_winter', fontsize=16)\n",
    "\n",
    "    # a) Histogram with a Kernel Density Estimate (KDE)\n",
    "    # This helps us visually assess the shape, center, and spread of the yield data.\n",
    "    # We are checking for positive skewness, which is characteristic of data modeled by a Gamma distribution.\n",
    "    sns.histplot(df_wheat_winter['yield_wheat_winter'], kde=True, ax=axes[0])\n",
    "    axes[0].set_title('Histogram of wheat_winter Yield')\n",
    "    axes[0].set_xlabel('Yield (tonnes per hectare)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    # b) Q-Q (Quantile-Quantile) Plot against a theoretical normal distribution\n",
    "    # This plot helps us assess if the data's distribution follows a specific theoretical distribution.\n",
    "    # Deviations from the red line suggest skewness or heavy tails.\n",
    "    # While our target is a Gamma GLM, a Q-Q plot vs. Normal is a standard first step to detect non-normality.\n",
    "    stats.probplot(df_wheat_winter['yield_wheat_winter'], dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot of wheat_winter Yield vs. Normal Distribution')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Distribution plots generated. Check for positive skew, which supports our choice of a Gamma GLM.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 2: Bivariate Scatter Plots of Yield vs. Key Stressors ---\n",
    "    print(\"--- Task 2: Visualizing relationships between yield and key climate stressors ---\")\n",
    "    \n",
    "    # Select a few key stressors based on agronomic theory for wheat_winter\n",
    "    key_stressors = ['temperature_May','potential_evaporation_Jan','precipitation_Apr','temperature_Nov','temperature_Mar','potential_evaporation_Apr']\n",
    "    \n",
    "    # Create a figure to hold the scatter plots\n",
    "    fig, axes = plt.subplots(1, len(key_stressors), figsize=(21, 6))\n",
    "    fig.suptitle('Bivariate Relationships: wheat_winter Yield vs. Key Stressors', fontsize=16)\n",
    "\n",
    "    for i, stressor in enumerate(key_stressors):\n",
    "        # We use a regression plot with a LOWESS (Locally Weighted Scatterplot Smoothing) curve.\n",
    "        # This is a non-parametric way to see the underlying trend without assuming a linear relationship.\n",
    "        # It's excellent for spotting potential non-linearities (like an inverted 'U' shape).\n",
    "        sns.regplot(\n",
    "            x=stressor,\n",
    "            y='yield_wheat_winter',\n",
    "            data=df_wheat_winter,\n",
    "            ax=axes[i],\n",
    "            lowess=True, # Use LOWESS smoother to detect non-linear patterns\n",
    "            scatter_kws={'alpha': 0.1, 'color': 'gray'}, # De-emphasize individual points\n",
    "            line_kws={'color': 'blue'} # Emphasize the trend line\n",
    "        )\n",
    "        axes[i].set_title(f'Yield vs. {stressor}')\n",
    "        axes[i].set_xlabel(f'{stressor}')\n",
    "        axes[i].set_ylabel('Yield (tonnes per hectare)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Scatter plots generated. Look for non-linear patterns that might inform our final model.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 3: Plot Key Variables Over Time ---\n",
    "    print(\"--- Task 3: Examining long-term trends in yield and a key climate variable ---\")\n",
    "    \n",
    "    # Calculate the mean of yield and a key stressor for each year\n",
    "    yearly_data = df_wheat_winter.groupby('year')[['yield_wheat_winter', 'temperature_May']].mean().reset_index()\n",
    "\n",
    "    # Create a plot with a primary and secondary y-axis to show both trends together.\n",
    "    # This confirms the necessity of including 'year' as a control variable to capture trends\n",
    "    # likely related to technology, while also checking for climate trends.\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plotting average yield on the primary (left) y-axis\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Average Yield (tonnes per hectare)', color=color)\n",
    "    ax1.plot(yearly_data['year'], yearly_data['yield_wheat_winter'], color=color, marker='o', label='Average Yield')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Create a second y-axis that shares the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plotting average temperature on the secondary (right) y-axis\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Average May Temperature (°C)', color=color)\n",
    "    ax2.plot(yearly_data['year'], yearly_data['temperature_May'], color=color, linestyle='--', marker='x', label='Avg. July Temp')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('Long-Term Trend of winter Wheat Yield and May Temperature (1982-2016)', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    # Adding a single legend for both lines\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"Time-series plot generated. Note the clear upward trend in yield, confirming the need for a 'year' control variable.\\n\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: A required column was not found in the dataset: {e}. Please check the CSV file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970bed5",
   "metadata": {},
   "source": [
    "# regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb7f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_winter = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Define the Full Model Formula ---\n",
    "    # Programmatically get all monthly stressor column names\n",
    "    monthly_stressors = [col for col in df_wheat_winter.columns if '_' in col and 'yield' not in col]\n",
    "    \n",
    "    # Join them with '+' to create the predictor part of the formula\n",
    "    stressor_formula_part = ' + '.join(monthly_stressors)\n",
    "    \n",
    "    # Construct the complete R-style formula string.\n",
    "    # We include our controls (year, spatial splines) and all potential predictors.\n",
    "    # Note: patsy's bs() function creates the basis spline columns.\n",
    "    formula = f\"yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + {stressor_formula_part}\"\n",
    "    \n",
    "    print(\"\\nGenerated model formula for patsy:\")\n",
    "    print(formula) # Uncomment to see the full, very long formula string\n",
    "\n",
    "    # --- 3. Create the Design Matrix (X) and Response Vector (y) ---\n",
    "    # patsy processes the formula and the dataframe to create the matrices needed for modeling.\n",
    "    # 'y' will be our dependent variable, 'X' will be the full set of predictors.\n",
    "    # The intercept is automatically included in 'X' by patsy.\n",
    "    print(\"\\nCreating design matrix (X) and response vector (y) using patsy...\")\n",
    "    y, X = dmatrices(formula, data=df_wheat_winter, return_type='dataframe')\n",
    "    \n",
    "    print(f\"Successfully created response vector y with shape: {y.shape}\")\n",
    "    print(f\"Successfully created design matrix X with shape: {X.shape}\")\n",
    "    print(f\"The {X.shape[1]} columns in X include the intercept, year, 8 spline bases (4 for lat, 4 for lon), and {len(monthly_stressors)} climate stressors.\")\n",
    "\n",
    "    # --- 4. Standardize the Predictor Matrix (X) ---\n",
    "    # We scale ALL predictors to have a mean of 0 and a standard deviation of 1.\n",
    "    # This ensures the regularization penalty is applied fairly to all variables.\n",
    "    # We do NOT scale the response variable y.\n",
    "    print(\"\\nStandardizing the design matrix X...\")\n",
    "    \n",
    "    # We remove the Intercept column before scaling, as it should not be regularized or scaled.\n",
    "    # We will add it back later if needed, but scikit-learn's models handle it by default.\n",
    "    X_no_intercept = X.drop('Intercept', axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_values = scaler.fit_transform(X_no_intercept)\n",
    "    \n",
    "    # Convert the scaled array back to a pandas DataFrame with the original column names\n",
    "    X_scaled = pd.DataFrame(X_scaled_values, columns=X_no_intercept.columns, index=X.index)\n",
    "    \n",
    "    print(\"Standardization complete.\")\n",
    "    \n",
    "    # Verification: Check the mean and standard deviation of a few scaled columns\n",
    "    print(\"\\n--- Verification of Standardization ---\")\n",
    "    verification_cols = ['year', 'bs(lat, df=4)[0]', 'temperature_Jul']\n",
    "    for col in verification_cols:\n",
    "        mean_val = X_scaled[col].mean()\n",
    "        std_val = X_scaled[col].std()\n",
    "        print(f\"Column '{col}': Mean = {mean_val:.4f}, Std Dev = {std_val:.4f}\")\n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc267372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume 'y' and 'X_scaled' are already in memory from the previous step.\n",
    "# If not, you would need to re-run the data preparation script.\n",
    "\n",
    "try:\n",
    "    # --- 1. Define the GLM Model ---\n",
    "    # We specify our model family (Gamma) and the link function (log) as per our project plan.\n",
    "    # We pass the prepared y and the fully scaled X matrix.\n",
    "    # Note: statsmodels requires the intercept to be in the X matrix, which patsy provided.\n",
    "    \n",
    "    # We need to add the intercept back to the scaled data for statsmodels GLM\n",
    "    X_scaled_with_intercept = X.copy() # Start with the original X to preserve intercept and structure\n",
    "    X_scaled_with_intercept[X_no_intercept.columns] = X_scaled # Replace non-intercept columns with scaled versions\n",
    "\n",
    "    gl_gamma = sm.GLM(y, X_scaled_with_intercept, family=sm.families.Gamma(link=sm.families.links.log()))\n",
    "    print(\"Successfully initialized Gamma GLM with a log link.\")\n",
    "\n",
    "    # --- 2. Set up the Regularization Path ---\n",
    "    # We need to test a series of alpha values (penalty strengths).\n",
    "    # A logarithmic scale is best for this, from a weak penalty to a strong one.\n",
    "    n_alphas = 100\n",
    "    alphas = np.logspace(-3, 0.5, n_alphas) # From 0.001 to ~3.16\n",
    "\n",
    "    # The L1_wt parameter controls the Elastic Net mix (0=Ridge, 1=Lasso). \n",
    "    # 0.5 is a balanced choice.\n",
    "    elastic_net_l1_wt = 0.5 \n",
    "    \n",
    "    print(f\"Will fit the model for {n_alphas} alpha values with L1_wt (l1_ratio) = {elastic_net_l1_wt}\")\n",
    "\n",
    "    # --- 3. Fit the Model for Each Alpha and Store Coefficients ---\n",
    "    # We will loop through our alphas and save the coefficients from each model fit.\n",
    "    coefficients = []\n",
    "    \n",
    "    for alpha_val in alphas:\n",
    "        # The fit_regularized method performs the Elastic Net estimation.\n",
    "        # We set refit=False because we want to see the shrunken coefficients for this analysis.\n",
    "        results = gl_gamma.fit_regularized(\n",
    "            method='elastic_net', \n",
    "            alpha=alpha_val, \n",
    "            L1_wt=elastic_net_l1_wt,\n",
    "            refit=False \n",
    "        )\n",
    "        coefficients.append(results.params)\n",
    "    \n",
    "    # Convert the list of coefficient series into a DataFrame for easy plotting\n",
    "    coef_df = pd.DataFrame(coefficients, index=alphas)\n",
    "    coef_df.index.name = \"alpha\"\n",
    "    \n",
    "    # Exclude the Intercept for plotting, as it's not regularized and has a different scale.\n",
    "    coef_df_no_intercept = coef_df.drop('Intercept', axis=1)\n",
    "    \n",
    "    print(\"\\nCompleted fitting models along the regularization path.\")\n",
    "\n",
    "    # --- 4. Visualize the Regularization Path ---\n",
    "    print(\"Generating the regularization path plot...\")\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    ax.plot(coef_df_no_intercept)\n",
    "    ax.set_xscale('log') # The alpha path is best viewed on a log scale\n",
    "    \n",
    "    # Add a vertical line at zero\n",
    "    ax.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    \n",
    "    ax.set_title('Regularization Path for sprwintering Wheat Model Coefficients (Elastic Net)', fontsize=18)\n",
    "    ax.set_xlabel('Alpha (Penalty Strength)', fontsize=14)\n",
    "    ax.set_ylabel('Standardized Coefficient Value', fontsize=14)\n",
    "    \n",
    "    # To avoid a cluttered legend, we don't add one here. The goal is to see the general pattern.\n",
    "    # Alternatively, for fewer variables, a legend could be useful:\n",
    "    # ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"ERROR: Make sure that 'y' and 'X_scaled' DataFrames from the previous step are available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e79f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Corrected code to identify the most robust variables ---\n",
    "# We will inspect the coefficients at a moderately high alpha value\n",
    "# This tells us which variables \"survived\" the penalty the longest.\n",
    "alpha_to_inspect = 0.03\n",
    "\n",
    "try:\n",
    "    # Find the alpha in our index that is closest to our target\n",
    "    # CORRECTED LINE: The operation works directly on the index without .flat\n",
    "    closest_alpha = coef_df.index[np.abs(coef_df.index - alpha_to_inspect).argmin()]\n",
    "\n",
    "    print(f\"--- Coefficients at alpha ≈ {closest_alpha:.4f} ---\")\n",
    "\n",
    "    # Get the coefficients at this alpha and sort them by absolute value\n",
    "    robust_coeffs = coef_df.loc[closest_alpha].copy()\n",
    "    robust_coeffs_sorted = robust_coeffs.abs().sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nVariables sorted by the absolute magnitude of their shrunken coefficient:\")\n",
    "    # We display more variables to get a fuller picture\n",
    "    print(robust_coeffs_sorted.head(15))\n",
    "\n",
    "    # Let's also see their actual values (positive or negative) for the top variables\n",
    "    print(\"\\n--- Actual coefficient values for the most robust variables ---\")\n",
    "    print(coef_df.loc[closest_alpha, robust_coeffs_sorted.index].head(15))\n",
    "\n",
    "except NameError:\n",
    "     print(\"ERROR: Make sure that 'coef_df' DataFrame from the previous step is available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa280721",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Base Champion Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_winter = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the base Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    base_formula = \"yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + temperature_May + temperature_Nov + temperature_Mar + potential_evaporation_Jan + potential_evaporation_Apr\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    base_model = smf.glm(\n",
    "        formula=base_formula,\n",
    "        data=df_wheat_winter,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    base_model_results = base_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(base_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Future Comparison ---\n",
    "    # The AIC is a key metric for comparing different model formulations. Lower is better.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Base Champion Model: {base_model_results.aic:.2f}\")\n",
    "    print(\"This will be our benchmark for comparison.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83716a6",
   "metadata": {},
   "source": [
    "# quadratic terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823212f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Quadratic Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_winter = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the base Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    quadratic_formula = \"yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + temperature_May + I(temperature_May**2) + temperature_Nov + temperature_Mar + I(temperature_Mar**2) + potential_evaporation_Jan + potential_evaporation_Apr\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    quadratic_model = smf.glm(\n",
    "        formula=quadratic_formula,\n",
    "        data=df_wheat_winter,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    quadratic_model_results = quadratic_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(quadratic_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion's AIC (3516.86).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Quadratic Model: {quadratic_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the Base Model's AIC (2681.90).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60dd65",
   "metadata": {},
   "source": [
    "AIC when temp may is made quadratic: 2566.27\n",
    "\n",
    "AIC when temp now is also made quadratic: 2565.54\n",
    "\n",
    "AIC when temp now is also made quadratic: 2563.08\n",
    "\n",
    "The model for AIC = 2563.08:\n",
    "quadratic_formula = \"yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + temperature_May + I(temperature_May**2) + temperature_Nov + I(temperature_Nov**2) + temperature_Mar + I(temperature_Mar**2) + potential_evaporation_Jan + potential_evaporation_Apr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the interaction Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_winter = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the base Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    interaction_formula = \"yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + \" \\\n",
    "    \"temperature_May + I(temperature_May**2) + temperature_Nov + temperature_Mar + \" \\\n",
    "    \"I(temperature_Mar**2) + potential_evaporation_Jan + potential_evaporation_Apr +\" \\\n",
    "    \"temperature_Mar*temperature_May +\" \\\n",
    "    \"potential_evaporation_Jan*temperature_May\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    interaction_model = smf.glm(\n",
    "        formula=interaction_formula,\n",
    "        data=df_wheat_winter,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    interaction_model_results = interaction_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(interaction_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion's AIC (3516.86).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Quadratic Model: {interaction_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the quadratic best Model's AIC (2563.84).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773f777",
   "metadata": {},
   "source": [
    "# visalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"--- Generating Final, Refined Yield Response Curves for Winter Wheat ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "df_wheat = pd.read_csv(file_path)\n",
    "df_wheat = df_wheat[df_wheat['yield_wheat_winter'] > 0].copy()\n",
    "print(\"Data prepared successfully.\")\n",
    "\n",
    "# --- 2. Fit Our Final Champion Model ---\n",
    "final_champion_formula = \"\"\"\n",
    "    yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                         temperature_May + I(temperature_May**2) + \n",
    "                         temperature_Nov + temperature_Mar + I(temperature_Mar**2) + \n",
    "                         potential_evaporation_Jan + potential_evaporation_Apr +\n",
    "                         temperature_Mar:temperature_May +\n",
    "                         potential_evaporation_Jan:temperature_May\n",
    "\"\"\"\n",
    "print(\"Fitting Final Champion model for Winter Wheat...\")\n",
    "final_model = smf.glm(\n",
    "    formula=final_champion_formula,\n",
    "    data=df_wheat,\n",
    "    family=sm.families.Gamma(link=sm.families.links.log())\n",
    ").fit()\n",
    "print(f\"Model fitted successfully. AIC: {final_model.aic:.2f}\")\n",
    "\n",
    "# --- 3. Prepare a Base Prediction Dictionary with Median Values ---\n",
    "median_values = {\n",
    "    'year': df_wheat['year'].median(),\n",
    "    'lat': df_wheat['lat'].median(),\n",
    "    'lon': df_wheat['lon'].median(),\n",
    "    'temperature_May': df_wheat['temperature_May'].median(),\n",
    "    'temperature_Nov': df_wheat['temperature_Nov'].median(),\n",
    "    'temperature_Mar': df_wheat['temperature_Mar'].median(),\n",
    "    'potential_evaporation_Jan': df_wheat['potential_evaporation_Jan'].median(),\n",
    "    'potential_evaporation_Apr': df_wheat['potential_evaporation_Apr'].median()\n",
    "}\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "# --- PLOT B (Now Plot 1): \"Money Plot\" #1 - Compounding Spring Heat Stress ---\n",
    "print('\\nGenerating Plot 1: The \"Compounding Spring Heat\" Interaction...')\n",
    "temp_mar_quantiles = df_wheat['temperature_Mar'].quantile([0.25, 0.5, 0.75])\n",
    "scenarios_mar = {\n",
    "    'Cold March (25th Pct)': {'value': temp_mar_quantiles[0.25], 'color': 'blue'},\n",
    "    'Average March (50th Pct)': {'value': temp_mar_quantiles[0.50], 'color': 'purple'},\n",
    "    'Warm March (75th Pct)': {'value': temp_mar_quantiles[0.75], 'color': 'red'}\n",
    "}\n",
    "\n",
    "temp_may_range = np.linspace(df_wheat['temperature_May'].min(), df_wheat['temperature_May'].max(), 100)\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "for scenario_name, props in scenarios_mar.items():\n",
    "    pred_df_interact = pd.DataFrame(median_values, index=range(100))\n",
    "    pred_df_interact['temperature_May'] = temp_may_range \n",
    "    pred_df_interact['temperature_Mar'] = props['value'] \n",
    "    \n",
    "    preds_interact = final_model.get_prediction(pred_df_interact).summary_frame(alpha=0.05)\n",
    "    \n",
    "    plt.plot(pred_df_interact['temperature_May'], preds_interact['mean'], color=props['color'], linewidth=3, label=scenario_name)\n",
    "    plt.fill_between(pred_df_interact['temperature_May'], preds_interact['mean_ci_lower'], preds_interact['mean_ci_upper'], color=props['color'], alpha=0.15)\n",
    "\n",
    "plt.title('Vulnerability to May Heat is Compounded by March Temperature', fontsize=18)\n",
    "plt.xlabel('Average May Temperature (°C)', fontsize=14)\n",
    "plt.ylabel('Predicted Winter Wheat Yield (t/ha)', fontsize=14)\n",
    "plt.legend(title='March Temperature Scenario')\n",
    "# --- MODIFIED LINE ---\n",
    "plt.ylim(2, 14) # Zooming in on the y-axis\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- PLOT C (Now Plot 2): \"Money Plot\" #2 - Winter Conditions x Spring Stress ---\n",
    "print('\\nGenerating Plot 2: The \"Winter x Spring\" Interaction...')\n",
    "pe_jan_quantiles = df_wheat['potential_evaporation_Jan'].quantile([0.25, 0.5, 0.75])\n",
    "scenarios_jan = {\n",
    "    'Cloudy/Cold Winter (25th Pct PE)': {'value': pe_jan_quantiles[0.25], 'color': 'gray'},\n",
    "    'Average Winter (50th Pct PE)': {'value': pe_jan_quantiles[0.50], 'color': 'purple'},\n",
    "    'Sunny/Mild Winter (75th Pct PE)': {'value': pe_jan_quantiles[0.75], 'color': 'orange'}\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "for scenario_name, props in scenarios_jan.items():\n",
    "    pred_df_interact2 = pd.DataFrame(median_values, index=range(100))\n",
    "    pred_df_interact2['temperature_May'] = temp_may_range \n",
    "    pred_df_interact2['potential_evaporation_Jan'] = props['value'] \n",
    "    \n",
    "    preds_interact2 = final_model.get_prediction(pred_df_interact2).summary_frame(alpha=0.05)\n",
    "    \n",
    "    plt.plot(pred_df_interact2['temperature_May'], preds_interact2['mean'], color=props['color'], linewidth=3, label=scenario_name)\n",
    "    plt.fill_between(pred_df_interact2['temperature_May'], preds_interact2['mean_ci_lower'], preds_interact2['mean_ci_upper'], color=props['color'], alpha=0.15)\n",
    "\n",
    "plt.title('Vulnerability to May Heat is Influenced by Winter Conditions', fontsize=18)\n",
    "plt.xlabel('Average May Temperature (°C)', fontsize=14)\n",
    "plt.ylabel('Predicted Winter Wheat Yield (t/ha)', fontsize=14)\n",
    "plt.legend(title='January PE Scenario')\n",
    "# --- MODIFIED LINE ---\n",
    "plt.ylim(2, 10) # Zooming in on the y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e5dfb6",
   "metadata": {},
   "source": [
    "### **Analysis of Winter Wheat Interaction Plots**\n",
    "\n",
    "These plots visualize the two critical interaction effects discovered by our final model. They reveal a complex story where the vulnerability of winter wheat to its primary stressor—heat during the May flowering period—is significantly altered by the weather conditions from earlier in its life cycle.\n",
    "\n",
    "#### **Plot 1: Vulnerability to May Heat is Compounded by March Temperature**\n",
    "\n",
    "*   **Primary Finding:** A warm March \"pre-stresses\" the plant, making it significantly more vulnerable to subsequent heat stress in May.\n",
    "*   **Interpretation:** This plot perfectly visualizes the \"compounding heat stress\" interaction.\n",
    "    *   The **red line (\"Warm March\")** is the steepest. It shows that while a warm March can lead to very high yields if May is cool, it also leads to the most catastrophic yield decline as May gets hotter. This suggests that a warm March might cause the plant to break dormancy too early or grow too quickly, leaving it exposed and less resilient.\n",
    "    *   The **blue line (\"Cold March\")** is the flattest. This is the key insight: a plant that experiences a proper cold period in March is far more **resilient** to May heat. While its peak yield potential is lower, it suffers a much smaller penalty from a hot May, resulting in a more stable and predictable harvest.\n",
    "\n",
    "#### **Plot 2: Vulnerability to May Heat is Influenced by Winter Conditions**\n",
    "\n",
    "*   **Primary Finding:** The type of winter the crop endures—cold and cloudy versus mild and sunny—fundamentally changes its response to spring heat stress.\n",
    "*   **Interpretation:**\n",
    "    *   The **gray line (\"Cloudy/Cold Winter\")** shows the highest peak yield potential. This is strong evidence that winter wheat requires a significant period of cold (vernalization) to maximize its reproductive potential. However, these plants also show a very steep decline in yield when faced with a hot May.\n",
    "    *   Conversely, the **orange line (\"Sunny/Mild Winter\")** shows the lowest yield potential across almost all conditions. This suggests that a mild winter fails to provide a sufficient vernalization signal, which limits the plant's yield ceiling from the very start. These plants are less productive overall but appear slightly less sensitive to extreme May heat because their yield potential was already compromised.\n",
    "\n",
    "#### **Overall Conclusion from Visuals**\n",
    "\n",
    "The key story for winter wheat is one of **sequential stress and resilience**. The highest yields are only possible when a \"proper\" cold winter is followed by a \"proper\" cool spring. A mild winter limits the plant's potential from the start. A warm spring, particularly a warm March, \"pre-stresses\" the plant, making it fragile and extremely vulnerable to the primary threat of a hot May during its critical flowering stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "print(\"--- Generating Final Vulnerability Curves for Winter Wheat ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat = pd.read_csv(file_path)\n",
    "    df_wheat = df_wheat[df_wheat['yield_wheat_winter'] > 0].copy()\n",
    "    print(\"Data prepared successfully.\")\n",
    "\n",
    "    # --- 2. Fit Our Final Champion Model ---\n",
    "    final_champion_formula = \"\"\"\n",
    "        yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                             temperature_May + I(temperature_May**2) + \n",
    "                             temperature_Nov + temperature_Mar + I(temperature_Mar**2) + \n",
    "                             potential_evaporation_Jan + potential_evaporation_Apr +\n",
    "                             temperature_Mar:temperature_May +\n",
    "                             potential_evaporation_Jan:temperature_May\n",
    "    \"\"\"\n",
    "    final_model = smf.glm(\n",
    "        formula=final_champion_formula,\n",
    "        data=df_wheat,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    ).fit()\n",
    "    print(f\"Final champion model for Winter Wheat fitted successfully. AIC: {final_model.aic:.2f}\\n\")\n",
    "\n",
    "    # --- 3. Define a Single, Consistent Baseline for All Plots ---\n",
    "    # Our \"typical year\" has median conditions for all key variables.\n",
    "    median_values = {\n",
    "        'year': df_wheat['year'].median(),\n",
    "        'lat': df_wheat['lat'].median(),\n",
    "        'lon': df_wheat['lon'].median(),\n",
    "        'temperature_May': df_wheat['temperature_May'].median(),\n",
    "        'temperature_Nov': df_wheat['temperature_Nov'].median(),\n",
    "        'temperature_Mar': df_wheat['temperature_Mar'].median(),\n",
    "        'potential_evaporation_Jan': df_wheat['potential_evaporation_Jan'].median(),\n",
    "        'potential_evaporation_Apr': df_wheat['potential_evaporation_Apr'].median()\n",
    "    }\n",
    "    X_baseline = pd.DataFrame(median_values, index=[0])\n",
    "    yield_baseline = final_model.get_prediction(X_baseline).summary_frame()['mean'].iloc[0]\n",
    "    print(f\"Predicted baseline yield for a typical case: {yield_baseline:.2f} t/ha\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # --- 4. Vulnerability Plot 1: Compounding Spring Heat Stress ---\n",
    "    print(\"\\nGenerating Vulnerability Plot 1: Compounding Spring Heat Interaction...\")\n",
    "    temp_mar_quantiles = df_wheat['temperature_Mar'].quantile([0.25, 0.5, 0.75])\n",
    "    scenarios_mar = {\n",
    "        'Cold March (25th Pct)': {'value': temp_mar_quantiles[0.25], 'color': 'blue'},\n",
    "        'Average March (50th Pct)': {'value': temp_mar_quantiles[0.50], 'color': 'purple'},\n",
    "        'Warm March (75th Pct)': {'value': temp_mar_quantiles[0.75], 'color': 'red'}\n",
    "    }\n",
    "    \n",
    "    temp_may_range = np.linspace(df_wheat['temperature_May'].min(), df_wheat['temperature_May'].max(), 100)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 9))\n",
    "    for scenario_name, props in scenarios_mar.items():\n",
    "        pred_df_scenario = pd.DataFrame(median_values, index=range(100))\n",
    "        pred_df_scenario['temperature_May'] = temp_may_range \n",
    "        pred_df_scenario['temperature_Mar'] = props['value']\n",
    "        \n",
    "        preds = final_model.get_prediction(pred_df_scenario).summary_frame(alpha=0.05)\n",
    "        yield_predicted = preds['mean']\n",
    "        yield_change_pct = ((yield_predicted - yield_baseline) / yield_baseline) * 100\n",
    "        \n",
    "        ax.plot(temp_may_range, yield_change_pct, color=props['color'], linewidth=3, label=scenario_name)\n",
    "\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_title('Vulnerability to May Heat is Compounded by March Temperature', fontsize=18)\n",
    "    ax.set_xlabel('Average May Temperature (°C)', fontsize=14)\n",
    "    ax.set_ylabel('Yield Change vs. Typical Year (%)', fontsize=14)\n",
    "    ax.legend(title='March Temperature Scenario')\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "    plt.show()\n",
    "\n",
    "    # --- 5. Vulnerability Plot 2: Winter Conditions x Spring Stress ---\n",
    "    print('\\nGenerating Vulnerability Plot 2: The \"Winter x Spring\" Interaction...')\n",
    "    pe_jan_quantiles = df_wheat['potential_evaporation_Jan'].quantile([0.25, 0.5, 0.75])\n",
    "    scenarios_jan = {\n",
    "        'Cloudy/Cold Winter (25th Pct PE)': {'value': pe_jan_quantiles[0.25], 'color': 'gray'},\n",
    "        'Average Winter (50th Pct PE)': {'value': pe_jan_quantiles[0.50], 'color': 'purple'},\n",
    "        'Sunny/Mild Winter (75th Pct PE)': {'value': pe_jan_quantiles[0.75], 'color': 'orange'}\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 9))\n",
    "    for scenario_name, props in scenarios_jan.items():\n",
    "        pred_df_scenario2 = pd.DataFrame(median_values, index=range(100))\n",
    "        pred_df_scenario2['temperature_May'] = temp_may_range\n",
    "        pred_df_scenario2['potential_evaporation_Jan'] = props['value']\n",
    "        \n",
    "        preds2 = final_model.get_prediction(pred_df_scenario2).summary_frame(alpha=0.05)\n",
    "        yield_predicted2 = preds2['mean']\n",
    "        yield_change_pct2 = ((yield_predicted2 - yield_baseline) / yield_baseline) * 100\n",
    "        \n",
    "        ax.plot(temp_may_range, yield_change_pct2, color=props['color'], linewidth=3, label=scenario_name)\n",
    "\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_title('Vulnerability to May Heat is Influenced by Winter Conditions', fontsize=18)\n",
    "    ax.set_xlabel('Average May Temperature (°C)', fontsize=14)\n",
    "    ax.set_ylabel('Yield Change vs. Typical Year (%)', fontsize=14)\n",
    "    ax.legend(title='January PE Scenario')\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93506a4",
   "metadata": {},
   "source": [
    "### **Analysis of the Final Winter Wheat Vulnerability Curves**\n",
    "\n",
    "These two vulnerability curves are the ultimate output of our winter wheat analysis. They translate the model's complex statistical interactions into clear, quantitative measures of compounding climate risk.\n",
    "\n",
    "#### **Plot 1: Vulnerability to May Heat is Compounded by March Temperature**\n",
    "\n",
    "*   **Primary Finding:** A warm March \"pre-stresses\" the plant, making it significantly more vulnerable to subsequent heat stress in May.\n",
    "*   **Interpretation:** This plot visualizes the powerful \"compounding heat stress\" interaction.\n",
    "    *   The **red line (\"Warm March\")** shows a high-risk, high-reward scenario. While it predicts the highest potential yield gains if May is very cool, it also shows the most catastrophic yield decline as May gets hotter, with losses exceeding **-20%** in a hot May. This suggests a warm March forces the plant into a fragile state.\n",
    "    *   The **blue line (\"Cold March\")** represents a more resilient plant. While its peak yield potential is lower, the curve is much flatter. This visually proves that a proper cold start to spring helps the plant withstand heat stress during the critical flowering period in May, resulting in more stable yields.\n",
    "\n",
    "#### **Plot 2: Vulnerability to May Heat is Influenced by Winter Conditions**\n",
    "\n",
    "*   **Primary Finding:** A \"proper\" cold and cloudy winter is essential for maximizing yield potential, but a mild, sunny winter leads to chronically lower yields.\n",
    "*   **Interpretation:** This plot reveals the critical role of winter conditions in setting the stage for the entire season.\n",
    "    *   The **gray line (\"Cloudy/Cold Winter\")** shows the highest yield potential, with gains of over **+125%** possible in a very cool May. This is strong evidence for the importance of **vernalization** (a cold period) for winter wheat's reproductive success.\n",
    "    *   Conversely, the **orange line (\"Sunny/Mild Winter\")** is the lowest curve across almost all conditions. This suggests that an insufficient cold signal during winter compromises the plant's yield potential from the very beginning, leading to consistently below-average outcomes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1badcdaa",
   "metadata": {},
   "source": [
    "### **Final Winter Wheat Model: Interpretation and Conclusions**\n",
    "\n",
    "This section summarizes the final champion model developed to explain the relationship between monthly climate stressors and winter wheat yield in Northern Italy. The model is the result of a multi-step workflow designed to be statistically robust, parsimonious, and highly insightful.\n",
    "\n",
    "#### **The Final Champion Model**\n",
    "\n",
    "After a data-driven process of variable selection and extensive iterative refinement, the final, best-performing model was determined to be a Gamma GLM with a complex structure of multiple non-linearities and interactions:\n",
    "\n",
    "**Final Model Formula:**\n",
    "```\n",
    "yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                     temperature_May + I(temperature_May**2) + \n",
    "                     temperature_Nov + temperature_Mar + I(temperature_Mar**2) + \n",
    "                     potential_evaporation_Jan + potential_evaporation_Apr +\n",
    "                     temperature_Mar:temperature_May +\n",
    "                     potential_evaporation_Jan:temperature_May\n",
    "```\n",
    "\n",
    "**Key Performance Metrics:**\n",
    "*   **Akaike Information Criterion (AIC):** `2476.85` (The lowest of all tested models)\n",
    "*   **Pseudo R-squared (CS):** `0.9101` (Explains approx. **91%** of the variation in yield)\n",
    "\n",
    "#### **The Modeling Journey: How We Arrived Here**\n",
    "\n",
    "The final model was the product of a systematic, evidence-based process:\n",
    "\n",
    "1.  **Variable Selection:** An **Elastic Net regularization** identified a broad set of robust predictors spanning the entire November-June growing season, suggesting a more complex set of drivers than for summer crops.\n",
    "\n",
    "2.  **Model Refinement (Parsimony):** A strategic base model was built, and non-significant spline components were correctly handled according to the Rule of Hierarchy.\n",
    "\n",
    "3.  **Testing for Non-Linearity:** Guided by strong evidence from our EDA, we systematically tested for non-linear effects. Adding quadratic terms for `temperature_May`, `temperature_Nov`, and `temperature_Mar` all resulted in **massive, successive drops in the AIC**, confirming that multiple, distinct \"optimal\" temperature conditions are critical for winter wheat.\n",
    "\n",
    "4.  **Testing for Interactions:** We tested our most plausible, theory-driven interaction hypotheses. This was the key to unlocking the model's full power. The data revealed two hugely significant interactions: **`temperature_Mar:temperature_May`** (compounding spring heat stress) and **`potential_evaporation_Jan:temperature_May`** (winter conditions interacting with spring stress).\n",
    "\n",
    "This structured process ensures our final model is not overfit and that its high complexity is justified by exceptionally strong statistical evidence.\n",
    "\n",
    "#### **Detailed Interpretation of the Final Model**\n",
    "\n",
    "*   **Control Variables:**\n",
    "    *   `year`: The positive, significant coefficient confirms a strong **technological trend**.\n",
    "    *   `bs(lat, df=4)` & `bs(lon, df=4)`: The high significance of the spatial splines confirms that **geography is a dominant driver** of yield.\n",
    "\n",
    "*   **Key Climate Drivers:**\n",
    "    *   **Multiple Non-Linear Temperature Effects:** The model successfully identified distinct non-linear responses to temperature in November, March, and May, indicating that the concept of an \"optimal\" temperature is critical at multiple, separate life-cycle stages.\n",
    "    *   **Interaction 1 (Compounding Spring Heat):** The `temperature_Mar:temperature_May` interaction is a core finding. It proves that heat stress is not simply additive; it's compounding. A warm March \"pre-stresses\" the crop, making it significantly more vulnerable to the damaging effects of a subsequent hot May.\n",
    "    *   **Interaction 2 (Winter's Legacy):** The `potential_evaporation_Jan:temperature_May` interaction reveals that the plant's response to spring heat is conditional on the winter it endured. A proper cold, cloudy winter (low PE) is essential for maximizing yield potential, likely due to vernalization.\n",
    "\n",
    "#### **Insights from Visualization**\n",
    "\n",
    "*   **The Interaction Plots:** The two multi-line yield response curves were essential for understanding the model. They visually confirmed that the impact of the primary stressor (May heat) is not a fixed curve but is dynamically altered by the conditions in both March and the preceding winter.\n",
    "\n",
    "*   **The Final Vulnerability Curves:** The vulnerability plots translate these complex interactions into quantitative risk. They show that:\n",
    "    *   **Resilience vs. Vulnerability:** A \"Cold March\" builds resilience, flattening the May vulnerability curve. A \"Warm March\" induces fragility, dramatically steepening the curve and increasing yield losses from May heat.\n",
    "    *   **The Importance of Vernalization:** A \"Cloudy/Cold Winter\" is the only scenario that allows for large potential yield gains, while a \"Sunny/Mild Winter\" leads to chronically below-average yields, confirming the crop's fundamental need for a proper cold period.\n",
    "\n",
    "#### **Overall Conclusion**\n",
    "\n",
    "The model provides a powerful and deeply nuanced explanation of winter wheat vulnerability. The dominant story is one of **sequential and compounding stress**. Unlike the summer crops, winter wheat's success is not tied to a single critical month but depends on a fragile sequence of conditions: a sufficiently cold winter for vernalization, followed by a mild but not-too-warm spring to build resilience. The model's core insight is that **deviations from this sequence create compounding vulnerability**, where an overly warm March can dramatically amplify the plant's susceptibility to the primary threat of a hot May during its critical flowering stage. This highlights a complex, time-dependent climate risk profile that is unique to winter wheat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
