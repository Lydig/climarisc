This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)

# Directory Structure
```
01-EDA-Maize.ipynb/
  01-EDA-Maize.ipynb
02-Advanced-EDA.ipynb/
  02-Advanced-EDA.ipynb
03-Multi-Crop-EDA.ipynb/
  03-Multi-Crop-EDA.ipynb
04-Generate-All-Maps.ipynb/
  04-Generate-All-Maps.ipynb
06-Crop-Overlap-Analysis.ipynb/
  06-Crop-Overlap-Analysis.ipynb
07-Change-Analysis.ipynb/
  07-Change-Analysis.ipynb
08-Generate-All-Change-Maps.ipynb/
  08-Generate-All-Change-Maps.ipynb
09-Climate-Data-Acquisition.ipynb/
  09-Climate-Data-Acquisition.ipynb
09-Scatterplot-Matrix.ipynb/
  09-Scatterplot-Matrix.ipynb
10-Climate-Data-Processing.ipynb/
  10-Climate-Data-Processing.ipynb
11-API-Download-Test.ipynb/
  11-API-Download-Test.ipynb
12-First-Yield-Climate-Analysis.ipynb/
  12-First-Yield-Climate-Analysis.ipynb
13-Monthly-Data-Workflow.ipynb/
  13-Monthly-Data-Workflow.ipynb
14-Monthly-Data-Workflow-v2.ipynb/
  14-Monthly-Data-Workflow-v2.ipynb
15-Visualize-Climate-Data.ipynb/
  15-Visualize-Climate-Data.ipynb
16-Download-All-Monthly-Climate-Data.ipynb/
  16-Download-All-Monthly-Climate-Data.ipynb
17-Verify-Climate-Data-Content.ipynb/
  17-Verify-Climate-Data-Content.ipynb
18-Scouting-Report-Candidates.ipynb/
  18-Scouting-Report-Candidates.ipynb
19-Growing-Season-Definition.ipynb/
  19-Growing-Season-Definition.ipynb
20-Comprehensive-Stressor-Analysis.ipynb/
  20-Comprehensive-Stressor-Analysis.ipynb
21-Italy-Proposal-Analysis.ipynb/
  21-Italy-Proposal-Analysis.ipynb
22-China-Proposal-Analysis.ipynb/
  22-China-Proposal-Analysis.ipynb
23-Italy-Maize-Analysis.ipynb/
  23-Italy-Maize-Analysis.ipynb
24-Final-Dataset-Pipeline.ipynb/
  24-Final-Dataset-Pipeline.ipynb
25-Final-Data-Verification.ipynb/
  25-Final-Data-Verification.ipynb
26-Identify-Core-Grid-Cells.ipynb/
  26-Identify-Core-Grid-Cells.ipynb
27-italy-dataset.ipynb/
  27-italy-dataset.ipynb
28-dataset-creation-pipeline.ipynb/
  28-dataset-creation-pipeline.ipynb
29-wheat-dataset-creation-pipeline.ipynb/
  29-wheat-dataset-creation-pipeline.ipynb
30-QA-and-Metadata.ipynb/
  30-QA-and-Metadata.ipynb
31-initial-modelling.ipynb/
  31-initial-modelling.ipynb
32-Multi-Crop-EDA-Italy.ipynb/
  32-Multi-Crop-EDA-Italy.ipynb
33-Maize-Heat-Stress-Model.ipynb/
  33-Maize-Heat-Stress-Model.ipynb
34-Feature-Engineering.ipynb/
  34-Feature-Engineering.ipynb
35-Scatterplot-Matrices.ipynb/
  35-Scatterplot-Matrices.ipynb
36-glm-models.ipynb/
  36-glm-models.ipynb
37-maize-gamma-models-vol2.ipynb/
  37-maize-gamma-models-vol2.ipynb
38-maize-gamma-updated.ipynb/
  38-maize-gamma-updated.ipynb
39-rice-gamma-model.ipynb/
  39-rice-gamma-model.ipynb
40-maize-gamma-model.ipynb/
  40-maize-gamma-model.ipynb
41-soybean-gamma-model.ipynb/
  41-soybean-gamma-model.ipynb
42-wheat-spring-gamma-model.ipynb/
  42-wheat-spring-gamma-model.ipynb
43-wheat-winter-gamma-models.ipynb/
  43-wheat-winter-gamma-models.ipynb
```

# Files

## File: 01-EDA-Maize.ipynb/01-EDA-Maize.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae440cea-dc30-42b6-8935-75167f7f1452",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: Maize Yields (1981-2016)\n",
    "\n",
    "**Goal:** To perform an initial exploration of the Iizumi et al. global gridded yield dataset for maize. We want to understand its structure, key trends, and variability before building vulnerability curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4264fd-f499-4891-8503-3e4cae93ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Data Loading\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob # Used to get a list of filenames\n",
    "import re   # Used for extracting the year from the filename\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set a nice style for our plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Define the path to our data. The '*' is a wildcard that matches all year files.\n",
    "# Make sure you have downloaded the data and placed it in the 'data' folder.\n",
    "DATA_PATH = '../data/maize_major/yield_*.nc4'\n",
    "\n",
    "# --- Load Data ---\n",
    "# We use combine='nested' and concat_dim='time' because the individual files\n",
    "# do not have a coordinate that xarray can use to automatically order them.\n",
    "# This method stacks them based on the filename order.\n",
    "try:\n",
    "    ds = xr.open_mfdataset(DATA_PATH, combine='nested', concat_dim='time')\n",
    "    \n",
    "    # --- Fix the Time Coordinate ---\n",
    "    # The 'time' dimension is now just [0, 1, 2, ...]. We need to replace it with the actual years.\n",
    "    # First, get a sorted list of the file paths\n",
    "    filepaths = sorted(glob.glob(DATA_PATH))\n",
    "    # Then, extract the year (a 4-digit number) from each filename\n",
    "    years = [int(re.search(r'(\\d{4})\\.nc4$', f).group(1)) for f in filepaths]\n",
    "    \n",
    "    # Finally, assign the correct years to the 'time' coordinate\n",
    "    ds = ds.assign_coords(time=years)\n",
    "    \n",
    "    print(\"Dataset loaded successfully:\")\n",
    "    print(ds)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Data not found at path: {DATA_PATH}\")\n",
    "    print(\"Please make sure you have downloaded the data and placed it in the correct directory.\")\n",
    "except ImportError as e:\n",
    "    print(f\"IMPORT ERROR: {e}\")\n",
    "    print(\"It looks like a required library is missing. Please run 'pip install dask' in your terminal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d487316d-7231-4481-8042-1864f28182c0",
   "metadata": {},
   "source": [
    "## 1. Summary and Basic Structure\n",
    "\n",
    "Let's start with a high-level overview. What does the data contain and what does it look like on a map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a84311-1a57-4330-bdfe-db75ee9a429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Summary and Map (Corrected)\n",
    "\n",
    "# Get the specific data variable we care about (which we now know is named 'var')\n",
    "maize_yield = ds['var']\n",
    "\n",
    "# --- Create a Summary Table ---\n",
    "# xarray.DataArray doesn't have a built-in .describe() method like pandas.\n",
    "# We compute the statistics individually and then create a pandas Series for a nice display.\n",
    "print(\"--- Summary Statistics for Maize Yield (tonnes/hectare) ---\")\n",
    "\n",
    "# .load() brings the data from dask into memory, which is good before computing multiple stats.\n",
    "maize_yield.load()\n",
    "\n",
    "summary_data = {\n",
    "    'mean': maize_yield.mean().item(),\n",
    "    'std': maize_yield.std().item(),\n",
    "    'min': maize_yield.min().item(),\n",
    "    '25%': maize_yield.quantile(0.25).item(),\n",
    "    '50% (median)': maize_yield.median().item(),\n",
    "    '75%': maize_yield.quantile(0.75).item(),\n",
    "    'max': maize_yield.max().item(),\n",
    "}\n",
    "summary_series = pd.Series(summary_data)\n",
    "print(summary_series)\n",
    "\n",
    "# --- Create the Map ---\n",
    "# Create a map of the average yield over the entire period to see the main growing regions.\n",
    "# We calculate the mean along the 'time' dimension.\n",
    "# We also replace zeros with 'NaN' (Not a Number) so they don't skew the color map. This makes oceans and non-growing areas blank.\n",
    "mean_yield_map = maize_yield.mean(dim='time').where(maize_yield.mean(dim='time') > 0)\n",
    "\n",
    "# Plotting the map\n",
    "plt.figure(figsize=(12, 6))\n",
    "mean_yield_map.plot(cmap='viridis', robust=True) # robust=True handles outliers in the color scale\n",
    "plt.title('Average Maize Yield (1981-2016)')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917b7cb-0fcb-4e9f-8896-417eddbe73d7",
   "metadata": {},
   "source": [
    "## 2. Time Series Analysis\n",
    "\n",
    "How has maize yield changed over time? We'll start by looking at the global trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b210428-f313-4b58-896a-26863786fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Global Yield Trend Over Time\n",
    "\n",
    "# To get the global trend, we calculate the mean yield across space (lat, lon) for each year.\n",
    "# We can't just do a simple mean because grid cells are different sizes (smaller at the poles).\n",
    "# We must create weights based on the cosine of the latitude to account for this.\n",
    "weights = np.cos(np.deg2rad(maize_yield.lat))\n",
    "weights.name = \"weights\"\n",
    "\n",
    "# Apply the weights to our data\n",
    "weighted_yield = maize_yield.weighted(weights)\n",
    "\n",
    "# Calculate the weighted mean across the spatial dimensions (\"lon\", \"lat\") for each time step\n",
    "global_mean_yield = weighted_yield.mean(dim=(\"lon\", \"lat\"))\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "global_mean_yield.plot()\n",
    "plt.title('Global Average Maize Yield Over Time (1981-2016)')\n",
    "plt.ylabel('Yield (tonnes per hectare)')\n",
    "plt.xlabel('Year')\n",
    "plt.grid(True) # Add grid lines for easier reading\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d3d7a-3592-491e-89cc-66165596dd57",
   "metadata": {},
   "source": [
    "## 3. Regional Comparison\n",
    "\n",
    "The global average can hide important regional differences. Let's compare the yield trends in two contrasting maize-growing regions: the high-yield, temperate system of the United States versus the lower-yield, tropical system of Nigeria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45127d4-ae96-466b-b7ac-03c5ca8602b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Regional Time Series Comparison\n",
    "\n",
    "# Define the latitude and longitude bounds for our regions of interest\n",
    "# These are approximate bounding boxes for each country's main agricultural areas.\n",
    "us_bounds = {'lat': slice(25, 50), 'lon': slice(235, 295)} # Longitude in 0-360\n",
    "nigeria_bounds = {'lat': slice(4, 14), 'lon': slice(2, 15)}\n",
    "\n",
    "# Select the data for each region using the .sel() method\n",
    "us_yield = maize_yield.sel(**us_bounds)\n",
    "nigeria_yield = maize_yield.sel(**nigeria_bounds)\n",
    "\n",
    "# Calculate the weighted average yield for each region over time\n",
    "us_mean_yield = us_yield.weighted(np.cos(np.deg2rad(us_yield.lat))).mean(dim=(\"lon\", \"lat\"))\n",
    "nigeria_mean_yield = nigeria_yield.weighted(np.cos(np.deg2rad(nigeria_yield.lat))).mean(dim=(\"lon\", \"lat\"))\n",
    "\n",
    "# Plot both time series on the same graph for comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "us_mean_yield.plot(label='United States')\n",
    "nigeria_mean_yield.plot(label='Nigeria')\n",
    "plt.title('Maize Yield Comparison: US vs. Nigeria (1981-2016)')\n",
    "plt.ylabel('Yield (tonnes per hectare)')\n",
    "plt.xlabel('Year')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b76f7-8292-4593-ac03-d1414eb0c39b",
   "metadata": {},
   "source": [
    "## 4. Initial EDA Summary & Next Steps\n",
    "\n",
    "**Summary of Findings:**\n",
    "\n",
    "*   **Data Structure & Quality:** The dataset is a 3D cube (lat, lon, time) covering 1981-2016. It loaded correctly, and the spatial and temporal dimensions are as expected.\n",
    "*   **Global Trend:** The global average yield shows a strong upward trend, doubling from ~2.5 to ~5.0 t/ha. This \"technology trend\" is the dominant signal in the raw data. The year-to-year fluctuations around this trend are what we will investigate as potential climate signals.\n",
    "*   **Regional Differences:** The comparison between the US and Nigeria highlights the critical need for stratification.\n",
    "    *   The **US** shows high absolute yields and a steep growth trend but also exhibits extreme year-to-year variability, suggesting high sensitivity to shocks.\n",
    "    *   **Nigeria** shows low absolute yields and a much flatter growth trend, representing a completely different agricultural system.\n",
    "*   **Conclusion:** The EDA confirms the dataset is suitable for our project. The clear regional differences validate our plan to stratify the analysis rather than relying on global averages.\n",
    "\n",
    "**Next Steps: Moving to the Main Analysis**\n",
    "\n",
    "This EDA is now complete. Our next phase of work will be to link this yield data to climate data to build our first vulnerability curves. This will likely involve:\n",
    "1.  **Sourcing Climate Data:** Finding and downloading a gridded climate dataset (e.g., temperature and precipitation) that matches our data's spatial and temporal resolution.\n",
    "2.  **Calculating Yield Anomalies:** We will need to \"de-trend\" the yield data to remove the long-term technology signal and isolate the year-to-year variability.\n",
    "3.  **Creating our first scatter plot:** Plotting yield anomalies against a climate variable (like growing-season temperature) to look for the dose-response relationship that will become our first vulnerability curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b5fd48-ae5f-418c-a25f-0bc225f419f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319d97a-375d-4911-bdcd-094f4e82288d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 02-Advanced-EDA.ipynb/02-Advanced-EDA.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc6137b-d19b-4187-87b7-ac2a9255c4ac",
   "metadata": {},
   "source": [
    "# Advanced Exploratory Data Analysis\n",
    "\n",
    "**Goal:** To dive deeper into the maize yield dataset. We will:\n",
    "1.  Inspect all variables within the dataset.\n",
    "2.  Create Europe-centric maps.\n",
    "3.  Visualize yield changes over time with a series of maps.\n",
    "4.  Add country borders for better context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0ae7f-8416-498c-ae72-0423cd4b26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Data Loading\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# New imports for advanced mapping\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# --- Configuration ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "DATA_PATH = '../data/maize_major/yield_*.nc4'\n",
    "\n",
    "# --- Load Data ---\n",
    "# Same loading logic as before\n",
    "try:\n",
    "    ds = xr.open_mfdataset(DATA_PATH, combine='nested', concat_dim='time')\n",
    "    filepaths = sorted(glob.glob(DATA_PATH))\n",
    "    years = [int(re.search(r'(\\d{4})\\.nc4$', f).group(1)) for f in filepaths]\n",
    "    ds = ds.assign_coords(time=years)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f693532-7601-4646-a724-f889ca6117e9",
   "metadata": {},
   "source": [
    "## 1. Are there other variables in the dataset?\n",
    "\n",
    "Let's inspect the full structure of the loaded `xarray.Dataset` to see all coordinates, dimensions, and data variables it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92074e14-7777-41a1-8a7f-3827bcef1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Inspecting the Dataset Contents\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f1842-900c-4673-9e0a-06c32b14243a",
   "metadata": {},
   "source": [
    "## 2. Creating a Better Map: Europe-Centric with Borders\n",
    "\n",
    "The default map centers on the Pacific Ocean (0 longitude). We can use `cartopy` to create a new map centered on Europe and overlay country borders for better geographical context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5fc036-5f5e-4e29-ab61-37720401f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 (New, Corrected): Europe-Centric Map with Improved Styling\n",
    "\n",
    "# Calculate the mean yield map, same as before\n",
    "maize_yield = ds['var']\n",
    "mean_yield_map = maize_yield.mean(dim='time').where(maize_yield.mean(dim='time') > 0)\n",
    "\n",
    "# --- Create the Plot ---\n",
    "# Create the figure and axes with a Cartopy projection centered on Europe\n",
    "fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree(central_longitude=10)})\n",
    "\n",
    "# --- Style the Map Background (Applying the new style) ---\n",
    "# Add a light grey land feature and very thin borders\n",
    "ax.add_feature(cfeature.LAND, edgecolor='lightgray', facecolor='#f0f0f0', linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "ax.coastlines(linewidth=0.5)\n",
    "\n",
    "# --- Plot the Data ---\n",
    "mean_yield_map.plot(\n",
    "    ax=ax,\n",
    "    cmap='viridis',\n",
    "    robust=True, # robust=True is fine for a single map\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cbar_kwargs={'shrink': 0.7, 'label': 'Yield (tonnes per hectare)'}\n",
    ")\n",
    "\n",
    "# Add gridlines for reference\n",
    "ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n",
    "\n",
    "ax.set_title('Average Maize Yield (1981-2016) - Europe-Centric View')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd86638-afc4-4ee6-bb09-23a8d2630521",
   "metadata": {},
   "source": [
    "## 3. Visualizing Yield Evolution: A Yearly Map Series\n",
    "\n",
    "To see the changes more clearly, let's create a series of large maps, one for each year in the dataset. We will use lighter map features to ensure the data stands out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b31f5-3e40-4bfe-9ab0-068502bbf993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 (New, Corrected): Yearly Map Series\n",
    "\n",
    "# To ensure the color scale is consistent across all maps, we'll calculate the\n",
    "# min and max yield values to use for the color bar.\n",
    "# Using the 2nd and 98th percentiles (like robust=True does) makes for a better visualization.\n",
    "# We add .compute() to force dask to actually calculate the value.\n",
    "vmin = maize_yield.quantile(0.02).compute().item()\n",
    "vmax = maize_yield.quantile(0.98).compute().item()\n",
    "\n",
    "# Loop through each year in the dataset's time coordinate\n",
    "for year in ds['time'].values:\n",
    "    \n",
    "    # Select the data for the current year in the loop\n",
    "    yield_for_one_year = maize_yield.sel(time=year).where(maize_yield.sel(time=year) > 0)\n",
    "    \n",
    "    # --- Create the Plot ---\n",
    "    # We create a new figure and axes for each year\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    \n",
    "    # --- Style the Map Background (Your Request) ---\n",
    "    # Add a light grey land feature and very thin borders\n",
    "    ax.add_feature(cfeature.LAND, edgecolor='lightgray', facecolor='#f0f0f0', linewidth=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "    ax.coastlines(linewidth=0.5)\n",
    "\n",
    "    # --- Plot the Data ---\n",
    "    yield_for_one_year.plot(\n",
    "        ax=ax,\n",
    "        cmap='viridis',\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        vmin=vmin,  # Use the consistent min/max for the color scale\n",
    "        vmax=vmax,\n",
    "        cbar_kwargs={'shrink': 0.7, 'label': 'Yield (tonnes per hectare)'}\n",
    "    )\n",
    "    \n",
    "    # Add a title for the specific year\n",
    "    ax.set_title(f'Maize Yield in {year}')\n",
    "    \n",
    "    # Show the plot for the current year before moving to the next\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf6800-bf93-4c2a-9b26-f4f9ecaf4ad3",
   "metadata": {},
   "source": [
    "## 4. Saving Key Visualizations\n",
    "\n",
    "Let's save our main plots as high-resolution files in the `reports/` directory so we can use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9a36a-aea1-4653-ae7e-a101f462317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Save the Average Yield Map\n",
    "\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the output directory for our figures\n",
    "output_dir = '../reports/figures/'\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Recreate the Plot (to make this cell self-contained) ---\n",
    "# Create the figure and axes with a Cartopy projection centered on Europe\n",
    "fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree(central_longitude=10)})\n",
    "\n",
    "# Style the map background\n",
    "ax.add_feature(cfeature.LAND, edgecolor='lightgray', facecolor='#f0f0f0', linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "ax.coastlines(linewidth=0.5)\n",
    "\n",
    "# Plot the data\n",
    "mean_yield_map.plot(\n",
    "    ax=ax,\n",
    "    cmap='viridis',\n",
    "    robust=True,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cbar_kwargs={'shrink': 0.7, 'label': 'Yield (tonnes per hectare)'}\n",
    ")\n",
    "\n",
    "# Add gridlines and title\n",
    "ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n",
    "ax.set_title('Average Maize Yield (1981-2016) - Europe-Centric View')\n",
    "\n",
    "# --- Save the Figure ---\n",
    "file_path = os.path.join(output_dir, 'avg_maize_yield_1981-2016.png')\n",
    "plt.savefig(file_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"Saved average map to: {file_path}\")\n",
    "\n",
    "# Display the plot in the notebook as well\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e2b95f-2757-4d6e-8cd8-70948debce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Save All Yearly Maps\n",
    "\n",
    "# The output directory should already exist from the previous cell, but we'll be safe.\n",
    "output_dir = '../reports/figures/yearly_maize/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Use the same consistent color scale as before\n",
    "vmin = maize_yield.quantile(0.02).compute().item()\n",
    "vmax = maize_yield.quantile(0.98).compute().item()\n",
    "\n",
    "print(f\"Starting to save 36 yearly maps to '{output_dir}'...\")\n",
    "\n",
    "# Loop through each year in the dataset\n",
    "for year in ds['time'].values:\n",
    "    \n",
    "    # Select the data for the current year\n",
    "    yield_for_one_year = maize_yield.sel(time=year).where(maize_yield.sel(time=year) > 0)\n",
    "    \n",
    "    # Create a new figure and axes for each plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    \n",
    "    # Style the map background\n",
    "    ax.add_feature(cfeature.LAND, edgecolor='lightgray', facecolor='#f0f0f0', linewidth=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "    ax.coastlines(linewidth=0.5)\n",
    "\n",
    "    # Plot the data\n",
    "    yield_for_one_year.plot(\n",
    "        ax=ax,\n",
    "        cmap='viridis',\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cbar_kwargs={'shrink': 0.7, 'label': 'Yield (tonnes per hectare)'}\n",
    "    )\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title(f'Maize Yield in {year}')\n",
    "    \n",
    "    # --- Save the Figure for this year ---\n",
    "    file_path = os.path.join(output_dir, f'maize_yield_{year}.png')\n",
    "    plt.savefig(file_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # We close the plot to prevent it from displaying in the notebook output\n",
    "    # This keeps the notebook clean and prevents it from getting too long.\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Finished saving all yearly maps.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 03-Multi-Crop-EDA.ipynb/03-Multi-Crop-EDA.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad8cf94-8ae9-40dd-a9ab-a97c5d9e8b89",
   "metadata": {},
   "source": [
    "# Multi-Crop Exploratory Data Analysis\n",
    "\n",
    "**Goal:** To get a quick overview of all available crop datasets (maize, rice, wheat, soybean, and their seasonal variations). We will create a reusable function to generate a standard report (summary table, time-series plot, and average map) for any given crop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f15358-8230-4d99-a2ce-a04feaa38821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and the Main Exploration Function (Corrected)\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import re\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "\n",
    "def explore_crop_dataset(crop_name: str):\n",
    "    \"\"\"\n",
    "    Loads a crop dataset, prints a summary, and plots its global trend and average map.\n",
    "\n",
    "    Args:\n",
    "        crop_name (str): The name of the folder in the /data directory (e.g., 'wheat_winter').\n",
    "    \"\"\"\n",
    "    print(f\"--- Exploring Dataset: {crop_name.upper()} ---\")\n",
    "    \n",
    "    # --- 1. Load Data ---\n",
    "    data_path = f'../data/{crop_name}/yield_*.nc4'\n",
    "    try:\n",
    "        # Use the same robust loading logic as before\n",
    "        ds = xr.open_mfdataset(data_path, combine='nested', concat_dim='time')\n",
    "        filepaths = sorted(glob.glob(data_path))\n",
    "        years = [int(re.search(r'(\\d{4})\\.nc4$', f).group(1)) for f in filepaths]\n",
    "        ds = ds.assign_coords(time=years)\n",
    "        yield_data = ds['var']\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load data for '{crop_name}'. Reason: {e}\")\n",
    "        return # Stop the function if data can't be loaded\n",
    "\n",
    "    # --- 2. Summary Statistics ---\n",
    "    print(\"\\n--- Summary Statistics (tonnes/hectare) ---\")\n",
    "    summary_data = {\n",
    "        'mean': yield_data.mean().compute().item(),\n",
    "        'std': yield_data.std().compute().item(),\n",
    "        'min': yield_data.min().compute().item(),\n",
    "        # CORRECTED LINE: Use quantile(0.5) instead of median()\n",
    "        '50% (median)': yield_data.quantile(0.5).compute().item(),\n",
    "        'max': yield_data.max().compute().item(),\n",
    "    }\n",
    "    print(pd.Series(summary_data))\n",
    "\n",
    "    # --- 3. Global Time Series Plot ---\n",
    "    weights = np.cos(np.deg2rad(yield_data.lat))\n",
    "    weighted_yield = yield_data.weighted(weights)\n",
    "    global_mean_yield = weighted_yield.mean(dim=(\"lon\", \"lat\"))\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    global_mean_yield.plot()\n",
    "    plt.title(f'Global Average Yield Over Time: {crop_name}')\n",
    "    plt.ylabel('Yield (tonnes per hectare)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # --- 4. Average Yield Map ---\n",
    "    mean_yield_map = yield_data.mean(dim='time').where(yield_data.mean(dim='time') > 0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 7), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.add_feature(cfeature.LAND, edgecolor='lightgray', facecolor='#f0f0f0', linewidth=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "    ax.coastlines(linewidth=0.5)\n",
    "\n",
    "    mean_yield_map.plot(ax=ax, cmap='viridis', robust=True, transform=ccrs.PlateCarree(), cbar_kwargs={'shrink': 0.7})\n",
    "    ax.set_title(f'Average Yield Map (1981-2016): {crop_name}')\n",
    "    plt.show()\n",
    "    print(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bfcc4e-fcb3-48f0-b527-03cf39420bdc",
   "metadata": {},
   "source": [
    "## Exploring the Different Crop Datasets\n",
    "\n",
    "Now we can use our `explore_crop_dataset` function to look at any crop we are interested in. Let's compare winter wheat and spring wheat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5010b5-e513-43a0-80f1-dec144690795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Exploring Wheat Varieties\n",
    "explore_crop_dataset('wheat_winter')\n",
    "explore_crop_dataset('wheat_spring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5584850-1262-4365-9b9a-7bdb7ea0821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_crop_dataset('rice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813fb07f-6ab6-4565-95cf-9ce9f8837d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 04-Generate-All-Maps.ipynb/04-Generate-All-Maps.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ab36b9-e5bf-4d57-8301-2280378432fb",
   "metadata": {},
   "source": [
    "# Generate and Save All Crop Maps\n",
    "\n",
    "**Goal:** This notebook automates the process of creating and saving high-quality maps for every available crop dataset. It will generate:\n",
    "1.  A single map of the average yield (1981-2016) for each crop.\n",
    "2.  A full series of yearly maps (1981-2016) for each crop.\n",
    "\n",
    "All files will be saved in the `reports/figures/` directory, organized by crop name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75824b57-d2cc-4347-8add-b216ff1f4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: The Complete Map Generation Script\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "\n",
    "def save_all_maps_for_crop(crop_name: str, data_dir: str = '../data/', output_dir: str = '../reports/figures/'):\n",
    "    \"\"\"\n",
    "    Loads a crop dataset, then generates and saves its average map and all yearly maps.\n",
    "\n",
    "    Args:\n",
    "        crop_name (str): The name of the folder in the data directory (e.g., 'wheat_winter').\n",
    "        data_dir (str): Path to the main data folder.\n",
    "        output_dir (str): Path to the main output folder for figures.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing Dataset: {crop_name.upper()} ---\")\n",
    "    \n",
    "    # --- 1. Load Data ---\n",
    "    data_path = os.path.join(data_dir, crop_name, 'yield_*.nc4')\n",
    "    try:\n",
    "        ds = xr.open_mfdataset(data_path, combine='nested', concat_dim='time')\n",
    "        filepaths = sorted(glob.glob(data_path))\n",
    "        years = [int(re.search(r'(\\d{4})\\.nc4$', f).group(1)) for f in filepaths]\n",
    "        ds = ds.assign_coords(time=years)\n",
    "        yield_data = ds['var']\n",
    "        print(f\"Loaded {len(years)} years of data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load data for '{crop_name}'. Skipping. Reason: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Create Output Directories ---\n",
    "    crop_output_dir = os.path.join(output_dir, crop_name)\n",
    "    yearly_output_dir = os.path.join(crop_output_dir, 'yearly')\n",
    "    os.makedirs(yearly_output_dir, exist_ok=True) # Creates both parent and child directories\n",
    "\n",
    "    # --- 3. Generate and Save Average Map ---\n",
    "    print(\"Generating average map...\")\n",
    "    mean_yield_map = yield_data.mean(dim='time').where(yield_data.mean(dim='time') > 0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.add_feature(cfeature.LAND, edgecolor='gray', facecolor='#f0f0f0', linewidth=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "    ax.coastlines(linewidth=0.5)\n",
    "    \n",
    "    mean_yield_map.plot(ax=ax, cmap='viridis', robust=True, transform=ccrs.PlateCarree(), cbar_kwargs={'shrink': 0.7})\n",
    "    ax.set_title(f'Average Yield Map (1981-2016): {crop_name}')\n",
    "    \n",
    "    avg_file_path = os.path.join(crop_output_dir, f'average_yield_{crop_name}.png')\n",
    "    plt.savefig(avg_file_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved average map to {avg_file_path}\")\n",
    "\n",
    "    # --- 4. Generate and Save Yearly Maps ---\n",
    "    print(\"Generating yearly maps...\")\n",
    "    vmin = yield_data.quantile(0.02).compute().item()\n",
    "    vmax = yield_data.quantile(0.98).compute().item()\n",
    "\n",
    "    for year in ds['time'].values:\n",
    "        yield_for_one_year = yield_data.sel(time=year).where(yield_data.sel(time=year) > 0)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "        ax.add_feature(cfeature.LAND, edgecolor='gray', facecolor='#f0f0f0', linewidth=0.5)\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "        ax.coastlines(linewidth=0.5)\n",
    "        \n",
    "        yield_for_one_year.plot(ax=ax, cmap='viridis', transform=ccrs.PlateCarree(), vmin=vmin, vmax=vmax, cbar_kwargs={'shrink': 0.7})\n",
    "        ax.set_title(f'Yield in {year}: {crop_name}')\n",
    "        \n",
    "        yearly_file_path = os.path.join(yearly_output_dir, f'yield_{crop_name}_{year}.png')\n",
    "        plt.savefig(yearly_file_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "    print(f\"Finished saving yearly maps for {crop_name}.\")\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Automatically find all the crop folders in the data directory\n",
    "base_data_dir = '../data/'\n",
    "crop_folders = [d for d in os.listdir(base_data_dir) if os.path.isdir(os.path.join(base_data_dir, d))]\n",
    "\n",
    "print(f\"Found {len(crop_folders)} crop datasets to process: {crop_folders}\\n\")\n",
    "\n",
    "# Loop through each folder and generate the maps\n",
    "for crop in crop_folders:\n",
    "    save_all_maps_for_crop(crop)\n",
    "    \n",
    "print(\"--- All Done! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe2ae4-7545-4482-a23a-8fac8c8a7702",
   "metadata": {},
   "source": [
    "# Analysis of Combined Crop Yields\n",
    "\n",
    "**Goal:** To create a single dataset representing the total yield of the four major crops (maize, rice, wheat, and soybean) for each grid cell and year. This will give us a macro view of overall agricultural productivity.\n",
    "\n",
    "**Methodology:**\n",
    "We will combine the top-level datasets for each crop (`maize`, `rice`, `wheat`, `soybean`). We are intentionally *not* using the seasonal datasets (e.g., `maize_major`, `maize_second`) in this sum to avoid double-counting yields in regions with multiple harvests. We will sum the yield values at each point in space and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132118e8-cbf5-4eec-a9c7-d4039dadcacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Combine, Analyze, and Save Total Yield Maps\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define which crop folders to combine. We use the main ones to avoid double-counting.\n",
    "CROPS_TO_COMBINE = ['maize', 'rice', 'wheat', 'soybean']\n",
    "DATA_DIR = '../data/'\n",
    "OUTPUT_DIR = '../reports/figures/combined_total_yield/'\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'yearly'), exist_ok=True)\n",
    "\n",
    "print(\"--- Starting Combined Yield Analysis ---\")\n",
    "\n",
    "# --- 1. Load and Combine Datasets ---\n",
    "all_crop_dataarrays = []\n",
    "for crop in CROPS_TO_COMBINE:\n",
    "    print(f\"Loading {crop}...\")\n",
    "    data_path = os.path.join(DATA_DIR, crop, 'yield_*.nc4')\n",
    "    try:\n",
    "        ds = xr.open_mfdataset(data_path, combine='nested', concat_dim='time')\n",
    "        # We only need the data variable, not the full dataset\n",
    "        all_crop_dataarrays.append(ds['var'])\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {crop}, skipping. Reason: {e}\")\n",
    "\n",
    "# Use xarray's concat and sum methods to combine them.\n",
    "# We fill any missing values (NaNs) with 0 so the sum works correctly.\n",
    "# For example, a grid cell with wheat but no maize should be treated as wheat_yield + 0.\n",
    "if all_crop_dataarrays:\n",
    "    combined_yield = xr.concat(all_crop_dataarrays, dim='crop').fillna(0).sum(dim='crop')\n",
    "    \n",
    "    # Re-assign the time coordinate, which can get lost in the process\n",
    "    years = range(1981, 2017)\n",
    "    combined_yield = combined_yield.assign_coords(time=years)\n",
    "    \n",
    "    print(\"\\nAll datasets combined successfully.\")\n",
    "else:\n",
    "    print(\"No datasets were loaded. Stopping.\")\n",
    "    # This stops the script if no data was found, preventing errors.\n",
    "    # You might need to add a 'pass' or 'raise' here depending on the desired behavior.\n",
    "\n",
    "\n",
    "# --- 2. Generate and Save the Average Combined Map ---\n",
    "print(\"Generating average combined map...\")\n",
    "mean_combined_map = combined_yield.mean(dim='time').where(combined_yield.mean(dim='time') > 0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.add_feature(cfeature.LAND, edgecolor='gray', facecolor='#f0f0f0', linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "ax.coastlines(linewidth=0.5)\n",
    "\n",
    "mean_combined_map.plot(ax=ax, cmap='viridis', robust=True, transform=ccrs.PlateCarree(), cbar_kwargs={'shrink': 0.7})\n",
    "ax.set_title('Average Combined Yield (Maize, Rice, Wheat, Soybean) 1981-2016')\n",
    "\n",
    "avg_file_path = os.path.join(OUTPUT_DIR, 'average_combined_yield.png')\n",
    "plt.savefig(avg_file_path, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "print(f\"Saved average combined map to {avg_file_path}\")\n",
    "\n",
    "# --- 3. Generate and Save Yearly Combined Maps ---\n",
    "print(\"Generating yearly combined maps...\")\n",
    "vmin = combined_yield.quantile(0.02).compute().item()\n",
    "vmax = combined_yield.quantile(0.98).compute().item()\n",
    "yearly_output_dir = os.path.join(OUTPUT_DIR, 'yearly')\n",
    "\n",
    "for year in combined_yield['time'].values:\n",
    "    yield_for_one_year = combined_yield.sel(time=year).where(combined_yield.sel(time=year) > 0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.add_feature(cfeature.LAND, edgecolor='gray', facecolor='#f0f0f0', linewidth=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "    ax.coastlines(linewidth=0.5)\n",
    "    \n",
    "    yield_for_one_year.plot(ax=ax, cmap='viridis', transform=ccrs.PlateCarree(), vmin=vmin, vmax=vmax, cbar_kwargs={'shrink': 0.7})\n",
    "    ax.set_title(f'Combined Yield in {year}')\n",
    "    \n",
    "    yearly_file_path = os.path.join(yearly_output_dir, f'combined_yield_{year}.png')\n",
    "    plt.savefig(yearly_file_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"Finished saving all yearly combined maps to {yearly_output_dir}\")\n",
    "print(\"\\n--- All Done! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 06-Crop-Overlap-Analysis.ipynb/06-Crop-Overlap-Analysis.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258e08d0-0913-48d0-ba6a-3c244b1f770f",
   "metadata": {},
   "source": [
    "# Analysis of Crop Overlap and Agricultural Systems\n",
    "\n",
    "**Goal:** To create a single map that visualizes where the world's major crops are grown, paying special attention to areas where multiple crops are cultivated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66514de-5790-46d8-9461-deff1d32d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Crop Count Map (Updated with Save Functionality)\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# --- 1. Load Average Yield Data for the Four Main Crops ---\n",
    "CROPS_TO_LOAD = ['maize', 'rice', 'wheat', 'soybean']\n",
    "DATA_DIR = '../data/'\n",
    "avg_yields = {}\n",
    "\n",
    "for crop in CROPS_TO_LOAD:\n",
    "    data_path = os.path.join(DATA_DIR, crop, 'yield_*.nc4')\n",
    "    with xr.open_mfdataset(data_path, combine='nested', concat_dim='time') as ds:\n",
    "        avg_yields[crop] = ds['var'].mean(dim='time').compute()\n",
    "        print(f\"Loaded and processed average yield for {crop}.\")\n",
    "\n",
    "# --- 2. Create the Crop Count Map ---\n",
    "crop_count = xr.zeros_like(avg_yields['maize'])\n",
    "for crop in CROPS_TO_LOAD:\n",
    "    crop_count = crop_count + xr.where(avg_yields[crop] > 0, 1, 0)\n",
    "\n",
    "# --- 3. Plot the Crop Count Map with Custom Colors ---\n",
    "fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "land_color = '#f0f0f0'\n",
    "ax.add_feature(cfeature.LAND, facecolor=land_color)\n",
    "ax.coastlines(linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "\n",
    "# --- Your Custom Color Scheme ---\n",
    "count_colors = ['#fdbe85', '#fd8d3c', '#e6550d', '#a63603']\n",
    "cmap = plt.matplotlib.colors.ListedColormap(count_colors)\n",
    "\n",
    "# Plot the data\n",
    "crop_count.where(crop_count > 0).plot(\n",
    "    ax=ax,\n",
    "    cmap=cmap,\n",
    "    levels=[0.5, 1.5, 2.5, 3.5, 4.5],\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    add_colorbar=False\n",
    ")\n",
    "\n",
    "# Create Custom Legend\n",
    "legend_labels = {\n",
    "    'No Data / No Crops': land_color,\n",
    "    '1 Crop': count_colors[0],\n",
    "    '2 Crops': count_colors[1],\n",
    "    '3 Crops': count_colors[2],\n",
    "    '4 Crops': count_colors[3]\n",
    "}\n",
    "legend_patches = [mpatches.Patch(color=color, label=label) for label, color in legend_labels.items()]\n",
    "ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "ax.set_title('Global Count of Major Crops per Grid Cell')\n",
    "\n",
    "# --- 4. Save the Figure ---\n",
    "output_dir = '../reports/figures/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "file_path = os.path.join(output_dir, 'global_crop_count_map.png')\n",
    "plt.savefig(file_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nSaved crop count map to: {file_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cddef-5d24-4e3c-bc0c-5d731e69dc89",
   "metadata": {},
   "source": [
    "## Mapping Major Agricultural Systems\n",
    "\n",
    "A crop count is useful, but it doesn't tell us *which* crops are grown together. Here, we create a categorical map to identify some of the world's most significant agricultural systems:\n",
    "- **Maize-Soybean System:** The classic rotation in the Americas.\n",
    "- **Rice-Wheat System:** A critical system in South and East Asia.\n",
    "- **Other combinations and single crops.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd387b8-790d-4da3-af87-1a606393086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Major Agricultural Systems Map (Corrected Logic v2)\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# --- 1. Create Boolean Masks and Crop Count (same as before) ---\n",
    "maize_present = avg_yields['maize'] > 0\n",
    "rice_present = avg_yields['rice'] > 0\n",
    "wheat_present = avg_yields['wheat'] > 0\n",
    "soybean_present = avg_yields['soybean'] > 0\n",
    "crop_count = (maize_present.astype(int) + rice_present.astype(int) + wheat_present.astype(int) + soybean_present.astype(int))\n",
    "\n",
    "# --- 2. Define Mutually Exclusive Categories ---\n",
    "# This new logic ensures that a grid cell can only belong to one category.\n",
    "# We check for the most specific cases first.\n",
    "\n",
    "# Start with a grid of zeros (No Data / No Crops)\n",
    "systems_map = xr.zeros_like(avg_yields['maize'], dtype=int)\n",
    "\n",
    "# CATEGORY 7: Other Crop Mix (any combination that isn't one of our specific systems)\n",
    "# We start by assigning this to any cell with 2 or more crops.\n",
    "systems_map = xr.where(crop_count >= 2, 7, systems_map)\n",
    "\n",
    "# CATEGORY 6: Rice-Wheat System (ONLY Rice and Wheat)\n",
    "# This condition is now very strict.\n",
    "systems_map = xr.where((rice_present & wheat_present) & (crop_count == 2), 6, systems_map)\n",
    "\n",
    "# CATEGORY 5: Maize-Soybean System (ONLY Maize and Soybean)\n",
    "# This condition is also now very strict.\n",
    "systems_map = xr.where((maize_present & soybean_present) & (crop_count == 2), 5, systems_map)\n",
    "\n",
    "# CATEGORY 1-4: Single Crops (where crop count is exactly 1)\n",
    "systems_map = xr.where(rice_present & (crop_count == 1), 4, systems_map)\n",
    "systems_map = xr.where(wheat_present & (crop_count == 1), 3, systems_map)\n",
    "systems_map = xr.where(soybean_present & (crop_count == 1), 2, systems_map)\n",
    "systems_map = xr.where(maize_present & (crop_count == 1), 1, systems_map)\n",
    "\n",
    "\n",
    "# --- 3. Create a Custom Colormap and Legend (same as before) ---\n",
    "category_labels = {\n",
    "    'No Data / No Crops': '#f0f0f0',\n",
    "    'Maize Only': '#8dd3c7',\n",
    "    'Soybean Only': '#ffffb3',\n",
    "    'Wheat Only': '#bebada',\n",
    "    'Rice Only': '#fb8072',\n",
    "    'Maize-Soybean System': '#80b1d3',\n",
    "    'Rice-Wheat System': '#fdb462',\n",
    "    'Other Crop Mix': '#b3de69'\n",
    "}\n",
    "cmap_colors = [v for k, v in category_labels.items() if k != 'No Data / No Crops']\n",
    "cmap = plt.matplotlib.colors.ListedColormap(cmap_colors)\n",
    "\n",
    "# --- 4. Plot the Map (same as before) ---\n",
    "fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.add_feature(cfeature.LAND, facecolor=category_labels['No Data / No Crops'])\n",
    "ax.coastlines(linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "\n",
    "systems_map.where(systems_map > 0).plot(ax=ax, cmap=cmap, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "\n",
    "# Create a dynamic legend that only shows categories present in the map\n",
    "unique_cats = np.unique(systems_map)\n",
    "legend_patches = [mpatches.Patch(color=v, label=k) for k, v in category_labels.items() if (list(category_labels.values()).index(v) in unique_cats)]\n",
    "ax.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "ax.set_title('Major Global Agricultural Systems')\n",
    "\n",
    "# --- 5. Save the Figure (same as before) ---\n",
    "output_dir = '../reports/figures/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "file_path = os.path.join(output_dir, 'global_ag_systems_map.png')\n",
    "plt.savefig(file_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nSaved agricultural systems map to: {file_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557842ad-188d-4feb-ae3c-f5e3b9d770bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 07-Change-Analysis.ipynb/07-Change-Analysis.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332c44c5-2d0a-4b84-885d-c63a49cdd03a",
   "metadata": {},
   "source": [
    "# Analysis of Change in Crop Yield and Variability\n",
    "\n",
    "**Goal:** To create maps that visualize how both the average yield and the year-to-year variability of maize have changed between the early and late periods of our dataset (1981-2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc852b-071d-431d-8a50-bd8e07036b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Map of Change in Average Yield\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "# We'll focus on maize for this example\n",
    "DATA_PATH = '../data/maize/yield_*.nc4'\n",
    "with xr.open_mfdataset(DATA_PATH, combine='nested', concat_dim='time') as ds:\n",
    "    years = range(1981, 2017)\n",
    "    ds = ds.assign_coords(time=years)\n",
    "    maize_yield = ds['var'].compute() # Load into memory for faster calculations\n",
    "    print(\"Maize data loaded.\")\n",
    "\n",
    "# --- 2. Define Time Periods and Calculate Change ---\n",
    "# Define the first and last 10 years of the dataset\n",
    "early_period = slice('1981', '1990')\n",
    "late_period = slice('2007', '2016')\n",
    "\n",
    "# Calculate the mean yield for each period\n",
    "yield_early = maize_yield.sel(time=early_period).mean(dim='time')\n",
    "yield_late = maize_yield.sel(time=late_period).mean(dim='time')\n",
    "\n",
    "# Calculate the percentage change\n",
    "# We use xr.where to avoid division by zero in areas with no crops\n",
    "percent_change = xr.where(\n",
    "    yield_early > 0,\n",
    "    100 * (yield_late - yield_early) / yield_early,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# --- 3. Plot the Change Map ---\n",
    "fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.add_feature(cfeature.LAND, facecolor='#f0f0f0')\n",
    "ax.coastlines(linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "\n",
    "# Use a diverging colormap: blue for increase, red for decrease, white for no change\n",
    "# We center the colormap on zero.\n",
    "cmap = 'RdBu_r'\n",
    "vmax = 100 # Cap the color scale at 100% change for better visualization\n",
    "\n",
    "percent_change.plot(\n",
    "    ax=ax,\n",
    "    cmap=cmap,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    vmax=vmax,\n",
    "    center=0, # This is crucial for a diverging colormap\n",
    "    cbar_kwargs={'label': 'Percent Change in Average Yield (%)'}\n",
    ")\n",
    "\n",
    "ax.set_title('Change in Average Maize Yield (2007-2016 vs 1981-1990)')\n",
    "\n",
    "# --- 4. Save the Figure ---\n",
    "output_dir = '../reports/figures/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "file_path = os.path.join(output_dir, 'maize_yield_change_map.png')\n",
    "plt.savefig(file_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved yield change map to: {file_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb14b7b-2817-4293-9eec-c22049cd6594",
   "metadata": {},
   "source": [
    "## Change in Yield Variability (Stability)\n",
    "\n",
    "Now we'll investigate if yields have become more or less stable over time. We'll calculate the standard deviation of yield for the first and second halves of the dataset and map the percentage change between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035caa22-3bb2-4c38-ab25-fc6731cfd716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Map of Change in Yield Variability\n",
    "\n",
    "# --- 1. Define Time Periods and Calculate Variability ---\n",
    "# Split the dataset into two halves\n",
    "first_half = slice('1981', '1998')\n",
    "second_half = slice('1999', '2016')\n",
    "\n",
    "# Calculate the standard deviation (a measure of variability) for each half\n",
    "variability_first = maize_yield.sel(time=first_half).std(dim='time')\n",
    "variability_second = maize_yield.sel(time=second_half).std(dim='time')\n",
    "\n",
    "# Calculate the percentage change in variability\n",
    "percent_change_variability = xr.where(\n",
    "    variability_first > 0.1, # Use a small threshold to avoid extreme values\n",
    "    100 * (variability_second - variability_first) / variability_first,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# --- 2. Plot the Variability Change Map ---\n",
    "fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.add_feature(cfeature.LAND, facecolor='#f0f0f0')\n",
    "ax.coastlines(linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "\n",
    "# Use a diverging colormap. Here, red means MORE variable (less stable), blue means LESS variable (more stable).\n",
    "cmap = 'RdBu' # Note: not '_r'. Red should be positive change (more variability).\n",
    "vmax = 100 # Cap at 100% change\n",
    "\n",
    "percent_change_variability.plot(\n",
    "    ax=ax,\n",
    "    cmap=cmap,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    vmax=vmax,\n",
    "    center=0,\n",
    "    cbar_kwargs={'label': 'Percent Change in Yield Variability (%)'}\n",
    ")\n",
    "\n",
    "ax.set_title('Change in Maize Yield Variability (1999-2016 vs 1981-1998)')\n",
    "\n",
    "# --- 3. Save the Figure ---\n",
    "output_dir = '../reports/figures/'\n",
    "file_path = os.path.join(output_dir, 'maize_variability_change_map.png')\n",
    "plt.savefig(file_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved variability change map to: {file_path}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 08-Generate-All-Change-Maps.ipynb/08-Generate-All-Change-Maps.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2676d85-086d-43a2-b97b-038ad80950c1",
   "metadata": {},
   "source": [
    "# Generate All Change Maps (Yield and Variability)\n",
    "\n",
    "**Goal:** This notebook automates the creation of change maps for every crop dataset. For each dataset, it generates and saves two maps:\n",
    "1.  **Change in Average Yield:** Comparing the last decade (2007-2016) to the first (1981-1990).\n",
    "2.  **Change in Yield Variability:** Comparing the second half of the period (1999-2016) to the first (1981-1998).\n",
    "\n",
    "All files are saved into a structured folder system within `reports/figures/change_maps/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90baf551-fcf0-4778-89de-533a926052c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: The Complete Change Map Generation Script\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def generate_change_maps(yield_data: xr.DataArray, crop_name: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Generates and saves the 'change in average' and 'change in variability' maps for a given yield dataset.\n",
    "    \"\"\"\n",
    "    print(f\"--- Generating change maps for: {crop_name.upper()} ---\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # --- 1. Map of Change in Average Yield ---\n",
    "    early_period = slice('1981', '1990')\n",
    "    late_period = slice('2007', '2016')\n",
    "    yield_early = yield_data.sel(time=early_period).mean(dim='time')\n",
    "    yield_late = yield_data.sel(time=late_period).mean(dim='time')\n",
    "    percent_change = xr.where(yield_early > 0.1, 100 * (yield_late - yield_early) / yield_early, np.nan)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.add_feature(cfeature.LAND, facecolor='#f0f0f0')\n",
    "    ax.coastlines(linewidth=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "    percent_change.plot(ax=ax, cmap='RdBu_r', transform=ccrs.PlateCarree(), vmax=100, center=0, cbar_kwargs={'label': 'Percent Change in Average Yield (%)'})\n",
    "    ax.set_title(f'Change in Average Yield: {crop_name} (2007-16 vs 1981-90)')\n",
    "    \n",
    "    avg_change_path = os.path.join(output_dir, f'change_avg_yield_{crop_name}.png')\n",
    "    plt.savefig(avg_change_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved average change map to {avg_change_path}\")\n",
    "\n",
    "    # --- 2. Map of Change in Yield Variability ---\n",
    "    first_half = slice('1981', '1998')\n",
    "    second_half = slice('1999', '2016')\n",
    "    variability_first = yield_data.sel(time=first_half).std(dim='time')\n",
    "    variability_second = yield_data.sel(time=second_half).std(dim='time')\n",
    "    percent_change_variability = xr.where(variability_first > 0.1, 100 * (variability_second - variability_first) / variability_first, np.nan)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.add_feature(cfeature.LAND, facecolor='#f0f0f0')\n",
    "    ax.coastlines(linewidth=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray', linewidth=0.5)\n",
    "    percent_change_variability.plot(ax=ax, cmap='RdBu_r', transform=ccrs.PlateCarree(), vmax=100, center=0, cbar_kwargs={'label': 'Percent Change in Yield Variability (%)'})\n",
    "    ax.set_title(f'Change in Yield Variability: {crop_name} (1999-2016 vs 1981-1998)')\n",
    "\n",
    "    var_change_path = os.path.join(output_dir, f'change_variability_{crop_name}.png')\n",
    "    plt.savefig(var_change_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved variability change map to {var_change_path}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "BASE_DATA_DIR = '../data/'\n",
    "BASE_OUTPUT_DIR = '../reports/figures/change_maps/'\n",
    "\n",
    "# --- Part A: Process all individual crop folders ---\n",
    "crop_folders = [d for d in os.listdir(BASE_DATA_DIR) if os.path.isdir(os.path.join(BASE_DATA_DIR, d))]\n",
    "for crop_name in crop_folders:\n",
    "    data_path = os.path.join(BASE_DATA_DIR, crop_name, 'yield_*.nc4')\n",
    "    with xr.open_mfdataset(data_path, combine='nested', concat_dim='time') as ds:\n",
    "        years = range(1981, 2017)\n",
    "        ds = ds.assign_coords(time=years)\n",
    "        yield_data = ds['var'].compute() # .compute() loads it all into memory\n",
    "        \n",
    "        output_path = os.path.join(BASE_OUTPUT_DIR, crop_name)\n",
    "        generate_change_maps(yield_data, crop_name, output_path)\n",
    "\n",
    "# --- Part B: Process the Combined Total Yield ---\n",
    "CROPS_TO_COMBINE = ['maize', 'rice', 'wheat', 'soybean']\n",
    "all_crop_dataarrays = []\n",
    "for crop in CROPS_TO_COMBINE:\n",
    "    data_path = os.path.join(BASE_DATA_DIR, crop, 'yield_*.nc4')\n",
    "    with xr.open_mfdataset(data_path, combine='nested', concat_dim='time') as ds:\n",
    "        all_crop_dataarrays.append(ds['var'])\n",
    "\n",
    "if all_crop_dataarrays:\n",
    "    combined_yield = xr.concat(all_crop_dataarrays, dim='crop').fillna(0).sum(dim='crop').compute()\n",
    "    years = range(1981, 2017)\n",
    "    combined_yield = combined_yield.assign_coords(time=years)\n",
    "    \n",
    "    output_path = os.path.join(BASE_OUTPUT_DIR, 'combined_total_yield')\n",
    "    generate_change_maps(combined_yield, 'combined_total_yield', output_path)\n",
    "\n",
    "print(\"--- All Done! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 09-Climate-Data-Acquisition.ipynb/09-Climate-Data-Acquisition.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a6d7ca-3f80-494e-aa4d-088b580bd7f7",
   "metadata": {},
   "source": [
    "# Climate Data Acquisition: ERA5-Land\n",
    "\n",
    "**Goal:** To download our first set of climate stressor data from the ERA5-Land dataset via the CDS API.\n",
    "\n",
    "**Plan:**\n",
    "1.  Download a test sample: daily **temperature** and **precipitation** data.\n",
    "2.  Focus on a specific region: the US Midwest.\n",
    "3.  Cover our full time period: 1981-2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31aa79-819c-4f07-be88-4b3e1777934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Download Script for ERA5-Land Data (with Year-Month Loop)\n",
    "import cdsapi\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "# We'll save each month's data into a yearly folder to keep things organized.\n",
    "output_dir_base = '../data/climate_raw/usa_era5_land_hourly/'\n",
    "os.makedirs(output_dir_base, exist_ok=True)\n",
    "\n",
    "# Define the years and months we want to download\n",
    "years_to_download = range(1981, 2017)\n",
    "months_to_download = range(1, 13)\n",
    "\n",
    "# --- API Request Nested Loop ---\n",
    "c = cdsapi.Client()\n",
    "\n",
    "print(f\"Starting download for {len(years_to_download)} years and {len(months_to_download)} months per year.\")\n",
    "print(\"This will make a total of {} requests.\".format(len(years_to_download) * len(months_to_download)))\n",
    "\n",
    "for year in years_to_download:\n",
    "    for month in months_to_download:\n",
    "        # Create a yearly sub-folder\n",
    "        year_dir = os.path.join(output_dir_base, str(year))\n",
    "        os.makedirs(year_dir, exist_ok=True)\n",
    "        \n",
    "        # Define a unique output file for each year and month\n",
    "        output_file = os.path.join(year_dir, f'era5_land_usa_{year}_{month:02d}.nc')\n",
    "        \n",
    "        # Check if the file already exists to avoid re-downloading\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"File for {year}-{month:02d} already exists. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nSubmitting request for: {year}-{month:02d}\")\n",
    "        \n",
    "        try:\n",
    "            request_dictionary = {\n",
    "                'format': 'netcdf',\n",
    "                'variable': [\n",
    "                    '2m_temperature', 'total_precipitation',\n",
    "                ],\n",
    "                'year': str(year),\n",
    "                'month': f'{month:02d}', # Request only the current month\n",
    "                'day': [f'{d:02d}' for d in range(1, 32)],\n",
    "                'time': [f'{h:02d}:00' for h in range(0, 24)],\n",
    "                'area': [50, -105, 25, -80], # North, West, South, East\n",
    "            }\n",
    "\n",
    "            c.retrieve(\n",
    "                'reanalysis-era5-land',\n",
    "                request_dictionary,\n",
    "                output_file\n",
    "            )\n",
    "            print(f\"Download for {year}-{month:02d} complete!\")\n",
    "            \n",
    "            # A small polite pause between requests\n",
    "            time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn error occurred for {year}-{month:02d}: {e}\")\n",
    "            print(\"The script will continue with the next month.\")\n",
    "            continue\n",
    "\n",
    "print(\"\\n--- All downloads finished! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 09-Scatterplot-Matrix.ipynb/09-Scatterplot-Matrix.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32819c0-cb68-4796-a974-449f24d63fa1",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis 1: Crop Yields**\n",
    "Our first goal is to explore the relationships between the yields of three of the primary crops: maize, soybean, and wheat. We will load the data for each, combine them into a single dataset based on their geographic location (latitude and longitude), and then generate a scatterplot matrix to visualize how the yields of these crops correlate with one another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd77b79-9dae-400f-8a78-2cc52f7ad6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Optional: Set a nice style for the plots\n",
    "sns.set_theme(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e16713a-2cc3-46ac-bb89-638bf699af5c",
   "metadata": {},
   "source": [
    "## **Step 1: Load and Prepare the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea701cfe-669a-4172-b348-92a976a17a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_crop_data(path_pattern):\n",
    "    \"\"\"\n",
    "    Loads, cleans, and prepares a single crop's data for analysis.\n",
    "    \"\"\"\n",
    "    filepaths = sorted(glob.glob(path_pattern))\n",
    "    if not filepaths: return None\n",
    "        \n",
    "    ds = xr.open_mfdataset(filepaths, combine='nested', concat_dim='time')\n",
    "    years = [int(re.search(r'(\\d{4})\\.nc4$', f).group(1)) for f in filepaths]\n",
    "    ds = ds.assign_coords(time=years)\n",
    "    mean_ds = ds.mean(dim='time')\n",
    "    df = mean_ds.to_dataframe().reset_index()\n",
    "    df = df.dropna(subset=['var'])\n",
    "    return df\n",
    "\n",
    "print(\"Setup complete. Helper function is ready.\")\n",
    "\n",
    "# Define the relative path to your desired output folder\n",
    "output_dir = '../reports/figures/scatterplot_matrices/'\n",
    "\n",
    "# Create the directory if it doesn't already exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Setup complete. Figures will be saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244f594-3243-4349-b327-9e4bf56f7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 (Revised): Load, Merge, and Plot a CONTROLLED Matrix\n",
    "\n",
    "# --- Configuration ---\n",
    "crops_to_compare = {\n",
    "    'maize':   '../data/maize/*.nc4',\n",
    "    'soybean': '../data/soybean/*.nc4',\n",
    "    'wheat':   '../data/wheat/*.nc4'\n",
    "}\n",
    "\n",
    "# --- Load the data for the selected crops ---\n",
    "crop_dfs = []\n",
    "for crop, path in crops_to_compare.items():\n",
    "    print(f\"Loading data for {crop}...\")\n",
    "    df = load_and_clean_crop_data(path)\n",
    "    if df is not None:\n",
    "        df = df.rename(columns={'var': f'{crop}_yield'})\n",
    "        crop_dfs.append(df[['lat', 'lon', f'{crop}_yield']])\n",
    "\n",
    "# --- Merge the DataFrames ---\n",
    "if crop_dfs:\n",
    "    merged_df = crop_dfs[0]\n",
    "    for i in range(1, len(crop_dfs)):\n",
    "        merged_df = pd.merge(merged_df, crop_dfs[i], on=['lat', 'lon'], how='inner')\n",
    "\n",
    "    print(f\"\\nFound {len(merged_df)} locations where all selected crops are grown.\")\n",
    "    print(\"Generating focused scatterplot matrix...\")\n",
    "\n",
    "    # --- THE FIX IS HERE ---\n",
    "    # 1. Define the specific columns and their desired order for the plot.\n",
    "    #    This excludes 'lat' and 'lon'.\n",
    "    yield_columns = ['maize_yield', 'soybean_yield', 'wheat_yield']\n",
    "\n",
    "    # 2. Use the 'vars' parameter to tell pairplot exactly what to plot.\n",
    "    g = sns.pairplot(merged_df, vars=yield_columns, corner=True)\n",
    "    \n",
    "    g.fig.suptitle(\"Yield Relationships for Co-located Crops (Maize, Soybean, Wheat)\", y=1.02)\n",
    "    plt.show()\n",
    "    g.savefig(os.path.join(output_dir, 'high_overlap_maize_soy_wheat.png'), dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    print(\"No data was loaded. Please check your file paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a529f-66ae-4ef2-9d20-d9a4ae4a1296",
   "metadata": {},
   "source": [
    "## Analysis of the Scatterplot Matrix\n",
    "This scatterplot matrix provides a powerful overview of the average yields for Maize, Soybean, and Wheat in locations where all three are grown. It allows us to explore both the individual characteristics of each crop's yield and the relationships between them.\n",
    "\n",
    "### How to Read This Plot\n",
    "The matrix is composed of two types of plots:\n",
    "- **Histograms (on the diagonal)**: The plots running down the center from top-left to bottom-right are histograms. Each histogram shows the distribution of a single variable. It helps us understand the range of yields and identify the most common yield values for that specific crop.\n",
    "- **Scatterplots (off the diagonal)**: All other plots are scatterplots that show the relationship between two different variables. To read a scatterplot, find its corresponding variable on the bottom x-axis (for the column) and the left y-axis (for the row).\n",
    "\n",
    "### Key Observations and Insights\n",
    "From this visualization, we can draw several important conclusions:\n",
    "\n",
    "1. Strong Positive Correlation between Maize and Soybean Yields:\n",
    "The scatterplot in the second row, first column (Soybean vs. Maize) shows a clear and relatively tight upward trend. This indicates a strong positive correlation. In simple terms, geographic locations that produce high yields of maize also tend to produce high yields of soybeans. This aligns with common agricultural knowledge, as these crops are often grown in rotation in the same fertile regions (like the U.S. Corn Belt).\n",
    "2. Weaker, More Complex Relationship with Wheat:\n",
    "The relationships between Wheat and the other two crops (bottom row) are also positive but are much more scattered or \"noisy.\" This suggests that while there is a general trend for yields to rise together, the connection is not as direct as it is between maize and soybean. The wheat_yield vs. maize_yield plot even shows a few distinct clusters, which could suggest different climate sub-regions or farming systems where the relationship between these two crops varies.\n",
    "3. Different Yield Distributions:\n",
    "The histograms on the diagonal are very revealing:\n",
    "    - Maize and Wheat show a similar right-skewed distribution. This means that for both crops, most locations have low-to-moderate yields, with a long \"tail\" of a few locations producing very high yields.\n",
    "    - Soybean shows a more normal (bell-shaped) distribution. The yields are more symmetrically clustered around a central average, with fewer extreme high or low values compared to the other crops.\n",
    "\n",
    "### Summary\n",
    "The most prominent feature of this analysis is the strong synergistic relationship between maize and soybean yields in co-located areas. Wheat also benefits from similar conditions but appears to have a more complex relationship, suggesting it may have a wider or different range of optimal growing conditions even within these shared regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50fc875-5157-44b4-9f11-028b3e443a3c",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis 2: Wheat seasonality**\n",
    "\n",
    "**Scientific Question:** How do the yields of winter and spring wheat relate to each other and to their geographic location? Are they grown in the same places? Is one variety inherently higher-yielding?\n",
    "\n",
    "From what we know about wheat growing, the reason we see both winter and spring harvests is because depending on the seasonality, countries either plant their wheat in the spring, and harvest in the fall, or countries plant their wheat in the winter, where it stays dormant until the winter is over, and then grows in the spring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b11ba-a945-42f3-aea4-cabcee4b61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Generate the Wheat Seasonality Scatterplot Matrix\n",
    "\n",
    "print(\"--- Starting Wheat Seasonality Analysis ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the paths for the two wheat varieties\n",
    "wheat_seasonality_paths = {\n",
    "    'wheat_winter': '../data/wheat_winter/*.nc4',\n",
    "    'wheat_spring': '../data/wheat_spring/*.nc4'\n",
    "}\n",
    "\n",
    "# --- Load the data for the selected crops ---\n",
    "seasonal_dfs = []\n",
    "for crop, path in wheat_seasonality_paths.items():\n",
    "    print(f\"Loading data for {crop}...\")\n",
    "    df = load_and_clean_crop_data(path)\n",
    "    if df is not None:\n",
    "        df = df.rename(columns={'var': f'{crop}_yield'})\n",
    "        seasonal_dfs.append(df)\n",
    "\n",
    "# --- Merge the DataFrames ---\n",
    "# We use an 'outer' merge here. This is crucial because we want to keep all\n",
    "# locations where EITHER winter or spring wheat is grown, not just the few\n",
    "# locations where they might overlap. This allows us to see the full\n",
    "# geographic range of each variety. Missing values will be filled with NaN.\n",
    "if len(seasonal_dfs) == 2:\n",
    "    wheat_merged_df = pd.merge(seasonal_dfs[0], seasonal_dfs[1], on=['lat', 'lon'], how='outer')\n",
    "\n",
    "    print(f\"\\nFound {len(wheat_merged_df)} total locations for winter and/or spring wheat.\")\n",
    "    print(\"Generating scatterplot matrix...\")\n",
    "\n",
    "    # --- Create the Scatterplot Matrix ---\n",
    "    # We explicitly include 'lat' and 'lon' as they are key to our analysis.\n",
    "    # Seaborn's pairplot is smart enough to handle the NaN values, only plotting\n",
    "    # the available data points in each subplot.\n",
    "    vars_to_plot = ['wheat_winter_yield', 'wheat_spring_yield', 'lat', 'lon']\n",
    "    \n",
    "    g = sns.pairplot(wheat_merged_df, vars=vars_to_plot, corner=True,\n",
    "                     plot_kws={'s': 10, 'alpha': 0.6}) # Add styling for better visibility\n",
    "    \n",
    "    g.fig.suptitle(\"Exploratory Analysis of Wheat Seasonality\", y=1.02)\n",
    "    plt.show()\n",
    "    g.savefig(os.path.join(output_dir, 'wheat_seasonality_deep_dive.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "else:\n",
    "    print(\"Could not load data for both wheat varieties. Please check file paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565d017-f023-4ce1-bc8b-f2fb044bc31e",
   "metadata": {},
   "source": [
    "## Key Observations and Insights\n",
    "\n",
    "This scatterplot matrix reveales an interesting relationship between the spring and winter wheat. As expected, we see some seasonality depicted in the data, however, interestingly, we see a strong correlation with the two seasons, as well as a large overlap in area. \n",
    "\n",
    "1. Strong Overall Positive Correlation:\n",
    "The most immediate finding is the strong positive correlation between wheat_spring_yield and wheat_winter_yield. The clear upward trend in their scatterplot (second row, first column) indicates that, on the whole, locations with high productivity for one variety also tend to have high productivity for the other. This suggests they thrive under similar soil and climate conditions in the regions where they coexist.\n",
    "2. Distinct Geographic Niches Revealed by Latitude:\n",
    "While the general cultivation zones overlap, the plots of yield vs. latitude (lat) reveal critical differences that contradict the idea of an identical distribution:\n",
    "    - A Tropical Zone for Spring Wheat: There is a clear gap in wheat_winter_yield data around the equator (latitude 0). In contrast, wheat_spring_yield shows a distinct cluster of cultivation in this tropical band. This is a key finding: the lack of a cold winter makes only spring wheat viable in these regions.\n",
    "    - Bimodal Temperate Zones: The histogram for lat is \"bimodal\" (has two peaks), confirming major cultivation zones in both the Northern and Southern Hemispheres. It is within these large temperate regions that the two crops overlap most significantly.\n",
    "3. Winter Wheat Dominance in Key Longitudes:\n",
    "The plots of yield vs. longitude (lon) further refine the story. While the patterns are similar, the data for wheat_winter_yield is visibly denser and more continuous across major agricultural belts (e.g., longitudes corresponding to the Americas and Eurasia). This suggests that while spring wheat is also grown, winter wheat is the dominant variety in these core temperate zones.\n",
    "\n",
    "**Conclusion** \n",
    "The analysis tells a story of both coexistence and specialization. The strong overall correlation is driven by the large temperate regions where both varieties are productive. However, the exceptions are just as important: spring wheat holds a unique advantage in the tropics, while winter wheat is the primary variety in the established temperate-zone wheat belts. The data shows that rather than being simple substitutes, each variety has a unique environmental edge at the margins of their growing regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e05a0-9667-4639-9ae3-1f6a84c8f34d",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis 3: Major vs Second Harvest of Rice and Maize**\n",
    "\n",
    "This analysis helps us understand the dynamics of multi-season cropping. \n",
    "\n",
    "**Scientific Question:** In areas that support a second harvest, is there a relationship between the yield of the main harvest and the second one? Is a second harvest only viable in certain latitudes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede80e88-e0a9-4b88-a9c1-4a68a6a32d54",
   "metadata": {},
   "source": [
    "## Maize - Major vs. Second Harvest \n",
    "\n",
    "This analysis focuses on maize to understand the relationship between the primary and secondary harvests. We will investigate how the yields relate to each other and how the practice of a second harvest is constrained by geography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0c05f-852c-4df9-878d-ee24ed9d77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Generate the Maize Major vs. Second Harvest Matrix\n",
    "\n",
    "print(\"--- Starting Maize Harvest Analysis ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "maize_harvest_paths = {\n",
    "    'maize_major':  '../data/maize_major/*.nc4',\n",
    "    'maize_second': '../data/maize_second/*.nc4'\n",
    "}\n",
    "\n",
    "# --- Load the data ---\n",
    "maize_harvest_dfs = []\n",
    "for crop, path in maize_harvest_paths.items():\n",
    "    print(f\"Loading data for {crop}...\")\n",
    "    df = load_and_clean_crop_data(path)\n",
    "    if df is not None:\n",
    "        df = df.rename(columns={'var': f'{crop}_yield'})\n",
    "        maize_harvest_dfs.append(df)\n",
    "\n",
    "# --- Merge the DataFrames ---\n",
    "# We use an 'inner' merge here because our scientific question is about the\n",
    "# relationship between the two harvests *in the specific areas that support both*.\n",
    "if len(maize_harvest_dfs) == 2:\n",
    "    maize_merged_df = pd.merge(maize_harvest_dfs[0], maize_harvest_dfs[1], on=['lat', 'lon'], how='inner')\n",
    "\n",
    "    print(f\"\\nFound {len(maize_merged_df)} locations that support both a major and second maize harvest.\")\n",
    "    print(\"Generating scatterplot matrix...\")\n",
    "\n",
    "    # --- Create the Scatterplot Matrix ---\n",
    "    vars_to_plot = ['maize_major_yield', 'maize_second_yield', 'lat', 'lon']\n",
    "    \n",
    "    g = sns.pairplot(maize_merged_df, vars=vars_to_plot, corner=True,\n",
    "                     plot_kws={'s': 10, 'alpha': 0.6})\n",
    "    \n",
    "    g.fig.suptitle(\"Analysis of Major vs. Second Maize Harvests\", y=1.02)\n",
    "    plt.show()\n",
    "    g.savefig(os.path.join(output_dir, 'multicrop_maize_major_vs_second.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "else:\n",
    "    print(\"Could not load data for both maize harvest types. Please check file paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c93d1-bf49-4d21-9511-ba97080eb8ec",
   "metadata": {},
   "source": [
    "## Rice - Major vs. Second Harvest \n",
    "\n",
    "This analysis focuses on maize to understand the relationship between the primary and secondary harvests. We will investigate how the yields relate to each other and how the practice of a second harvest is constrained by geography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7b897-ec1d-417c-9e79-b90e3133fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate the Rice Major vs. Second Harvest Matrix\n",
    "\n",
    "print(\"--- Starting Rice Harvest Analysis ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "rice_harvest_paths = {\n",
    "    'rice_major':  '../data/rice_major/*.nc4',\n",
    "    'rice_second': '../data/rice_second/*.nc4'\n",
    "}\n",
    "\n",
    "# --- Load the data ---\n",
    "rice_harvest_dfs = []\n",
    "for crop, path in rice_harvest_paths.items():\n",
    "    print(f\"Loading data for {crop}...\")\n",
    "    df = load_and_clean_crop_data(path)\n",
    "    if df is not None:\n",
    "        df = df.rename(columns={'var': f'{crop}_yield'})\n",
    "        rice_harvest_dfs.append(df)\n",
    "\n",
    "# --- Merge the DataFrames ---\n",
    "if len(rice_harvest_dfs) == 2:\n",
    "    rice_merged_df = pd.merge(rice_harvest_dfs[0], rice_harvest_dfs[1], on=['lat', 'lon'], how='inner')\n",
    "\n",
    "    print(f\"\\nFound {len(rice_merged_df)} locations that support both a major and second rice harvest.\")\n",
    "    print(\"Generating scatterplot matrix...\")\n",
    "\n",
    "    # --- Create the Scatterplot Matrix ---\n",
    "    vars_to_plot = ['rice_major_yield', 'rice_second_yield', 'lat', 'lon']\n",
    "    \n",
    "    g = sns.pairplot(rice_merged_df, vars=vars_to_plot, corner=True,\n",
    "                     plot_kws={'s': 10, 'alpha': 0.6})\n",
    "    \n",
    "    g.fig.suptitle(\"Analysis of Major vs. Second Rice Harvests\", y=1.02)\n",
    "    plt.show()\n",
    "    g.savefig(os.path.join(output_dir, 'multicrop_rice_major_vs_second.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "else:\n",
    "    print(\"Could not load data for both rice harvest types. Please check file paths.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9822cc2-42d3-4b32-8e27-472482c23966",
   "metadata": {},
   "source": [
    "## Analysis of plots\n",
    "\n",
    "### Analysis of the Maize Plot\n",
    "- Yield Relationship: The scatterplot of maize_second_yield vs. maize_major_yield shows a very strong, clear positive correlation. In places that support two harvests, a good main harvest strongly predicts a good second harvest.\n",
    "- The Crucial Latitude Insight: Look at the lat vs. yield plots and the lat histogram. The entire dataset of multi-crop maize exists almost exclusively between -30 and +30 degrees latitude. This is the answer to the second part of our question. It visually proves that a second maize harvest is a tropical and subtropical phenomenon. You cannot do this in the temperate zones of the US or Europe because the growing season is too short.\n",
    "\n",
    "### Analysis of the Rice Plot\n",
    "- Yield Relationship: Like maize, the rice plot shows a strong positive correlation between the major and second harvests.\n",
    "- The Crucial Latitude Insight: The geographic constraint is even more dramatic for rice. The data is almost entirely clustered between 0 and +30 degrees latitude. This tells us that multi-season rice cultivation is overwhelmingly a Northern Hemisphere tropical practice, which corresponds perfectly with the major rice-growing regions of South and Southeast Asia. The lon plot confirms this, showing a heavy concentration around 80-120 degrees longitude.\n",
    "\n",
    "**Conclusion**\n",
    "The inner merge correctly isolated the phenomenon we wanted to study (multi-cropping), and the inclusion of lat and lon was the key to understanding the powerful geographic constraints that govern it. The plots demonstrate that having a second harvest isn't just about farming technique; it's a possibility that is only open to specific regions of the world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8386f5-7745-49d7-90aa-c85330231998",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis 4: Low regional overlap crop comparison**\n",
    "\n",
    "**Scientific Question:** In the specific, limited regions where both Wheat and Rice are cultivated (e.g., Northern India, parts of China), is there any discernible relationship between their yields?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7d157-e219-4a41-9eef-106a20411dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Low-Overlap Regional Analysis (Rice-Wheat System)\n",
    "\n",
    "print(\"--- Starting Low-Overlap Regional Analysis: Rice-Wheat System ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "low_overlap_paths = {\n",
    "    'rice':   '../data/rice/yield_*.nc4',\n",
    "    'wheat':  '../data/wheat/yield_*.nc4'\n",
    "}\n",
    "\n",
    "# --- Load the data ---\n",
    "low_overlap_dfs = []\n",
    "for crop, path in low_overlap_paths.items():\n",
    "    print(f\"Loading data for {crop}...\")\n",
    "    df = load_and_clean_crop_data(path)\n",
    "    if df is not None:\n",
    "        df = df.rename(columns={'var': f'{crop}_yield'})\n",
    "        low_overlap_dfs.append(df)\n",
    "\n",
    "# --- Merge to find all co-located points globally ---\n",
    "if len(low_overlap_dfs) == 2:\n",
    "    # First, find all locations on Earth where both are grown\n",
    "    global_overlap_df = pd.merge(low_overlap_dfs[0], low_overlap_dfs[1], on=['lat', 'lon'], how='inner')\n",
    "    print(f\"\\nFound {len(global_overlap_df)} total co-located Rice/Wheat points globally.\")\n",
    "\n",
    "    # --- NEW STEP: Filter to a specific region of interest ---\n",
    "    # We define a bounding box for the Indo-Gangetic Plain (Northern India, Pakistan, etc.)\n",
    "    # This is the most prominent orange area on your map.\n",
    "    lat_min, lat_max = 20, 35\n",
    "    lon_min, lon_max = 70, 120\n",
    "\n",
    "    regional_df = global_overlap_df[\n",
    "        (global_overlap_df['lat'].between(lat_min, lat_max)) &\n",
    "        (global_overlap_df['lon'].between(lon_min, lon_max))\n",
    "    ]\n",
    "    print(f\"Filtered down to {len(regional_df)} points within the Indo-Gangetic Plain region.\")\n",
    "\n",
    "    # --- Create the Scatterplot Matrix for the specific region ---\n",
    "    if len(regional_df) > 0:\n",
    "        vars_to_plot = ['rice_yield', 'wheat_yield'] # Keep it focused on the yields\n",
    "        \n",
    "        g = sns.pairplot(regional_df, vars=vars_to_plot, corner=True, kind='reg',\n",
    "                         plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.5, 's': 15}})\n",
    "        \n",
    "        g.fig.suptitle(\"Yield Relationship in the Rice-Wheat System of the Indo-Gangetic Plain\", y=1.02)\n",
    "        plt.show()\n",
    "        g.savefig(os.path.join(output_dir, 'regional_rice_wheat_indo_gangetic.png'), dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        print(\"No overlapping data points were found within the specified regional filter.\")\n",
    "else:\n",
    "    print(\"Could not load data for both Rice and Wheat. Please check file paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e7bc5-b7ff-4c22-991f-f2c473e0cf77",
   "metadata": {},
   "source": [
    "## Analysis of the Indo-Gangetic Plain Rice-Wheat System\n",
    "This scatterplot matrix provides a focused look into the yield dynamics of the intensive Rice-Wheat agricultural system in the Indo-Gangetic Plain. By filtering to this specific region, we can analyze the relationship between the two cornerstone crops.\n",
    "\n",
    "**Key Observations and Insights**\n",
    "1. Strong Positive Correlation: The most prominent feature is the clear positive correlation between rice_yield and wheat_yield, illustrated by the upward trend of the data points and the fitted red regression line. This indicates that the productivity of the two crops is linked. In general, a location and year that produce a high rice yield are also likely to produce a high wheat yield in the subsequent season. This suggests that shared underlying factorssuch as well-managed irrigation, soil health, fertilizer availability, and favorable policiesare primary drivers of success for the entire system.\n",
    "2. Increasing Variance at Higher Yields (Heteroscedasticity): A more subtle and powerful insight comes from the changing shape of the data cloud. The points are tightly clustered in the lower-left corner (low yields) and become significantly more spread out in the upper-right (high yields). This pattern is known as heteroscedasticity.\n",
    "Interpretation: At lower productivity levels, both crops are likely constrained by the same fundamental limitations (e.g., water scarcity, poor soil), resulting in little variation. However, in high-productivity scenarios where these basic needs are met, other, more variable factorssuch as the adoption of advanced high-yield seed varieties, precision farming techniques, or slight variations in weatherbegin to dominate. This introduces more variance, explaining why some high-yield rice locations produce exceptionally high wheat yields while others produce more moderate wheat yields.\n",
    "3. Right-Skewed Yield Distributions: The histograms on the diagonal for both rice and wheat show a distinct right skew (or positive skew). This means the distributions have a long \"tail\" extending to the right.\n",
    "    - Interpretation: This shape reveals that for both crops within this system, the majority of locations produce low-to-moderate yields. A smaller number of highly productive, optimized locations are responsible for the highest yields, creating the tail of the distribution. This provides important context for the scatterplot, explaining why the data is densest in the lower-left quadrant.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "In conclusion, the analysis reveals a synergistic but complex relationship between rice and wheat in the Indo-Gangetic Plain. While their success is broadly linked, the system's behavior changes as productivity increases, becoming more variable. This suggests that while foundational improvements can lift the entire system, achieving elite-level yields for both crops is a more complex challenge influenced by a wider range of factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b0a472-6878-420d-bbf4-5780f02603d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 (Corrected): Low-Overlap Regional Analysis (Andean Rice-Wheat System)\n",
    "\n",
    "print(\"--- Starting Low-Overlap Regional Analysis: Andean Rice-Wheat System ---\")\n",
    "\n",
    "if 'global_overlap_df' in locals():\n",
    "    # --- Filter to the Andean region (Peru/Ecuador) with CORRECTED coordinates ---\n",
    "    # Define a bounding box for the Andean region.\n",
    "    # NOTE: We convert longitude to the 0-360 degree system to match the data.\n",
    "    lat_min, lat_max = -20, 5     # Latitude remains the same\n",
    "    lon_min, lon_max = 275, 300   # CORRECTED Longitude for Western South America\n",
    "\n",
    "    andean_regional_df = global_overlap_df[\n",
    "        (global_overlap_df['lat'].between(lat_min, lat_max)) &\n",
    "        (global_overlap_df['lon'].between(lon_min, lon_max))\n",
    "    ]\n",
    "    print(f\"Filtered down to {len(andean_regional_df)} points within the Andean region.\")\n",
    "\n",
    "    # --- Create the Scatterplot Matrix for this specific region ---\n",
    "    if len(andean_regional_df) > 0:\n",
    "        vars_to_plot = ['rice_yield', 'wheat_yield']\n",
    "        \n",
    "        g = sns.pairplot(andean_regional_df, vars=vars_to_plot, corner=True, kind='reg',\n",
    "                         plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.5, 's': 15}})\n",
    "        \n",
    "        g.fig.suptitle(\"Yield Relationship in the Andean Rice-Wheat System (Peru/Ecuador)\", y=1.02)\n",
    "        plt.show()\n",
    "        g.savefig(os.path.join(output_dir, 'regional_rice_wheat_andean.png'), dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        print(\"No overlapping data points were found within the specified Andean regional filter.\")\n",
    "else:\n",
    "    print(\"Please run the previous cell first to generate 'global_overlap_df'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178148bf-eced-4476-aa48-51700bbb354b",
   "metadata": {},
   "source": [
    "## Analysis of the Andean Rice-Wheat System (Peru/Ecuador)\n",
    "\n",
    "This analysis of the Andean Rice-Wheat system reveals a fascinating and complex picture, starkly different from the Indo-Gangetic Plain. The data strongly suggests we are observing two distinct agricultural sub-systems within the same geographic filter.\n",
    "\n",
    "**Key Observations and Insights**\n",
    "1. A Tale of Two Systems: While the overall regression line shows a strong positive correlation, the most significant feature is the separation of the data into two distinct clusters. There is a dense, lower-yield cluster and a more diffuse, high-yield cluster, connected by a sparse line of data points. This strongly suggests the presence of a confounding variable, likely geography, separating two different farming systems. A leading hypothesis is that these represent high-altitude mountain agriculture (lower yields) and irrigated coastal plain agriculture (higher yields).\n",
    "2. Bimodal (\"U-Shaped\") Distributions: This \"two-system\" hypothesis is powerfully confirmed by the histograms on the diagonal. Both rice_yield and wheat_yield exhibit a bimodal distribution (having two peaks), which you aptly described as \"U-shaped.\" This indicates that for both crops, there are large numbers of locations producing low-to-moderate yields and another large group producing high yields, with very few in between. This is the statistical signature of two different populations being plotted together.\n",
    "3. Distinct Cluster Characteristics: The shapes of the two clusters are also informative. The lower-yield cluster displays a \"fan shape\" (heteroscedasticity), where variability increases with yield. In contrast, the high-yield \"hypercarry\" cluster is more cohesive, perhaps indicating more standardized and consistently high-output commercial farming practices in the most productive zones.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Unlike the continuous system seen in the Indo-Gangetic Plain, the Andean Rice-Wheat system is characterized by a distinct duality. The relationship between rice and wheat yields is still positive within each sub-system, but the existence of two different production environments (likely driven by altitude) is the dominant story. This analysis highlights the critical importance of regional context and demonstrates how a single scatterplot matrix can uncover deep structural patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 10-Climate-Data-Processing.ipynb/10-Climate-Data-Processing.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff68c9-1d7d-4a25-b295-d7bad9b1162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f215fe-b99b-4be4-9f6e-88184ac8a24c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(r\"C:\\Users\\lytten\\programming\\climarisc\\data\\climate_raw\\usa_era5_land_hourly\\1981\\test.nc\")\n",
    "print(p)                # sanity check the string\n",
    "print(p.exists())       # should be True\n",
    "print(p.is_file())      # should be True\n",
    "print(p.stat().st_size) # size in bytes if it exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d474d6-c15b-47ee-bed2-1406ad548d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Open the NetCDF file\n",
    "ds = xr.open_dataset(r\"C:\\Users\\lytten\\programming\\climarisc\\data\\climate_raw\\usa_era5_land_hourly\\1981\\era5_land_usa_1981_01.nc\")\n",
    "\n",
    "# Show the dataset structure\n",
    "print(ds)\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4240a530-be8f-4a3b-b169-cf62b4d60050",
   "metadata": {},
   "source": [
    "# Processing ERA5-Land Climate Data - 1981 Test Case\n",
    "\n",
    "**Goal:** To process the raw hourly ERA5-Land data for 1981 into analysis-ready, yearly stressor metrics that align with our crop yield data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e7659-b7e9-4d9a-bb80-9232d673694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Hourly Data for 1981 (Using the CORRECT cfgrib engine)\n",
    "\n",
    "# Path to the downloaded files for the year 1981\n",
    "DATA_PATH = '../data/climate_raw/usa_era5_land_hourly/1981/test.nc'\n",
    "\n",
    "try:\n",
    "    # Use open_mfdataset to combine the 12 monthly files along the time dimension\n",
    "    # FINAL CORRECTION: Using engine='cfgrib' as indicated by the file's own metadata.\n",
    "    ds_hourly = xr.open_mfdataset(DATA_PATH, engine='cfgrib')\n",
    "    \n",
    "    print(\"Successfully loaded 12 monthly files for 1981.\")\n",
    "    print(ds_hourly)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load data. Please ensure the files for 1981 have finished downloading.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc870cc-7a93-4e53-831a-9ce7c6e7676f",
   "metadata": {},
   "source": [
    "## 1. Aggregate from Hourly to Daily\n",
    "\n",
    "We will convert the hourly data into key daily metrics:\n",
    "- **Max Temperature (Tmax):** The highest temperature of the day.\n",
    "- **Min Temperature (Tmin):** The lowest temperature of the day.\n",
    "- **Total Precipitation (Precip):** The total rainfall for the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d8c589-ae8f-49f2-b365-4d715393b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Resample to Daily Data\n",
    "\n",
    "# The 't2m' variable is '2m_temperature' in Kelvin.\n",
    "# We resample by taking the daily max and min, and convert to Celsius.\n",
    "daily_tmax = ds_hourly['t2m'].resample(time='1D').max() - 273.15\n",
    "daily_tmin = ds_hourly['t2m'].resample(time='1D').min() - 273.15\n",
    "\n",
    "# The 'tp' variable is 'total_precipitation' in meters.\n",
    "# It's a cumulative value, so we resample by taking the daily sum and convert to mm.\n",
    "daily_precip = ds_hourly['tp'].resample(time='1D').sum() * 1000\n",
    "\n",
    "# Combine these into a new, clean daily dataset\n",
    "ds_daily = xr.Dataset({\n",
    "    'tmax': daily_tmax,\n",
    "    'tmin': daily_tmin,\n",
    "    'precip': daily_precip\n",
    "})\n",
    "\n",
    "print(\"Resampled to daily data:\")\n",
    "print(ds_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e286d-f662-4c6a-97cc-2c76619a1f20",
   "metadata": {},
   "source": [
    "## 2. Calculate Yearly Growing-Season Metrics\n",
    "\n",
    "We'll assume a growing season for maize in the US is from April to September (months 4-9) and calculate summary statistics for that period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca1070-6e3e-457f-88a2-419d7656d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Calculate Yearly Stress Metrics\n",
    "\n",
    "# Select only the months in the growing season\n",
    "growing_season = ds_daily.where(ds_daily['time.month'].isin(range(4, 10)), drop=True)\n",
    "\n",
    "# Calculate yearly metrics\n",
    "# 1. Total growing season precipitation\n",
    "total_precip_1981 = growing_season['precip'].sum(dim='time')\n",
    "\n",
    "# 2. Average maximum temperature during the growing season\n",
    "avg_tmax_1981 = growing_season['tmax'].mean(dim='time')\n",
    "\n",
    "# 3. Number of extreme heat days (e.g., days > 30C)\n",
    "extreme_heat_days_1981 = growing_season['tmax'].where(growing_season['tmax'] > 30).count(dim='time')\n",
    "\n",
    "# Combine into a yearly dataset\n",
    "ds_yearly = xr.Dataset({\n",
    "    'total_precip': total_precip_1981,\n",
    "    'avg_tmax': avg_tmax_1981,\n",
    "    'extreme_heat_days': extreme_heat_days_1981\n",
    "})\n",
    "\n",
    "print(\"Calculated yearly metrics for 1981 growing season:\")\n",
    "print(ds_yearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949fdef-3a25-4337-ae0f-f84b5f50a6f3",
   "metadata": {},
   "source": [
    "## 3. Align Climate Grid to Yield Grid\n",
    "\n",
    "The ERA5 data is on a 0.1 grid, but our yield data is on a 0.5 grid. We must resample the climate data to match the yield data's coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a0aec-df52-4cd8-8a13-5c8b5b75898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Regrid Climate Data and Save\n",
    "\n",
    "# First, load a sample of the yield data to use as a grid template\n",
    "YIELD_PATH = '../data/maize/yield_1981.nc4'\n",
    "yield_grid = xr.open_dataset(YIELD_PATH)\n",
    "\n",
    "# Use .interp_like() to resample our yearly climate data to the yield grid\n",
    "print(\"Regridding climate data to match yield data...\")\n",
    "ds_yearly_aligned = ds_yearly.interp_like(yield_grid)\n",
    "\n",
    "print(\"\\nFinal, aligned dataset for 1981:\")\n",
    "print(ds_yearly_aligned)\n",
    "\n",
    "# --- Save the final processed file ---\n",
    "output_dir = '../data/climate_processed/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'climate_yearly_usa_1981.nc')\n",
    "ds_yearly_aligned.to_netcdf(output_file)\n",
    "\n",
    "print(f\"\\nSuccessfully saved processed 1981 climate data to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 11-API-Download-Test.ipynb/11-API-Download-Test.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd715da-99a5-4382-b470-9677d6a25e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Single File Download Test\n",
    "import cdsapi\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Create a dedicated folder for this test\n",
    "output_dir = '../data/climate_raw/api_test/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'era5_land_usa_1981_01_temp.grib')\n",
    "\n",
    "# --- API Request for a Single Month ---\n",
    "print(\"Building request for January 1981...\")\n",
    "request_dictionary = {\n",
    "    'format': 'grib',\n",
    "    'variable': [\n",
    "        '2m_temperature',\n",
    "    ],\n",
    "    'year': '1981',\n",
    "    'month': '01',\n",
    "    'day': [f'{d:02d}' for d in range(1, 32)],\n",
    "    'time': [f'{h:02d}' for h in range(0, 24)],\n",
    "    'area': [\n",
    "        50, -105, 25, -80, # North, West, South, East\n",
    "    ],\n",
    "}\n",
    "\n",
    "# --- Execute Download ---\n",
    "try:\n",
    "    if not os.path.exists(output_file):\n",
    "        c = cdsapi.Client()\n",
    "        print(\"Submitting API request to the CDS...\")\n",
    "        print(f\"File will be saved to: {output_file}\")\n",
    "        \n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land',\n",
    "            request_dictionary,\n",
    "            output_file\n",
    "        )\n",
    "        print(\"\\nDownload complete!\")\n",
    "        print(f\"File size: {os.path.getsize(output_file) / 1e6:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"File already exists, skipping download: {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "    print(\"Please double-check that the 'climarisc_env' is active and your .cdsapirc file is correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab5bea3-4abb-4141-ba0f-49060179adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load and Inspect the Downloaded GRIB Files\n",
    "import xarray as xr\n",
    "\n",
    "# --- File Paths ---\n",
    "# Path to the file downloaded via our Python API script\n",
    "api_file_path = r'../data/climate_raw/api_test/era5_land_usa_1981_01_temp.grib'\n",
    "\n",
    "# Path to the file you downloaded manually from the website\n",
    "manual_file_path = r'../data/climate_raw/api_test/data.grib'\n",
    "\n",
    "# --- Test 1: Open the API-downloaded file ---\n",
    "print(\"--- Attempting to open the API-downloaded file ---\")\n",
    "try:\n",
    "    # We must explicitly use the 'cfgrib' engine to read GRIB files\n",
    "    ds_api = xr.open_dataset(api_file_path, engine='cfgrib')\n",
    "    print(\"SUCCESS: API file loaded correctly.\")\n",
    "    print(ds_api)\n",
    "except Exception as e:\n",
    "    print(f\"FAILED to load API file. Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- Test 2: Open the manually-downloaded file ---\n",
    "print(\"--- Attempting to open the manually-downloaded file ---\")\n",
    "try:\n",
    "    # Use the same engine to open the manual file\n",
    "    ds_manual = xr.open_dataset(manual_file_path, engine='cfgrib')\n",
    "    print(\"SUCCESS: Manually downloaded file loaded correctly.\")\n",
    "    print(ds_manual)\n",
    "except Exception as e:\n",
    "    print(f\"FAILED to load manual file. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb3f9c-3ea4-48c1-84bd-f4fbb16bfed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Single File Download Test (Final Robust Version)\n",
    "import cdsapi\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "# --- Configuration ---\n",
    "output_dir = '../data/climate_raw/api_test/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'era5_land_usa_1981_01_temp_robust.grib')\n",
    "\n",
    "# --- API Request for a Single Month ---\n",
    "print(\"Building ROBUST request for January 1981...\")\n",
    "request_dictionary = {\n",
    "    'format': 'grib',\n",
    "    'variable': [\n",
    "        '2m_temperature',\n",
    "    ],\n",
    "    'year': '1981',\n",
    "    'month': '01',\n",
    "    'day': [f'{d:02d}' for d in range(1, 32)],\n",
    "    'time': [f'{h:02d}:00' for h in range(0, 24)],\n",
    "    'area': [\n",
    "        50, -105, 25, -80, # North, West, South, East\n",
    "    ],\n",
    "    # ADDED: This is a common option to ensure the file is properly packaged\n",
    "    'download_format': 'unarchived'\n",
    "}\n",
    "\n",
    "# --- Execute Download ---\n",
    "try:\n",
    "    if not os.path.exists(output_file):\n",
    "        c = cdsapi.Client()\n",
    "        print(\"Submitting API request to the CDS...\")\n",
    "        print(f\"File will be saved to: {output_file}\")\n",
    "        \n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land',\n",
    "            request_dictionary,\n",
    "            output_file\n",
    "        )\n",
    "        print(\"\\nDownload complete!\")\n",
    "    else:\n",
    "        print(f\"File already exists, skipping download: {output_file}\")\n",
    "\n",
    "    # --- Verification Step ---\n",
    "    print(\"\\n--- Verifying the downloaded file ---\")\n",
    "    ds_test = xr.open_dataset(output_file, engine='cfgrib')\n",
    "    print(\"SUCCESS! The downloaded file was opened correctly.\")\n",
    "    print(ds_test)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04153269-2355-4ea1-b828-979bca467a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Download All Months for 1981\n",
    "import cdsapi\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "# Create a dedicated folder for the full 1981 download\n",
    "output_dir = '../data/climate_raw/api_test_1981/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- API Request Loop ---\n",
    "c = cdsapi.Client()\n",
    "print(\"Starting download for all 12 months of 1981...\")\n",
    "\n",
    "for month in range(1, 13):\n",
    "    output_file = os.path.join(output_dir, f'era5_land_usa_1981_{month:02d}.grib')\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"File for 1981-{month:02d} already exists. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nSubmitting request for: 1981-{month:02d}\")\n",
    "    \n",
    "    try:\n",
    "        request_dictionary = {\n",
    "            'format': 'grib',\n",
    "            'variable': ['2m_temperature', 'total_precipitation'],\n",
    "            'year': '1981',\n",
    "            'month': f'{month:02d}',\n",
    "            'day': [f'{d:02d}' for d in range(1, 32)],\n",
    "            'time': [f'{h:02d}' for h in range(0, 24)],\n",
    "            'area': [50, -105, 25, -80],\n",
    "        }\n",
    "        c.retrieve('reanalysis-era5-land', request_dictionary, output_file)\n",
    "        print(f\"Download for 1981-{month:02d} complete!\")\n",
    "        time.sleep(1) # A small polite pause\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred for 1981-{month:02d}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n--- Download for 1981 finished! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 12-First-Yield-Climate-Analysis.ipynb/12-First-Yield-Climate-Analysis.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace8aa30-4d93-49ea-b1a7-b48f44462f4f",
   "metadata": {},
   "source": [
    "# First Analysis: Linking Climate to Yield (1981 Test Case)\n",
    "\n",
    "**Goal:** To perform a proof-of-concept analysis by linking our newly downloaded climate data with the corresponding crop yield data for the year 1981.\n",
    "\n",
    "**Methodology:**\n",
    "1.  Load the 1981 maize yield data.\n",
    "2.  Load the January 1981 temperature data.\n",
    "3.  Process the hourly temperature data into a single metric for the month (average temperature).\n",
    "4.  Align the high-resolution climate grid to the lower-resolution yield grid.\n",
    "5.  Create a scatter plot to visualize the relationship between temperature and yield for each grid cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8be32-15b8-4837-b53a-0640913e5642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Load Data\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Load Maize Yield Data for 1981 ---\n",
    "YIELD_PATH = '../data/maize/yield_1981.nc4'\n",
    "ds_yield = xr.open_dataset(YIELD_PATH)\n",
    "print(\"Yield data for 1981 loaded.\")\n",
    "\n",
    "# --- Load Climate Data for Jan 1981 ---\n",
    "CLIMATE_PATH = '../data/climate_raw/api_test/era5_land_usa_1981_01_temp_robust.grib'\n",
    "# Use the cfgrib engine, which we know works\n",
    "ds_climate = xr.open_dataset(CLIMATE_PATH, engine='cfgrib')\n",
    "print(\"Climate data for Jan 1981 loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca608e9-bb98-4f24-9c3d-5ce3a2e4d9e0",
   "metadata": {},
   "source": [
    "## Data Processing and Alignment\n",
    "\n",
    "We need to make the two datasets compatible. This involves calculating a single temperature metric for the month and then resampling the climate grid to match the yield grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f34b19-4dec-432d-a02a-71e7efa78f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Process and Align\n",
    "\n",
    "# 1. Process Climate Data: Calculate the mean temperature for the month and convert to Celsius\n",
    "# The climate data has a 'valid_time' dimension for the hours of the month. We average over it.\n",
    "# We subtract 273.15 to convert from Kelvin to Celsius.\n",
    "avg_temp_jan = ds_climate['t2m'].mean(dim='time') - 273.15\n",
    "\n",
    "# 2. Align Grids: Resample the high-res temperature map to the low-res yield grid\n",
    "print(\"Aligning climate grid to yield grid...\")\n",
    "avg_temp_aligned = avg_temp_jan.interp_like(ds_yield)\n",
    "\n",
    "# 3. Combine into a single dataset\n",
    "# We'll rename the yield variable for clarity\n",
    "analysis_ds = xr.Dataset({\n",
    "    'yield': ds_yield['var'],\n",
    "    'avg_temp': avg_temp_aligned\n",
    "})\n",
    "\n",
    "print(\"\\nData processed and aligned successfully.\")\n",
    "print(analysis_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79bdd07-1c29-4454-a27f-2acee920cd91",
   "metadata": {},
   "source": [
    "## First Scatter Plot: Yield vs. Temperature\n",
    "\n",
    "Now we can plot the yield of each grid cell against the average January temperature of that same grid cell. This is the first step toward building a vulnerability curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fac42e-8065-490e-b769-47ab66449d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Create the Scatter Plot\n",
    "\n",
    "# Convert the xarray Dataset to a pandas DataFrame, which is ideal for this kind of plotting.\n",
    "# This turns our 2D maps into a list of paired (temperature, yield) values.\n",
    "df = analysis_ds.to_dataframe()\n",
    "\n",
    "# Drop any rows that have missing data (e.g., ocean grid cells)\n",
    "df_clean = df.dropna()\n",
    "\n",
    "print(f\"Plotting {len(df_clean)} data points...\")\n",
    "\n",
    "# Create the scatter plot using seaborn for a nice look\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_clean, x='avg_temp', y='yield', alpha=0.5, s=10) # s=10 makes points smaller\n",
    "\n",
    "plt.title('Maize Yield vs. Average January Temperature (US Midwest, 1981)')\n",
    "plt.xlabel('Average January Temperature (C)')\n",
    "plt.ylabel('Maize Yield (tonnes per hectare)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 13-Monthly-Data-Workflow.ipynb/13-Monthly-Data-Workflow.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07857411-74ca-49e9-9896-8cede4c70792",
   "metadata": {},
   "source": [
    "# Monthly Data Workflow: 1995 Test Case\n",
    "\n",
    "**Goal:** To download, process, and analyze one year of ERA5-Land monthly averaged data, creating a robust pipeline for the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f6828-f60f-4598-8052-e165a2bff65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Download Script for 1995\n",
    "import cdsapi\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "output_dir = '../data/climate_monthly/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'era5_land_monthly_1995.grib')\n",
    "\n",
    "# --- API Request (from your notes, modified for 1995 only) ---\n",
    "request_dictionary = {\n",
    "    \"product_type\": \"monthly_averaged_reanalysis\",\n",
    "    \"variable\": [\n",
    "        \"2m_temperature\", \"total_precipitation\", \"volumetric_soil_water_layer_1\",\n",
    "        \"surface_net_solar_radiation\", \"potential_evaporation\"\n",
    "    ],\n",
    "    \"year\": \"1995\", # Requesting only our test year\n",
    "    \"month\": [f'{m:02d}' for m in range(1, 13)],\n",
    "    \"time\": \"00:00\",\n",
    "    \"format\": \"grib\",\n",
    "}\n",
    "\n",
    "# --- Execute Download ---\n",
    "try:\n",
    "    if not os.path.exists(output_file):\n",
    "        c = cdsapi.Client()\n",
    "        print(\"Submitting API request for 1995 monthly data...\")\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land-monthly-means',\n",
    "            request_dictionary,\n",
    "            output_file\n",
    "        )\n",
    "        print(f\"\\nDownload complete! File saved to: {output_file}\")\n",
    "    else:\n",
    "        print(f\"File already exists, skipping download: {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61315ce1-abd4-401c-96d7-f30db1858f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHATGPT\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "path = r\"C:\\Users\\lytten\\programming\\climarisc\\data\\climate_monthly\\4892b2d112b82341e4dbea09a283b32e.grib\"\n",
    "\n",
    "print(\"File exists for Python:\", os.path.exists(path))\n",
    "\n",
    "ds = xr.open_dataset(\n",
    "    path,\n",
    "    engine=\"cfgrib\",\n",
    "    backend_kwargs={\n",
    "        \"indexpath\": \"\",\n",
    "        \"filter_by_keys\": {\"shortName\": \"2t\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5fc2ad-569d-4ae1-ad58-63724e19a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t = ds[\"t2m\"]\n",
    "\n",
    "stats = pd.DataFrame({\n",
    "    \"time\": pd.to_datetime(ds.time.values),\n",
    "    \"mean_C\": (t.mean((\"latitude\",\"longitude\")) - 273.15).values,\n",
    "    \"min_C\":  (t.min((\"latitude\",\"longitude\"))  - 273.15).values,\n",
    "    \"max_C\":  (t.max((\"latitude\",\"longitude\"))  - 273.15).values,\n",
    "})\n",
    "\n",
    "print(stats.round(2).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda33d3a-7a63-437f-bc09-075c87b157de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cdsapi\n",
    "\n",
    "out = r\"C:\\Users\\lytten\\programming\\climarisc\\data\\climate_monthly\\era5_land_monthly_1995_api.grib\"\n",
    "\n",
    "c = cdsapi.Client()\n",
    "c.retrieve(\n",
    "    \"reanalysis-era5-land-monthly-means\",\n",
    "    {\n",
    "        \"product_type\": \"monthly_averaged_reanalysis\",\n",
    "        \"variable\": \"2m_temperature\",\n",
    "        \"year\": \"1995\",\n",
    "        \"month\": [f\"{m:02d}\" for m in range(1, 13)],\n",
    "        \"time\": \"00:00\",\n",
    "        \"data_format\": \"grib\",\n",
    "        \"download_format\": \"unarchived\"   # <-- important\n",
    "    },\n",
    "    out\n",
    ")\n",
    "\n",
    "print(\"File exists:\", os.path.exists(out), \"size_MB:\", round(os.path.getsize(out)/1e6, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97edab97-cf45-4bdf-b6d2-6e43da956639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = r\"C:\\Users\\lytten\\programming\\climarisc\\data\\climate_monthly\\era5_land_monthly_1995_api.grib\"\n",
    "\n",
    "# Open only the 2m temperature field\n",
    "ds = xr.open_dataset(\n",
    "    path,\n",
    "    engine=\"cfgrib\",\n",
    "    backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": {\"shortName\": \"2t\"}},\n",
    ")\n",
    "\n",
    "# Select January 1995\n",
    "jan = ds[\"t2m\"].sel(time=\"1995-01-01\") - 273.15  # convert to C\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "jan.plot(\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-40, vmax=40,  # clamp to typical temperature range for better contrast\n",
    "    cbar_kwargs={\"label\": \"Temperature (C)\"}\n",
    ")\n",
    "plt.title(\"ERA5-Land 2m Temperature  January 1995\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86b206-2651-498f-8d30-3e1ee98a8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\lytten\\programming\\climarisc\\data\\climate_monthly\\era5_land_monthly_1995_api.grib\"\n",
    "\n",
    "ds = xr.open_dataset(\n",
    "    path,\n",
    "    engine=\"cfgrib\",\n",
    "    backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": {\"shortName\": \"2t\"}},\n",
    ")\n",
    "\n",
    "t = ds[\"t2m\"]\n",
    "\n",
    "stats = pd.DataFrame({\n",
    "    \"time\": pd.to_datetime(ds.time.values),\n",
    "    \"mean_C\": (t.mean((\"latitude\",\"longitude\")) - 273.15).values,\n",
    "    \"min_C\":  (t.min((\"latitude\",\"longitude\"))  - 273.15).values,\n",
    "    \"max_C\":  (t.max((\"latitude\",\"longitude\"))  - 273.15).values,\n",
    "})\n",
    "\n",
    "print(stats.round(2).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1541c3-f591-4a81-a634-4cc625a0f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Yield and Climate Data for 1995 (Final, Robust Method)\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Load 1995 Maize Yield Data ---\n",
    "ds_yield = xr.open_dataset('../data/maize/yield_1995.nc4')\n",
    "print(\"Yield data for 1995 loaded.\")\n",
    "\n",
    "# --- Load 1995 Monthly Climate Data ---\n",
    "climate_filepath = '../data/climate_monthly/era5_land_monthly_1995.grib'\n",
    "# Define an explicit path for the index file that cfgrib will create.\n",
    "index_filepath = climate_filepath + '.idx' \n",
    "print(f\"Attempting to load climate data from: {climate_filepath}\")\n",
    "\n",
    "try:\n",
    "    # We will now pass 'backend_kwargs' to give a direct instruction to the 'cfgrib' engine.\n",
    "    # This tells it exactly where to create its index file, avoiding any system confusion.\n",
    "    ds_climate_monthly = xr.open_dataset(\n",
    "        climate_filepath, \n",
    "        engine='cfgrib',\n",
    "        backend_kwargs={'indexpath': index_filepath}\n",
    "    )\n",
    "    \n",
    "    print(\"SUCCESS: Monthly climate data for 1995 loaded correctly.\")\n",
    "    print(ds_climate_monthly)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nFAILED to load the GRIB file.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6effb88e-6229-441a-84d8-fc36dbec0667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Process Monthly Data into Yearly Growing-Season Metrics\n",
    "\n",
    "# 1. Define Growing Season: For US Maize, we'll assume April-September (months 4-9)\n",
    "growing_season_climate = ds_climate_monthly.where(ds_climate_monthly['time.month'].isin(range(4, 10)), drop=True)\n",
    "\n",
    "# 2. Aggregate to a single metric for the year: Average growing season temperature\n",
    "# We take the mean across the 'time' (monthly) dimension.\n",
    "avg_growing_season_temp = growing_season_climate['t2m'].mean(dim='time') - 273.15 # Convert to Celsius\n",
    "\n",
    "# 3. Rename coordinates to match yield data ('lat', 'lon')\n",
    "avg_growing_season_temp = avg_growing_season_temp.rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "\n",
    "# 4. Align Grids using Nearest Neighbor Interpolation\n",
    "# As the supervisor suggested, we'll reindex the climate data to the yield grid.\n",
    "print(\"Aligning climate grid to yield grid using 'nearest' method...\")\n",
    "temp_aligned = avg_growing_season_temp.reindex_like(ds_yield, method='nearest')\n",
    "\n",
    "# 5. Combine into a single analysis dataset\n",
    "analysis_ds = xr.Dataset({\n",
    "    'yield': ds_yield['var'],\n",
    "    'growing_season_temp': temp_aligned\n",
    "})\n",
    "\n",
    "print(\"\\nData processed and aligned successfully.\")\n",
    "print(analysis_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00979c74-96da-40f9-a805-b082e71f0f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create the Scatter Plot\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = analysis_ds.to_dataframe().dropna()\n",
    "print(f\"Plotting {len(df)} data points...\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(data=df, x='growing_season_temp', y='yield', \n",
    "            scatter_kws={'alpha':0.2, 's':10}, \n",
    "            line_kws={'color':'red'})\n",
    "\n",
    "plt.title('Maize Yield vs. Avg. Growing Season Temperature (1995)')\n",
    "plt.xlabel('Average Temperature April-September (C)')\n",
    "plt.ylabel('Maize Yield (tonnes per hectare)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3511dfb7-6e1b-4472-9e45-2a7327cb88ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 14-Monthly-Data-Workflow-v2.ipynb/14-Monthly-Data-Workflow-v2.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be545b10-4c84-42e1-9ea7-68851211a512",
   "metadata": {},
   "source": [
    "# Monthly Data Workflow v2: Downloading and Verifying Multi-Variable Data\n",
    "\n",
    "**Goal:** To download one full year (1995) of monthly-averaged data for our key climate variables and perform a sanity check to ensure the file is valid and readable. This workflow is based on the successful test download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4ab1a-3cac-4d52-9511-f73ff550e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Download Multi-Variable Data for 1995\n",
    "import cdsapi\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "output_dir = '../data/climate_monthly/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'era5_land_monthly_1995_multi-variable.grib')\n",
    "\n",
    "# --- API Request ---\n",
    "request_dictionary = {\n",
    "    'product_type': 'monthly_averaged_reanalysis',\n",
    "    'variable': [\n",
    "        '2m_temperature', 'total_precipitation', 'volumetric_soil_water_layer_1',\n",
    "        'surface_net_solar_radiation', 'potential_evaporation',\n",
    "    ],\n",
    "    'year': '1995',\n",
    "    'month': [f'{m:02d}' for m in range(1, 13)],\n",
    "    'time': '00:00',\n",
    "    'format': 'grib',\n",
    "    # The crucial key that ensures a clean, unarchived file\n",
    "    'download_format': 'unarchived',\n",
    "}\n",
    "\n",
    "# --- Execute Download ---\n",
    "try:\n",
    "    if not os.path.exists(output_file):\n",
    "        c = cdsapi.Client()\n",
    "        print(\"Submitting API request for 1995 multi-variable monthly data...\")\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land-monthly-means',\n",
    "            request_dictionary,\n",
    "            output_file\n",
    "        )\n",
    "        print(f\"\\nDownload complete! File saved to: {output_file}\")\n",
    "        print(f\"File size: {os.path.getsize(output_file) / 1e6:.2f} MB\")\n",
    "    else:\n",
    "        print(f\"File already exists, skipping download: {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7949d-4663-48c4-912a-f06a62b6af43",
   "metadata": {},
   "source": [
    "## Sanity Check\n",
    "\n",
    "Now we will attempt to open the GRIB file we just downloaded to confirm it is valid and contains all the variables we requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c4a58-e266-4a5a-9c4c-caec7c51a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Sanity Check the Downloaded File\n",
    "import xarray as xr\n",
    "\n",
    "# --- File Path ---\n",
    "file_path = '../data/climate_monthly/era5_land_monthly_1995_multi-variable.grib'\n",
    "\n",
    "# --- Attempt to Open the File ---\n",
    "try:\n",
    "    # We use the cfgrib engine, which we know works with the server's GRIB format.\n",
    "    ds_climate = xr.open_dataset(file_path, engine='cfgrib')\n",
    "    \n",
    "    print(\"--- SUCCESS! ---\")\n",
    "    print(\"The multi-variable GRIB file was loaded correctly.\")\n",
    "    print(\"\\nDataset structure:\")\n",
    "    print(ds_climate)\n",
    "    \n",
    "    print(\"\\nVariables found in file:\")\n",
    "    for var in ds_climate.data_vars:\n",
    "        print(f\"- {var}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"--- FAILED ---\")\n",
    "    print(f\"An error occurred while trying to open the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50ae28-0a0e-4dae-9a07-51138154a972",
   "metadata": {},
   "source": [
    "## Analysis: Linking Yield to Climate (1995)\n",
    "\n",
    "Now that we have successfully downloaded the multi-variable climate data for 1995, we will process it and link it to the 1995 maize yield data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696cf4a9-f4d8-4b85-8a49-f215e60a7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 (Corrected): Load Data for Analysis using correct GRIB short names\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load 1995 Maize Yield Data ---\n",
    "YIELD_PATH = '../data/maize/yield_1995.nc4'\n",
    "ds_yield = xr.open_dataset(YIELD_PATH)\n",
    "print(\"--- Yield Data for 1995 ---\")\n",
    "print(ds_yield)\n",
    "\n",
    "# --- 2. Load 1995 Monthly Climate Data (Variable by Variable) ---\n",
    "CLIMATE_PATH = '../data/climate_monthly/era5_land_monthly_1995_multi-variable.grib'\n",
    "\n",
    "# Define the correct GRIB short names for the variables we want.\n",
    "variables_to_load = {\n",
    "    '2t': '2m_temperature',\n",
    "    'tp': 'total_precipitation',\n",
    "    'swvl1': 'volumetric_soil_water_layer_1',\n",
    "    'ssr': 'surface_net_solar_radiation',\n",
    "    'pev': 'potential_evaporation'\n",
    "}\n",
    "\n",
    "data_arrays = []\n",
    "print(\"\\n--- Loading Monthly Climate Data for 1995 (Variable by Variable) ---\")\n",
    "\n",
    "for short_name in variables_to_load.keys():\n",
    "    try:\n",
    "        # Open the SAME file multiple times, but filter for only ONE variable each time\n",
    "        ds_single_var = xr.open_dataset(\n",
    "            CLIMATE_PATH, \n",
    "            engine='cfgrib',\n",
    "            backend_kwargs={'filter_by_keys': {'shortName': short_name}}\n",
    "        )\n",
    "        print(f\"Successfully loaded '{short_name}'\")\n",
    "        data_arrays.append(ds_single_var)\n",
    "    except Exception as e:\n",
    "        # This will now correctly tell us if a variable like 'swvl1' is truly missing\n",
    "        print(f\"Could not load variable with shortName '{short_name}'. It may not be in the file. Skipping.\")\n",
    "\n",
    "# --- 3. Merge the individual variables into a single Dataset ---\n",
    "if data_arrays:\n",
    "    # Use compat='no_conflicts' to drop the conflicting 'step' coordinate\n",
    "    ds_climate_monthly = xr.merge(data_arrays, compat='no_conflicts')\n",
    "    \n",
    "    print(\"\\n--- Merged Monthly Climate Data for 1995 ---\")\n",
    "    print(ds_climate_monthly)\n",
    "else:\n",
    "    print(\"No climate variables were successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9ecd0-0f5a-4998-a476-57f85a5c9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to verify all variables using the correct method\n",
    "import xarray as xr\n",
    "\n",
    "# --- Configuration ---\n",
    "file_path = r'../data/climate_monthly/era5_land_monthly_1995_multi-variable.grib'\n",
    "\n",
    "# The correct GRIB short names for the variables we requested.\n",
    "variables_to_check = {\n",
    "    '2t': '2m_temperature',\n",
    "    'tp': 'total_precipitation',\n",
    "    'swvl1': 'volumetric_soil_water_layer_1',\n",
    "    'ssr': 'surface_net_solar_radiation',\n",
    "    'pev': 'potential_evaporation'\n",
    "}\n",
    "\n",
    "print(f\"--- Verifying variables in file: {file_path} ---\")\n",
    "\n",
    "# --- Loop and Check Each Variable ---\n",
    "for short_name, long_name in variables_to_check.items():\n",
    "    print(f\"\\nChecking for: {long_name} (shortName: {short_name})\")\n",
    "    try:\n",
    "        # Using the correct method you provided\n",
    "        ds_var = xr.open_dataset(\n",
    "            file_path,\n",
    "            engine=\"cfgrib\",\n",
    "            backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": {\"shortName\": short_name}},\n",
    "        )\n",
    "        print(f\"  -> SUCCESS: Found and loaded '{long_name}'.\")\n",
    "        # print(ds_var) # Optional: uncomment to see the structure\n",
    "    except Exception as e:\n",
    "        print(f\"  -> FAILED: Variable '{long_name}' not found or could not be loaded. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 15-Visualize-Climate-Data.ipynb/15-Visualize-Climate-Data.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45689aa7-cf60-4234-bc6a-76806c0e8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Visualize All Downloaded Climate Variables for Jan 1995\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# --- Configuration ---\n",
    "file_path = r'../data/climate_monthly/era5_land_monthly_1995_multi-variable.grib'\n",
    "\n",
    "# Define the variables we want to see, their GRIB short names, and their units\n",
    "variables_to_plot = {\n",
    "    '2t': {'name': '2m Temperature', 'units': 'C'},\n",
    "    'tp': {'name': 'Total Precipitation', 'units': 'mm'},\n",
    "    'swvl1': {'name': 'Volumetric Soil Water (Layer 1)', 'units': 'm/m'},\n",
    "    'ssr': {'name': 'Surface Net Solar Radiation', 'units': 'J/m'},\n",
    "    'pev': {'name': 'Potential Evaporation', 'units': 'm'}\n",
    "}\n",
    "\n",
    "print(\"--- Generating a map for each climate variable for January 1995 ---\")\n",
    "\n",
    "# --- Loop Through Each Variable and Create a Map ---\n",
    "for short_name, info in variables_to_plot.items():\n",
    "    print(f\"\\nProcessing: {info['name']}\")\n",
    "    \n",
    "    try:\n",
    "        # --- Load ONE variable using the correct, working method ---\n",
    "        ds = xr.open_dataset(\n",
    "            file_path,\n",
    "            engine=\"cfgrib\",\n",
    "            backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": {\"shortName\": short_name}},\n",
    "        )\n",
    "        \n",
    "        # Extract the data array for the variable (the name might be different from the shortName)\n",
    "        # We find the variable name by looking at what's in the dataset\n",
    "        var_name = list(ds.data_vars)[0]\n",
    "        data_array = ds[var_name]\n",
    "\n",
    "        # Select the data for January 1995\n",
    "        data_jan_1995 = data_array.sel(time=\"1995-01-01\")\n",
    "\n",
    "        # --- Unit Conversions ---\n",
    "        if short_name == '2t': # Temperature\n",
    "            data_jan_1995 = data_jan_1995 - 273.15\n",
    "        if short_name == 'tp': # Precipitation\n",
    "            data_jan_1995 = data_jan_1995 * 1000 # Convert m to mm\n",
    "\n",
    "        # --- Plotting ---\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "        ax.coastlines()\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray')\n",
    "        \n",
    "        # Use a suitable colormap for each variable\n",
    "        cmap = 'coolwarm' if short_name == '2t' else 'viridis'\n",
    "\n",
    "        data_jan_1995.plot(\n",
    "            ax=ax,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            cmap=cmap,\n",
    "            cbar_kwargs={'label': f\"{info['name']} ({info['units']})\"}\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"{info['name']} - January 1995\")\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  -> FAILED to process and plot '{info['name']}'. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 16-Download-All-Monthly-Climate-Data.ipynb/16-Download-All-Monthly-Climate-Data.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2db33-6248-4387-8799-99ae88fb3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Bulk Download All Monthly Climate Data with Verification (1981-2016)\n",
    "import cdsapi\n",
    "import os\n",
    "import time\n",
    "import xarray as xr\n",
    "\n",
    "# --- Configuration ---\n",
    "# A dedicated folder for the full monthly dataset\n",
    "output_dir = '../data/climate_monthly_full/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# The full range of years for our analysis\n",
    "years_to_download = range(1981, 2017)\n",
    "\n",
    "# The variables we want to download\n",
    "variables_to_download = [\n",
    "    '2m_temperature', 'total_precipitation', 'volumetric_soil_water_layer_1',\n",
    "    'surface_net_solar_radiation', 'potential_evaporation',\n",
    "]\n",
    "\n",
    "# --- API Request Loop ---\n",
    "c = cdsapi.Client()\n",
    "\n",
    "print(f\"Starting robust bulk download for {len(years_to_download)} years.\")\n",
    "print(f\"Files will be saved in: {output_dir}\")\n",
    "\n",
    "for year in years_to_download:\n",
    "    output_file = os.path.join(output_dir, f'era5_land_monthly_{year}.grib')\n",
    "    \n",
    "    # This check makes the script resumable if it gets interrupted\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"File for year {year} already exists. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--------------------------------------------------\")\n",
    "    print(f\"Submitting request for year: {year}\")\n",
    "    \n",
    "    try:\n",
    "        request_dictionary = {\n",
    "            'product_type': 'monthly_averaged_reanalysis',\n",
    "            'variable': variables_to_download,\n",
    "            'year': str(year),\n",
    "            'month': [f'{m:02d}' for m in range(1, 13)],\n",
    "            'time': '00:00',\n",
    "            'format': 'grib',\n",
    "            'download_format': 'unarchived',\n",
    "        }\n",
    "\n",
    "        # --- DOWNLOAD STEP ---\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land-monthly-means',\n",
    "            request_dictionary,\n",
    "            output_file\n",
    "        )\n",
    "        print(f\"Download for year {year} complete! File size: {os.path.getsize(output_file) / 1e6:.2f} MB\")\n",
    "        \n",
    "        # --- VERIFICATION STEP ---\n",
    "        print(f\"  -> Verifying file integrity for {year}...\")\n",
    "        try:\n",
    "            # Attempt to open the file we just downloaded, checking for one variable\n",
    "            ds_check = xr.open_dataset(\n",
    "                output_file,\n",
    "                engine=\"cfgrib\",\n",
    "                backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": {\"shortName\": \"2t\"}}\n",
    "            )\n",
    "            print(f\"  -> SUCCESS: File for {year} is valid.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  -> !!! FAILED VERIFICATION: File for {year} is corrupted or unreadable. Deleting it.\")\n",
    "            print(f\"     Error was: {e}\")\n",
    "            os.remove(output_file) # Delete the bad file so we can try again later\n",
    "\n",
    "        # A small polite pause between requests\n",
    "        time.sleep(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during download for year {year}: {e}\")\n",
    "        print(\"The script will continue with the next year.\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n--- All downloads finished! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 17-Verify-Climate-Data-Content.ipynb/17-Verify-Climate-Data-Content.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efbc8a3f-74ca-4a7b-be3f-582e6ff0ce94",
   "metadata": {},
   "source": [
    "# Visual Verification of Climate Data Content\n",
    "\n",
    "**Goal:** To visually confirm that the downloaded yearly GRIB files contain unique and variable data, despite having the same file size. We will plot samples from different years and for different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53a5a1-9919-4657-8c32-a27a482a0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Reusable Plotting Function\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "\n",
    "def plot_climate_variable(year, month, variable_short_name, variable_long_name, units, cmap='viridis'):\n",
    "    \"\"\"\n",
    "    Loads a specific variable for a specific month and year, and plots it on a map.\n",
    "    \"\"\"\n",
    "    print(f\"--- Plotting: {variable_long_name} for {year}-{month:02d} ---\")\n",
    "    \n",
    "    # --- Configuration ---\n",
    "    file_path = f'../data/climate_monthly_full/era5_land_monthly_{year}.grib'\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"ERROR: File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # --- Load the specific variable from the file ---\n",
    "        ds = xr.open_dataset(\n",
    "            file_path,\n",
    "            engine=\"cfgrib\",\n",
    "            backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": {\"shortName\": variable_short_name}},\n",
    "        )\n",
    "        \n",
    "        # Find the variable's name in the dataset (e.g., 't2m' for shortName '2t')\n",
    "        var_name = list(ds.data_vars)[0]\n",
    "        data_array = ds[var_name]\n",
    "\n",
    "        # Select the data for the specific month\n",
    "        # The monthly data is timestamped on the 1st of the month\n",
    "        date_str = f\"{year}-{month:02d}-01\"\n",
    "        data_month = data_array.sel(time=date_str)\n",
    "\n",
    "        # --- Unit Conversions ---\n",
    "        if variable_short_name == '2t':\n",
    "            data_month = data_month - 273.15 # Kelvin to Celsius\n",
    "        if variable_short_name == 'tp':\n",
    "            data_month = data_month * 1000 # m to mm\n",
    "\n",
    "        # --- Plotting ---\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "        ax.coastlines()\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray')\n",
    "        \n",
    "        data_month.plot(\n",
    "            ax=ax,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            cmap=cmap,\n",
    "            cbar_kwargs={'label': f\"{variable_long_name} ({units})\"}\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"{variable_long_name} - {year}-{month:02d}\")\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  -> FAILED to process and plot. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2365acad-faec-49c8-a414-2658c94bdfff",
   "metadata": {},
   "source": [
    "## Comparison 1: Same Month, Different Years\n",
    "\n",
    "Let's compare the temperature in July for an early year (1985) versus a late year (2015). If the data is unique, these maps should look different, likely showing a warming trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762e0bd-899e-46a9-81b5-f70edcc8cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Temperature Comparison\n",
    "\n",
    "# Plot July temperature for an early year\n",
    "plot_climate_variable(year=1985, month=7, \n",
    "                      variable_short_name='2t', variable_long_name='2m Temperature', \n",
    "                      units='C', cmap='coolwarm')\n",
    "\n",
    "# Plot July temperature for a late year\n",
    "plot_climate_variable(year=2015, month=7, \n",
    "                      variable_short_name='2t', variable_long_name='2m Temperature', \n",
    "                      units='C', cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3adf18-7cf0-4aac-86e7-e36a9a884e44",
   "metadata": {},
   "source": [
    "## Comparison 2: Same Year, Different Variables\n",
    "\n",
    "Now let's compare two different variables (Temperature and Precipitation) for the exact same month and year (July 1995). This will prove that the files contain distinct physical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad03ab4-2d0a-4bf8-9565-697ac217cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Variable Comparison\n",
    "\n",
    "# Plot July temperature for 1995\n",
    "plot_climate_variable(year=1995, month=7, \n",
    "                      variable_short_name='2t', variable_long_name='2m Temperature', \n",
    "                      units='C', cmap='coolwarm')\n",
    "\n",
    "# Plot July precipitation for 1995\n",
    "plot_climate_variable(year=1995, month=7, \n",
    "                      variable_short_name='tp', variable_long_name='Total Precipitation', \n",
    "                      units='mm', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9dddf-f5d2-4830-b313-3cb7674f8739",
   "metadata": {},
   "source": [
    "## Quantitative Comparison: July 1985 vs. July 2015\n",
    "\n",
    "The visual difference between the two temperature maps is subtle. To confirm the data is unique, we will:\n",
    "1.  Calculate and compare summary statistics for both maps.\n",
    "2.  Create a \"difference map\" by subtracting the 1985 data from the 2015 data to highlight the areas of warming and cooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe6e43-57e3-4df7-849a-62997716bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Quantitative and Visual Difference Analysis\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# --- 1. Load the data for both years ---\n",
    "print(\"--- Loading data for comparison ---\")\n",
    "\n",
    "# Load 1985 Data\n",
    "ds_1985 = xr.open_dataset(\n",
    "    '../data/climate_monthly_full/era5_land_monthly_1985.grib',\n",
    "    engine=\"cfgrib\",\n",
    "    backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": {\"shortName\": \"2t\"}},\n",
    ")\n",
    "temp_1985 = ds_1985['t2m'].sel(time=\"1985-07-01\") - 273.15\n",
    "print(\"Loaded July 1985 data.\")\n",
    "\n",
    "# Load 2015 Data\n",
    "ds_2015 = xr.open_dataset(\n",
    "    '../data/climate_monthly_full/era5_land_monthly_2015.grib',\n",
    "    engine=\"cfgrib\",\n",
    "    backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": {\"shortName\": \"2t\"}},\n",
    ")\n",
    "temp_2015 = ds_2015['t2m'].sel(time=\"2015-07-01\") - 273.15\n",
    "print(\"Loaded July 2015 data.\")\n",
    "\n",
    "# --- 2. Create the Summary Statistics Table ---\n",
    "print(\"\\n--- Summary Statistics (C) ---\")\n",
    "\n",
    "stats = {\n",
    "    'July 1985': {\n",
    "        'Mean': temp_1985.mean().item(),\n",
    "        'Min': temp_1985.min().item(),\n",
    "        'Max': temp_1985.max().item(),\n",
    "    },\n",
    "    'July 2015': {\n",
    "        'Mean': temp_2015.mean().item(),\n",
    "        'Min': temp_2015.min().item(),\n",
    "        'Max': temp_2015.max().item(),\n",
    "    }\n",
    "}\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "print(stats_df.round(2))\n",
    "\n",
    "# --- 3. Calculate and Plot the Difference Map ---\n",
    "print(\"\\n--- Plotting the difference between the two maps (2015 minus 1985) ---\")\n",
    "\n",
    "# Calculate the difference\n",
    "temp_difference = temp_1985 - temp_2015\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray')\n",
    "\n",
    "# A diverging colormap is essential for showing differences.\n",
    "# Red = 2015 was warmer. Blue = 2015 was cooler. White = No change.\n",
    "temp_difference.plot(\n",
    "    ax=ax,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap='coolwarm',\n",
    "    center=0, # This centers the colormap on zero\n",
    "    cbar_kwargs={'label': 'Temperature Difference (C)'}\n",
    ")\n",
    "\n",
    "plt.title(\"Temperature Change: July 1985 vs. July 2015\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 18-Scouting-Report-Candidates.ipynb/18-Scouting-Report-Candidates.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "120822d5-74de-46e8-9088-26ccc562e8f4",
   "metadata": {},
   "source": [
    "# Scouting Report: China, Italy, Peru, and Chile\n",
    "\n",
    "**Goal:** To analyze the yield and climate data for our four candidate regions to decide on the best focus for our main analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf9c24a-0959-463b-aa2d-14dcadd7f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Crop Presence Dashboard (Corrected Bounding Boxes)\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# --- Define Bounding Boxes with CORRECTED Longitudes [0 to 360] ---\n",
    "regions = {\n",
    "    'China': {'lon': slice(73, 135), 'lat': slice(18, 54)},\n",
    "    'Italy': {'lon': slice(6, 19), 'lat': slice(35, 47)},\n",
    "    # CORRECTED: Peru's longitude from [-81, -68] to [279, 292]\n",
    "    'Peru': {'lon': slice(279, 292), 'lat': slice(-18, 0)},\n",
    "    # CORRECTED: Chile's longitude from [-74, -64] to [286, 296]\n",
    "    'Chile': {'lon': slice(286, 296), 'lat': slice(-56, -17)}\n",
    "}\n",
    "\n",
    "# --- Load all average yield data ---\n",
    "CROPS = ['maize', 'rice', 'wheat', 'soybean']\n",
    "avg_yields = {}\n",
    "print(\"--- Loading and processing average yield data ---\")\n",
    "\n",
    "for crop in CROPS:\n",
    "    data_path = f'../data/{crop}/yield_*.nc4'\n",
    "    filepaths = sorted(glob.glob(data_path))\n",
    "    years = [int(re.search(r'(\\d{4})\\.nc4$', f).group(1)) for f in filepaths]\n",
    "    with xr.open_mfdataset(data_path, combine='nested', concat_dim='time') as ds:\n",
    "        ds = ds.assign_coords(time=years)\n",
    "        avg_yields[crop] = ds['var'].mean(dim='time').compute()\n",
    "        print(f\"Processed: {crop}\")\n",
    "\n",
    "# --- Create the Dashboard Plot ---\n",
    "fig, axes = plt.subplots(nrows=len(regions), ncols=len(CROPS), figsize=(16, 12), \n",
    "                         subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "fig.suptitle('Crop Presence and Average Yield (1981-2016)', fontsize=16)\n",
    "\n",
    "for i, (region_name, bounds) in enumerate(regions.items()):\n",
    "    for j, crop_name in enumerate(CROPS):\n",
    "        ax = axes[i, j]\n",
    "        # Set the map extent using the original [-180, 180] coordinates for cartopy\n",
    "        original_lon = regions[region_name]['lon']\n",
    "        lon_start = original_lon.start if original_lon.start > 0 else original_lon.start + 360\n",
    "        lon_stop = original_lon.stop if original_lon.stop > 0 else original_lon.stop + 360\n",
    "        \n",
    "        ax.set_extent([regions[region_name]['lon'].start, regions[region_name]['lon'].stop, \n",
    "                       regions[region_name]['lat'].start, regions[region_name]['lat'].stop], \n",
    "                      crs=ccrs.PlateCarree(central_longitude=0))\n",
    "\n",
    "        avg_yields[crop_name].where(avg_yields[crop_name] > 0.1).plot(\n",
    "            ax=ax, transform=ccrs.PlateCarree(), cmap='viridis', add_colorbar=False\n",
    "        )\n",
    "        ax.coastlines()\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        if j == 0:\n",
    "            ax.text(-0.15, 0.5, region_name, va='center', ha='right', fontsize=14, transform=ax.transAxes)\n",
    "        if i == 0:\n",
    "            ax.set_title(crop_name.capitalize())\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a019d-d40c-4fa4-a2f9-1ec4c67f0bcb",
   "metadata": {},
   "source": [
    "## Scouting Question 2: What is the Yield Story?\n",
    "\n",
    "Now that we know which crops are grown where, let's look at their yield trends and variability over time in each region. This will tell us which systems might be most vulnerable or interesting to study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7943300-fbd5-4e9f-9cde-1f83482c0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Regional Yield Time-Series Analysis (Corrected and Improved)\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# We will now attempt to plot ALL major crops for each region\n",
    "CROPS_TO_ANALYZE_ALL = ['maize', 'rice', 'wheat', 'soybean']\n",
    "\n",
    "# --- Load all full yield datasets (if not already loaded) ---\n",
    "if 'full_yields' not in locals():\n",
    "    full_yields = {}\n",
    "    for crop in CROPS_TO_ANALYZE_ALL:\n",
    "        data_path = f'../data/{crop}/yield_*.nc4'\n",
    "        filepaths = sorted(glob.glob(data_path))\n",
    "        years = [int(re.search(r'(\\d{4})\\.nc4$', f).group(1)) for f in filepaths]\n",
    "        with xr.open_mfdataset(data_path, combine='nested', concat_dim='time') as ds:\n",
    "            ds = ds.assign_coords(time=years)\n",
    "            full_yields[crop] = ds['var'].compute()\n",
    "    print(\"Loaded all crop datasets.\")\n",
    "\n",
    "# --- Create Time-Series Plots ---\n",
    "fig, axes = plt.subplots(nrows=len(regions), ncols=1, figsize=(10, 12), sharex=True)\n",
    "fig.suptitle('Yield Trends in Candidate Regions', fontsize=16)\n",
    "\n",
    "for i, (region_name, bounds) in enumerate(regions.items()):\n",
    "    ax = axes[i]\n",
    "    has_data = False # A flag to track if we plot anything for this region\n",
    "    \n",
    "    for crop_name in CROPS_TO_ANALYZE_ALL:\n",
    "        # Select the region from the full dataset\n",
    "        regional_yield = full_yields[crop_name].sel(**bounds)\n",
    "        \n",
    "        # --- CRITICAL CHECK: Does this slice contain any actual data? ---\n",
    "        # We check if there are any non-NaN values.\n",
    "        if regional_yield.notnull().any():\n",
    "            has_data = True # We found data to plot!\n",
    "            \n",
    "            # Calculate the weighted average yield for the region over time\n",
    "            weights = np.cos(np.deg2rad(regional_yield.lat))\n",
    "            mean_regional_yield = regional_yield.weighted(weights).mean(dim=('lat', 'lon'))\n",
    "            \n",
    "            # Plot the time series\n",
    "            mean_regional_yield.plot(ax=ax, label=crop_name.capitalize())\n",
    "    \n",
    "    # --- Improve Plotting ---\n",
    "    if not has_data:\n",
    "        ax.text(0.5, 0.5, 'No significant yield data found in this region', \n",
    "                ha='center', va='center', transform=ax.transAxes, color='red')\n",
    "\n",
    "    ax.set_title(region_name)\n",
    "    ax.set_ylabel('Yield (t/ha)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb36da-d286-40c2-b9cc-4bf0b284569b",
   "metadata": {},
   "source": [
    "## Scouting Question 3: What is the Climate Story?\n",
    "\n",
    "The final step in our scouting report is to see if there is a strong climate signal in our candidate regions. We will load the full 36-year monthly climate dataset and plot the trend of the average growing season temperature for each region. This will show us which regions have experienced the most significant warming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e709eb-37b5-43d8-84ad-66ec869e5a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Regional Climate Trend Analysis (Corrected Averaging Logic)\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Load the full 36-year monthly climate dataset (if not already loaded) ---\n",
    "if 'ds_climate_full' not in locals():\n",
    "    print(\"--- Loading full monthly climate data (1981-2016) ---\")\n",
    "    CLIMATE_PATH = '../data/climate_monthly_full/era5_land_monthly_*.grib'\n",
    "    ds_climate_full = xr.open_mfdataset(\n",
    "        CLIMATE_PATH, \n",
    "        engine='cfgrib',\n",
    "        combine='nested',\n",
    "        concat_dim='time',\n",
    "        backend_kwargs={'filter_by_keys': {'shortName': '2t'}}\n",
    "    )\n",
    "    ds_climate_full = ds_climate_full.assign_coords(time=pd.to_datetime(ds_climate_full.time.values))\n",
    "    print(\"Climate data loaded successfully.\")\n",
    "\n",
    "# --- 2. Define the regions dictionary with the correct coordinate names ---\n",
    "climate_regions = {\n",
    "    'China': {'longitude': slice(73, 135), 'latitude': slice(54, 18)},\n",
    "    'Italy': {'longitude': slice(6, 19), 'latitude': slice(47, 35)},\n",
    "    'Peru': {'longitude': slice(279, 292), 'latitude': slice(0, -18)},\n",
    "    'Chile': {'longitude': slice(286, 296), 'latitude': slice(-17, -56)}\n",
    "}\n",
    "\n",
    "# --- 3. Define the Growing Season Mask (T > 10C) ---\n",
    "temp_celsius = ds_climate_full['t2m'] - 273.15\n",
    "growing_season_mask = temp_celsius > 10\n",
    "\n",
    "# --- 4. Create the Climate Trend Plots ---\n",
    "fig, axes = plt.subplots(nrows=len(climate_regions), ncols=1, figsize=(10, 12), sharex=True)\n",
    "fig.suptitle('Growing Season Temperature Trends in Candidate Regions', fontsize=16)\n",
    "\n",
    "for i, (region_name, bounds) in enumerate(climate_regions.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Select the regional data\n",
    "    regional_temp = temp_celsius.sel(**bounds)\n",
    "    regional_mask = growing_season_mask.sel(**bounds)\n",
    "    \n",
    "    # Apply the mask to get only growing season temperatures\n",
    "    seasonal_data = regional_temp.where(regional_mask)\n",
    "    \n",
    "    # --- THIS IS THE CORRECTED LOGIC ---\n",
    "    # 1. Group by year and average over the months in the season (dim='time')\n",
    "    # 2. Then, average over the spatial dimensions to get ONE number per year\n",
    "    yearly_avg_temp = seasonal_data.groupby('time.year').mean(dim='time').mean(dim=['latitude', 'longitude'])\n",
    "    \n",
    "    # Plot the time series\n",
    "    yearly_avg_temp.plot(ax=ax, label=f'Avg. Growing Season Temp.')\n",
    "    \n",
    "    ax.set_title(region_name)\n",
    "    ax.set_ylabel('Temperature (C)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc2183-250f-46f6-ba24-a13f20fe91f5",
   "metadata": {},
   "source": [
    "## Final Analysis: Plotting Yield Anomaly vs. Temperature\n",
    "\n",
    "We now have all the pieces to make our decision. This final step will create a dashboard of scatter plots, showing the relationship between the de-trended **yield anomaly** and the **average growing season temperature** for each crop in each candidate region.\n",
    "\n",
    "The slope of the line in these plots is a preliminary vulnerability curve. A steep negative slope suggests high vulnerability to warming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701a5f2-7112-4e5a-98c1-ddea5aad36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Yield Anomaly vs. Temperature Scatter Plot Dashboard (Corrected for NaNs)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.signal import detrend\n",
    "\n",
    "# --- Create the Dashboard Plot ---\n",
    "fig, axes = plt.subplots(nrows=len(regions), ncols=len(CROPS), figsize=(16, 12), sharey=True)\n",
    "fig.suptitle('Yield Anomaly vs. Growing Season Temperature', fontsize=16)\n",
    "\n",
    "for i, (region_name, bounds) in enumerate(regions.items()):\n",
    "    climate_bounds = climate_regions[region_name]\n",
    "    \n",
    "    for j, crop_name in enumerate(CROPS):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        regional_yield = full_yields[crop_name].sel(**bounds)\n",
    "        \n",
    "        if regional_yield.notnull().any():\n",
    "            weights = np.cos(np.deg2rad(regional_yield.lat))\n",
    "            yield_ts = regional_yield.weighted(weights).mean(dim=('lat', 'lon'))\n",
    "            \n",
    "            # --- THIS IS THE CORRECTED LOGIC ---\n",
    "            # Check if the calculated time series has any missing values\n",
    "            if yield_ts.isnull().any():\n",
    "                # If yes, use the simpler anomaly method (subtract the mean)\n",
    "                yield_anomaly = yield_ts - yield_ts.mean()\n",
    "            else:\n",
    "                # If no, use the more sophisticated detrend method\n",
    "                yield_anomaly = detrend(yield_ts.values)\n",
    "\n",
    "            # The rest of the code is the same\n",
    "            regional_temp = temp_celsius.sel(**climate_bounds)\n",
    "            regional_mask = growing_season_mask.sel(**climate_bounds)\n",
    "            seasonal_data = regional_temp.where(regional_mask)\n",
    "            yearly_stressor = seasonal_data.groupby('time.year').mean(dim='time').mean(dim=['latitude', 'longitude'])\n",
    "\n",
    "            df = pd.DataFrame({\n",
    "                'temperature': yearly_stressor.values,\n",
    "                'yield_anomaly': yield_anomaly\n",
    "            }).dropna() # Drop rows where either value might be NaN\n",
    "            \n",
    "            if not df.empty:\n",
    "                sns.regplot(data=df, x='temperature', y='yield_anomaly', ax=ax,\n",
    "                            scatter_kws={'alpha': 0.5, 's': 15},\n",
    "                            line_kws={'color': 'red'})\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'Insufficient Data', ha='center', va='center', transform=ax.transAxes)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "        # --- Formatting ---\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f\"{region_name}\\nYield Anomaly (t/ha)\")\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "        if i == 0:\n",
    "            ax.set_title(crop_name.capitalize())\n",
    "        if i == len(regions) - 1:\n",
    "            ax.set_xlabel(\"Avg. Season Temp (C)\")\n",
    "        else:\n",
    "            ax.set_xlabel('')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 19-Growing-Season-Definition.ipynb/19-Growing-Season-Definition.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca7807c-cee7-4a1d-a0e5-878910edc240",
   "metadata": {},
   "source": [
    "# A Data-Driven Definition of the Growing Season\n",
    "\n",
    "**Goal:** To create a dynamic, grid-cell-specific definition of the growing season based on a temperature threshold, as a superior alternative to a simple hemisphere rule.\n",
    "\n",
    "**Methodology:**\n",
    "1.  Load the full 36-year monthly temperature dataset.\n",
    "2.  Define the growing season for each grid cell as any month where the average temperature is above 10C.\n",
    "3.  Calculate the length of the growing season (in months) for each grid cell and map it to verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d1487-5884-4550-8723-6983d08d4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Calculate and Map Growing Season Length\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# --- 1. Load the full 36-year monthly climate dataset ---\n",
    "print(\"--- Loading full monthly climate data (1981-2016) ---\")\n",
    "CLIMATE_PATH = '../data/climate_monthly_full/era5_land_monthly_*.grib'\n",
    "ds_climate_full = xr.open_mfdataset(\n",
    "    CLIMATE_PATH, \n",
    "    engine='cfgrib',\n",
    "    combine='nested',\n",
    "    concat_dim='time',\n",
    "    backend_kwargs={'filter_by_keys': {'shortName': '2t'}}\n",
    ")\n",
    "ds_climate_full = ds_climate_full.assign_coords(time=pd.to_datetime(ds_climate_full.time.values))\n",
    "temp_celsius = ds_climate_full['t2m'] - 273.15\n",
    "print(\"Climate data loaded successfully.\")\n",
    "\n",
    "# --- 2. Define the Growing Season using a Temperature Threshold ---\n",
    "# We define the growing season as any month where the mean temperature is > 10C.\n",
    "# This creates a boolean (True/False) mask for every grid cell for every month.\n",
    "growing_season_mask = temp_celsius > 10\n",
    "\n",
    "# --- 3. Calculate the Length of the Growing Season ---\n",
    "# We group by 'year' and sum the boolean mask. True counts as 1, False as 0.\n",
    "# This gives us the number of months per year that are part of the growing season.\n",
    "months_in_season_per_year = growing_season_mask.groupby('time.year').sum(dim='time')\n",
    "\n",
    "# Now, calculate the average length of the growing season over the whole period for mapping.\n",
    "avg_season_length = months_in_season_per_year.mean(dim='year')\n",
    "\n",
    "# --- 4. Visualize the Result ---\n",
    "print(\"\\n--- Plotting the average length of the growing season (in months) ---\")\n",
    "plt.figure(figsize=(15, 7))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "avg_season_length.where(avg_season_length > 0).plot(\n",
    "    ax=ax,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap='viridis',\n",
    "    cbar_kwargs={'label': 'Average Number of Months per Year with T > 10C'}\n",
    ")\n",
    "\n",
    "plt.title('Data-Driven Average Growing Season Length (1981-2016)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 20-Comprehensive-Stressor-Analysis.ipynb/20-Comprehensive-Stressor-Analysis.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b82ebcd-5908-424d-a22c-7b1b611cf8ff",
   "metadata": {},
   "source": [
    "# Comprehensive Analysis: Raw Yield vs. All Climate Stressors\n",
    "\n",
    "**Goal:** To create an extensive dashboard of scatter plots showing the relationship between raw, un-adjusted crop yield and five key climate stressors for our four candidate regions. This will give us a complete, unfiltered view of the data to help guide our main analysis.\n",
    "\n",
    "**Methodology:**\n",
    "- We will generate one large figure for each country.\n",
    "- Each figure will contain a grid of plots: one for each crop (rows) and each climate stressor (columns).\n",
    "- The yield data is the raw, technology-influenced trend.\n",
    "- Climate stressors are calculated as the mean (for temperature, soil water, radiation, evaporation) or sum (for precipitation) over a data-driven growing season (months with T > 10C)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6512d16-bc60-4f31-a3e6-bf04d95497bf",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6803b-cba5-49b0-a893-1a56188d9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: The Full Dashboard Generation Script\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- 1. Load All Necessary Data (This may take a minute) ---\n",
    "print(\"--- Loading all required datasets ---\")\n",
    "\n",
    "# Load all full yield datasets\n",
    "CROPS = ['maize', 'rice', 'wheat', 'soybean']\n",
    "full_yields = {}\n",
    "for crop in CROPS:\n",
    "    data_path = f'../data/{crop}/yield_*.nc4'\n",
    "    filepaths = sorted(glob.glob(data_path))\n",
    "    years = [int(re.search(r'(\\d{4})\\.nc4$', f).group(1)) for f in filepaths]\n",
    "    with xr.open_mfdataset(data_path, combine='nested', concat_dim='time') as ds:\n",
    "        ds = ds.assign_coords(time=years)\n",
    "        full_yields[crop] = ds['var'].compute()\n",
    "print(\"Loaded all yield data.\")\n",
    "\n",
    "# Load all full climate datasets\n",
    "CLIMATE_PATH_TEMPLATE = '../data/climate_monthly_full/era5_land_monthly_*.grib'\n",
    "STRESSORS = {\n",
    "    '2t': {'name': 'Temperature', 'units': 'C', 'agg': 'mean'},\n",
    "    'tp': {'name': 'Precipitation', 'units': 'mm', 'agg': 'sum'},\n",
    "    'swvl1': {'name': 'Soil Water (L1)', 'units': 'm/m', 'agg': 'mean'},\n",
    "    'ssr': {'name': 'Solar Radiation', 'units': 'J/m', 'agg': 'mean'},\n",
    "    'pev': {'name': 'Potential Evaporation', 'units': 'm', 'agg': 'mean'}\n",
    "}\n",
    "full_climate = {}\n",
    "for short_name in STRESSORS.keys():\n",
    "    ds_var = xr.open_mfdataset(\n",
    "        CLIMATE_PATH_TEMPLATE, engine='cfgrib', combine='nested', concat_dim='time',\n",
    "        backend_kwargs={'filter_by_keys': {'shortName': short_name}}\n",
    "    )\n",
    "    full_climate[short_name] = ds_var.assign_coords(time=pd.to_datetime(ds_var.time.values))\n",
    "print(\"Loaded all climate data.\")\n",
    "\n",
    "# --- 2. Define Regions and Growing Season ---\n",
    "regions = {\n",
    "    'China': {'lon': slice(73, 135), 'lat': slice(18, 54)},\n",
    "    'Italy': {'lon': slice(6, 19), 'lat': slice(35, 47)},\n",
    "    'Peru': {'lon': slice(279, 292), 'lat': slice(-18, 0)},\n",
    "    'Chile': {'lon': slice(286, 296), 'lat': slice(-56, -17)}\n",
    "}\n",
    "climate_regions = {\n",
    "    'China': {'longitude': slice(73, 135), 'latitude': slice(54, 18)},\n",
    "    'Italy': {'longitude': slice(6, 19), 'latitude': slice(47, 35)},\n",
    "    'Peru': {'longitude': slice(279, 292), 'latitude': slice(0, -18)},\n",
    "    'Chile': {'longitude': slice(286, 296), 'latitude': slice(-17, -56)}\n",
    "}\n",
    "temp_celsius = full_climate['2t']['t2m'] - 273.15\n",
    "growing_season_mask = temp_celsius > 10\n",
    "print(\"Defined regions and growing season mask.\")\n",
    "\n",
    "# --- 3. Main Plotting Loop ---\n",
    "for region_name, bounds in regions.items():\n",
    "    print(f\"\\n--- Generating dashboard for {region_name} ---\")\n",
    "    fig, axes = plt.subplots(nrows=len(CROPS), ncols=len(STRESSORS), figsize=(20, 15), sharex='col')\n",
    "    fig.suptitle(f'Raw Yield vs. Climate Stressors in {region_name}', fontsize=20)\n",
    "    \n",
    "    for i, crop_name in enumerate(CROPS):\n",
    "        for j, (short_name, info) in enumerate(STRESSORS.items()):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # --- Prepare Data for this plot ---\n",
    "            regional_yield = full_yields[crop_name].sel(**bounds)\n",
    "            \n",
    "            if regional_yield.notnull().any():\n",
    "                # 1. Get yield time-series (raw, not de-trended)\n",
    "                weights = np.cos(np.deg2rad(regional_yield.lat))\n",
    "                yield_ts = regional_yield.weighted(weights).mean(dim=('lat', 'lon'))\n",
    "                \n",
    "                # 2. Get climate time-series\n",
    "                var_name = list(full_climate[short_name].data_vars)[0]\n",
    "                regional_climate = full_climate[short_name][var_name].sel(**climate_regions[region_name])\n",
    "                regional_mask = growing_season_mask.sel(**climate_regions[region_name])\n",
    "                seasonal_data = regional_climate.where(regional_mask)\n",
    "                \n",
    "                # Apply the correct aggregation method\n",
    "                if info['agg'] == 'sum':\n",
    "                    yearly_stressor = seasonal_data.groupby('time.year').sum(dim='time').mean(dim=['latitude', 'longitude'])\n",
    "                else: # 'mean'\n",
    "                    yearly_stressor = seasonal_data.groupby('time.year').mean(dim='time').mean(dim=['latitude', 'longitude'])\n",
    "                \n",
    "                # Unit conversions\n",
    "                if short_name == '2t': yearly_stressor -= 273.15\n",
    "                if short_name == 'tp': yearly_stressor *= 1000\n",
    "\n",
    "                # 3. Combine and Plot\n",
    "                df = pd.DataFrame({'stressor': yearly_stressor.values, 'yield': yield_ts.values}).dropna()\n",
    "                if not df.empty:\n",
    "                    sns.regplot(data=df, x='stressor', y='yield', ax=ax,\n",
    "                                scatter_kws={'alpha': 0.5, 's': 15})\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "            # --- Formatting ---\n",
    "            if j == 0: ax.set_ylabel(f\"{crop_name.capitalize()}\\nYield (t/ha)\")\n",
    "            if i == 0: ax.set_title(info['name'])\n",
    "            if i == len(CROPS) - 1: ax.set_xlabel(info['units'])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- All dashboards generated! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 21-Italy-Proposal-Analysis.ipynb/21-Italy-Proposal-Analysis.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e306c-fdd0-4fce-aa84-5c71c93dcd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Italy Correlation Analysis (Corrected Aggregation)\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import detrend\n",
    "\n",
    "# --- 1. Load All Data ---\n",
    "print(\"--- Loading all required datasets ---\")\n",
    "CROPS = ['maize', 'rice', 'wheat', 'soybean']\n",
    "full_yields = {crop: xr.open_mfdataset(f'../data/{crop}/yield_*.nc4', combine='nested', concat_dim='time')['var'].compute() for crop in CROPS}\n",
    "STRESSORS = {'2t': 'Temperature', 'tp': 'Precipitation', 'swvl1': 'Soil Water (L1)', 'ssr': 'Solar Radiation', 'pev': 'Potential Evaporation'}\n",
    "full_climate = {short_name: xr.open_mfdataset(f'../data/climate_monthly_full/era5_land_monthly_*.grib', engine='cfgrib', combine='nested', concat_dim='time', backend_kwargs={'filter_by_keys': {'shortName': short_name}}) for short_name in STRESSORS.keys()}\n",
    "for ds in full_climate.values(): ds.time.values[:] = pd.to_datetime(ds.time.values)\n",
    "\n",
    "# --- 2. Define Regions and Growing Season ---\n",
    "climate_region = {'longitude': slice(6, 19), 'latitude': slice(47, 35)}\n",
    "yield_region = {'lon': slice(6, 19), 'lat': slice(35, 47)}\n",
    "temp_celsius = full_climate['2t']['t2m'] - 273.15\n",
    "growing_season_mask = temp_celsius > 10\n",
    "\n",
    "# --- 3. Calculate Regional Time-Series ---\n",
    "all_series = {}\n",
    "# Yield Anomalies\n",
    "for crop in CROPS:\n",
    "    regional_yield = full_yields[crop].sel(**yield_region)\n",
    "    if regional_yield.notnull().any():\n",
    "        weights = np.cos(np.deg2rad(regional_yield.lat))\n",
    "        yield_ts = regional_yield.weighted(weights).mean(dim=('lat', 'lon'))\n",
    "        if yield_ts.isnull().any():\n",
    "            all_series[f'{crop}_yield_anomaly'] = (yield_ts - yield_ts.mean()).values\n",
    "        else:\n",
    "            all_series[f'{crop}_yield_anomaly'] = detrend(yield_ts.values)\n",
    "\n",
    "# Climate Stressors\n",
    "for short_name, long_name in STRESSORS.items():\n",
    "    var_name = list(full_climate[short_name].data_vars)[0]\n",
    "    regional_climate = full_climate[short_name][var_name].sel(**climate_region)\n",
    "    seasonal_data = regional_climate.where(growing_season_mask.sel(**climate_region))\n",
    "    agg_method = 'sum' if short_name == 'tp' else 'mean'\n",
    "    \n",
    "    # --- THIS IS THE CORRECTED LOGIC ---\n",
    "    grouped_by_year = seasonal_data.groupby('time.year')\n",
    "    if agg_method == 'sum':\n",
    "        yearly_agg = grouped_by_year.sum(dim='time')\n",
    "    else: # agg_method is 'mean'\n",
    "        yearly_agg = grouped_by_year.mean(dim='time')\n",
    "    yearly_stressor = yearly_agg.mean(dim=['latitude', 'longitude'])\n",
    "    # --- END CORRECTION ---\n",
    "\n",
    "    if short_name == '2t': yearly_stressor -= 273.15\n",
    "    if short_name == 'tp': yearly_stressor *= 1000\n",
    "    all_series[long_name] = yearly_stressor.values\n",
    "\n",
    "# --- 4. Create and Plot Correlation Matrix ---\n",
    "df_italy = pd.DataFrame(all_series)\n",
    "corr_matrix = df_italy.corr()\n",
    "yield_stressor_corr = corr_matrix.loc[[c for c in df_italy.columns if 'yield' in c], [c for c in df_italy.columns if 'yield' not in c]]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(yield_stressor_corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation between Yield Anomaly and Climate Stressors in Italy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 22-China-Proposal-Analysis.ipynb/22-China-Proposal-Analysis.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f19a78-bcfe-4b45-abe6-11e5beb2884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Italy Correlation Analysis (Corrected Aggregation)\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import detrend\n",
    "\n",
    "# --- 1. Load All Data ---\n",
    "print(\"--- Loading all required datasets ---\")\n",
    "CROPS = ['maize', 'rice', 'wheat', 'soybean']\n",
    "full_yields = {crop: xr.open_mfdataset(f'../data/{crop}/yield_*.nc4', combine='nested', concat_dim='time')['var'].compute() for crop in CROPS}\n",
    "STRESSORS = {'2t': 'Temperature', 'tp': 'Precipitation', 'swvl1': 'Soil Water (L1)', 'ssr': 'Solar Radiation', 'pev': 'Potential Evaporation'}\n",
    "full_climate = {short_name: xr.open_mfdataset(f'../data/climate_monthly_full/era5_land_monthly_*.grib', engine='cfgrib', combine='nested', concat_dim='time', backend_kwargs={'filter_by_keys': {'shortName': short_name}}) for short_name in STRESSORS.keys()}\n",
    "for ds in full_climate.values(): ds.time.values[:] = pd.to_datetime(ds.time.values)\n",
    "\n",
    "# --- 2. Define Regions and Growing Season ---\n",
    "climate_region = {'longitude': slice(73, 135), 'latitude': slice(54, 18)}\n",
    "yield_region = {'lon': slice(73, 135), 'lat': slice(18, 54)}\n",
    "temp_celsius = full_climate['2t']['t2m'] - 273.15\n",
    "growing_season_mask = temp_celsius > 10\n",
    "\n",
    "# --- 3. Calculate Regional Time-Series ---\n",
    "all_series = {}\n",
    "# Yield Anomalies\n",
    "for crop in CROPS:\n",
    "    regional_yield = full_yields[crop].sel(**yield_region)\n",
    "    if regional_yield.notnull().any():\n",
    "        weights = np.cos(np.deg2rad(regional_yield.lat))\n",
    "        yield_ts = regional_yield.weighted(weights).mean(dim=('lat', 'lon'))\n",
    "        if yield_ts.isnull().any():\n",
    "            all_series[f'{crop}_yield_anomaly'] = (yield_ts - yield_ts.mean()).values\n",
    "        else:\n",
    "            all_series[f'{crop}_yield_anomaly'] = detrend(yield_ts.values)\n",
    "\n",
    "# Climate Stressors\n",
    "for short_name, long_name in STRESSORS.items():\n",
    "    var_name = list(full_climate[short_name].data_vars)[0]\n",
    "    regional_climate = full_climate[short_name][var_name].sel(**climate_region)\n",
    "    seasonal_data = regional_climate.where(growing_season_mask.sel(**climate_region))\n",
    "    agg_method = 'sum' if short_name == 'tp' else 'mean'\n",
    "    \n",
    "    # --- THIS IS THE CORRECTED LOGIC ---\n",
    "    grouped_by_year = seasonal_data.groupby('time.year')\n",
    "    if agg_method == 'sum':\n",
    "        yearly_agg = grouped_by_year.sum(dim='time')\n",
    "    else: # agg_method is 'mean'\n",
    "        yearly_agg = grouped_by_year.mean(dim='time')\n",
    "    yearly_stressor = yearly_agg.mean(dim=['latitude', 'longitude'])\n",
    "    # --- END CORRECTION ---\n",
    "\n",
    "    if short_name == '2t': yearly_stressor -= 273.15\n",
    "    if short_name == 'tp': yearly_stressor *= 1000\n",
    "    all_series[long_name] = yearly_stressor.values\n",
    "\n",
    "# --- 4. Create and Plot Correlation Matrix ---\n",
    "df_italy = pd.DataFrame(all_series)\n",
    "corr_matrix = df_italy.corr()\n",
    "yield_stressor_corr = corr_matrix.loc[[c for c in df_italy.columns if 'yield' in c], [c for c in df_italy.columns if 'yield' not in c]]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(yield_stressor_corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation between Yield Anomaly and Climate Stressors in China (Overall)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10d336-f54a-4892-aa5a-2baeb4a1f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: North vs. South China Comparison (Corrected)\n",
    "import pandas as pd\n",
    "from scipy.signal import detrend\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Define North and South Bounding Boxes ---\n",
    "# North China Plain (Wheat/Maize belt)\n",
    "climate_north = {'longitude': slice(110, 122), 'latitude': slice(42, 32)}\n",
    "yield_north = {'lon': slice(110, 122), 'lat': slice(32, 42)}\n",
    "# South China (Rice belt)\n",
    "climate_south = {'longitude': slice(105, 122), 'latitude': slice(32, 22)}\n",
    "yield_south = {'lon': slice(105, 122), 'lat': slice(22, 32)}\n",
    "\n",
    "# --- Function to generate a correlation matrix for a sub-region ---\n",
    "def generate_regional_corr(yield_bounds, climate_bounds):\n",
    "    all_series = {}\n",
    "    for crop in CROPS:\n",
    "        regional_yield = full_yields[crop].sel(**yield_bounds)\n",
    "        if regional_yield.notnull().any():\n",
    "            weights = np.cos(np.deg2rad(regional_yield.lat))\n",
    "            yield_ts = regional_yield.weighted(weights).mean(dim=('lat', 'lon'))\n",
    "            if yield_ts.isnull().any():\n",
    "                all_series[f'{crop}_yield_anomaly'] = (yield_ts - yield_ts.mean()).values\n",
    "            else:\n",
    "                all_series[f'{crop}_yield_anomaly'] = detrend(yield_ts.values)\n",
    "    \n",
    "    for short_name, long_name in STRESSORS.items():\n",
    "        var_name = list(full_climate[short_name].data_vars)[0]\n",
    "        regional_climate = full_climate[short_name][var_name].sel(**climate_bounds)\n",
    "        seasonal_data = regional_climate.where(growing_season_mask.sel(**climate_bounds))\n",
    "        agg_method = 'sum' if short_name == 'tp' else 'mean'\n",
    "        \n",
    "        # --- THIS IS THE CORRECTED LOGIC (Copied from Cell 1) ---\n",
    "        grouped_by_year = seasonal_data.groupby('time.year')\n",
    "        if agg_method == 'sum':\n",
    "            yearly_agg = grouped_by_year.sum(dim='time')\n",
    "        else: # agg_method is 'mean'\n",
    "            yearly_agg = grouped_by_year.mean(dim='time')\n",
    "        yearly_stressor = yearly_agg.mean(dim=['latitude', 'longitude'])\n",
    "        # --- END CORRECTION ---\n",
    "\n",
    "        if short_name == '2t': yearly_stressor -= 273.15\n",
    "        if short_name == 'tp': yearly_stressor *= 1000\n",
    "        all_series[long_name] = yearly_stressor.values\n",
    "        \n",
    "    return pd.DataFrame(all_series).corr()\n",
    "\n",
    "# --- Generate and Plot Both Matrices ---\n",
    "corr_north = generate_regional_corr(yield_north, climate_north)\n",
    "corr_south = generate_regional_corr(yield_south, climate_south)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "sns.heatmap(corr_north.loc[[c for c in corr_north.columns if 'yield' in c], [c for c in corr_north.columns if 'yield' not in c]],\n",
    "            annot=True, cmap='coolwarm', center=0, ax=ax1)\n",
    "ax1.set_title('North China Plain')\n",
    "\n",
    "sns.heatmap(corr_south.loc[[c for c in corr_south.columns if 'yield' in c], [c for c in corr_south.columns if 'yield' not in c]],\n",
    "            annot=True, cmap='coolwarm', center=0, ax=ax2)\n",
    "ax2.set_title('South China')\n",
    "\n",
    "plt.suptitle('Regional Heterogeneity of Climate-Yield Correlations in China')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 23-Italy-Maize-Analysis.ipynb/23-Italy-Maize-Analysis.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4054f58-31b1-408d-89d5-a96bbae3680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Load Average Yield Data\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# --- Load all average yield data ---\n",
    "CROPS = ['maize', 'rice', 'wheat', 'soybean']\n",
    "avg_yields = {}\n",
    "print(\"--- Loading and processing average yield data ---\")\n",
    "\n",
    "for crop in CROPS:\n",
    "    data_path = f'../data/{crop}/yield_*.nc4'\n",
    "    filepaths = sorted(glob.glob(data_path))\n",
    "    if not filepaths:\n",
    "        print(f\"No files found for {crop}, skipping.\")\n",
    "        continue\n",
    "    years = [int(re.search(r'(\\d{4})\\.nc4$', f).group(1)) for f in filepaths]\n",
    "    \n",
    "    with xr.open_mfdataset(data_path, combine='nested', concat_dim='time') as ds:\n",
    "        ds = ds.assign_coords(time=years)\n",
    "        # Use dictionary-style access ds['var']\n",
    "        avg_yields[crop] = ds['var'].mean(dim='time').compute()\n",
    "        print(f\"Processed: {crop}\")\n",
    "        \n",
    "print(\"\\nAll average yield data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c783380-a9b1-4465-a579-531832bd3f1c",
   "metadata": {},
   "source": [
    "## Step 1: Defining the \"Northern Italy 4-Crop\" Mask\n",
    "\n",
    "We will create a precise mask to isolate only the grid cells that meet two criteria:\n",
    "1. They are located within our defined bounding box for Northern Italy.\n",
    "2. All four major crops (Maize, Rice, Wheat, Soybean) are grown there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad76ef-e31d-4470-aa13-9a08f87ce974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Create and Visualize the Final Mask\n",
    "\n",
    "# --- 1. Define the Geographical Bounding Box for Northern Italy ---\n",
    "# These are our initial estimates. The map will show if they are good.\n",
    "n_italy_bounds = {'lon': slice(6, 13), 'lat': slice(43, 47)}\n",
    "\n",
    "# Create a boolean mask for the geographical area\n",
    "# We use one of our loaded yield maps as a template for the grid shape\n",
    "template_grid = avg_yields['maize']\n",
    "geo_mask = xr.ones_like(template_grid, dtype=bool).where(\n",
    "    (template_grid.lat >= n_italy_bounds['lat'].start) & (template_grid.lat <= n_italy_bounds['lat'].stop) &\n",
    "    (template_grid.lon >= n_italy_bounds['lon'].start) & (template_grid.lon <= n_italy_bounds['lon'].stop),\n",
    "    False\n",
    ")\n",
    "\n",
    "# --- 2. Create the \"4-Crop\" Mask ---\n",
    "# Start with a mask that is True everywhere\n",
    "four_crop_mask = xr.ones_like(template_grid, dtype=bool)\n",
    "\n",
    "# Use a logical AND to sequentially filter the mask\n",
    "# A grid cell will only remain True if ALL crops are present\n",
    "for crop_name, yield_data in avg_yields.items():\n",
    "    # We consider a crop \"present\" if its average yield is > 0.1 t/ha\n",
    "    four_crop_mask = four_crop_mask & (yield_data > 0.1)\n",
    "\n",
    "# --- 3. Combine the Masks ---\n",
    "# The final mask is True only where BOTH the geo_mask AND four_crop_mask are True\n",
    "final_mask = geo_mask & four_crop_mask\n",
    "print(f\"Found {int(final_mask.sum())} grid cells that meet both criteria.\")\n",
    "\n",
    "# --- 4. Verification Plot ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Set the map extent to zoom in on Italy for a better view\n",
    "ax.set_extent([5, 19, 36, 48], crs=ccrs.PlateCarree())\n",
    "\n",
    "ax.add_feature(cfeature.LAND, facecolor='#f0f0f0')\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "# Plot the final mask. The `where` function will only show the True values.\n",
    "final_mask.where(final_mask).plot(\n",
    "    ax=ax,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap='viridis',\n",
    "    add_colorbar=False\n",
    ")\n",
    "\n",
    "plt.title('Final Mask: Northern Italy Cells with All 4 Crops')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e386f6b-9103-4e7a-84a7-0991a12fdc5b",
   "metadata": {},
   "source": [
    "## Step 2: Process the Maize Yield Data\n",
    "\n",
    "Now that we have our final mask, we will use it to extract the full 36-year time-series of maize yields for our 44 target grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d03c57-c6ae-4945-b363-507b6e72a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Extract, Verify, and Plot Regional Maize Yield\n",
    "\n",
    "# --- 1. Load the full 36-year maize yield data ---\n",
    "CROP_TO_ANALYZE = 'maize'\n",
    "data_path = f'../data/{CROP_TO_ANALYZE}/yield_*.nc4'\n",
    "filepaths = sorted(glob.glob(data_path))\n",
    "years = [int(re.search(r'(\\d{4})\\.nc4$', f).group(1)) for f in filepaths]\n",
    "\n",
    "with xr.open_mfdataset(data_path, combine='nested', concat_dim='time') as ds:\n",
    "    ds = ds.assign_coords(time=years)\n",
    "    full_yield_data = ds['var'].compute()\n",
    "\n",
    "# --- 2. Apply the final_mask ---\n",
    "# The .where() function keeps data where the mask is True and sets everything else to NaN\n",
    "regional_yield = full_yield_data.where(final_mask)\n",
    "\n",
    "# --- 3. Calculate and Plot the Average Trend ---\n",
    "# We calculate the mean over all selected grid cells for each year\n",
    "mean_yield_trend = regional_yield.mean(dim=['lat', 'lon'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "mean_yield_trend.plot()\n",
    "plt.title(f'Average Maize Yield Trend in Northern Italy (44 Grid Cells)')\n",
    "plt.ylabel('Yield (tonnes per hectare)')\n",
    "plt.xlabel('Year')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 4. VISUAL VERIFICATION: Map the Masked Data for a Single Year ---\n",
    "print(\"\\n--- Visual Verification: Displaying masked data for the year 2015 ---\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([5, 19, 36, 48], crs=ccrs.PlateCarree())\n",
    "\n",
    "ax.add_feature(cfeature.LAND, facecolor='#f0f0f0')\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "# Select and plot the data for just one year from our masked dataset\n",
    "regional_yield.sel(time=2015).plot(\n",
    "    ax=ax,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap='viridis'\n",
    ")\n",
    "\n",
    "plt.title('Verification: Masked Maize Yield Data for 2015')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf7047-96c2-4a29-a34d-fac38fe695bf",
   "metadata": {},
   "source": [
    "## Step 3: Process the Climate Data (Temperature)\n",
    "\n",
    "Now we will process the high-resolution monthly temperature data to create a single, analysis-ready time-series of the average growing season temperature for our specific 44-cell region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8c935-2cc9-4dfd-ac4b-f1daf05db9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Process and Align Temperature Data (Corrected for Coordinate Mismatch)\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Research and Define the Growing Season ---\n",
    "growing_season_months = [5, 6, 7, 8, 9]\n",
    "\n",
    "# --- 2. Load the full 36-year monthly temperature data ---\n",
    "print(\"Loading full monthly temperature data (1981-2016)...\")\n",
    "CLIMATE_PATH = '../data/climate_monthly_full/era5_land_monthly_*.grib'\n",
    "ds_temp_full = xr.open_mfdataset(\n",
    "    CLIMATE_PATH, \n",
    "    engine='cfgrib',\n",
    "    combine='nested',\n",
    "    concat_dim='time',\n",
    "    backend_kwargs={'filter_by_keys': {'shortName': '2t'}}\n",
    ")\n",
    "ds_temp_full = ds_temp_full.assign_coords(time=pd.to_datetime(ds_temp_full.time.values))\n",
    "temp_celsius = ds_temp_full['t2m'] - 273.15\n",
    "print(\"Climate data loaded.\")\n",
    "\n",
    "# --- 3. Filter by Season and Aggregate to Yearly Values ---\n",
    "seasonal_temp = temp_celsius.where(temp_celsius['time.month'].isin(growing_season_months))\n",
    "yearly_avg_temp = seasonal_temp.groupby('time.year').mean(dim='time')\n",
    "\n",
    "# --- 4. Align the Grids (Regrid by Averaging) ---\n",
    "# Coarsen to average the 0.1 data into ~0.5 blocks\n",
    "coarsened_temp = yearly_avg_temp.coarsen(latitude=5, longitude=5, boundary='trim').mean()\n",
    "\n",
    "# --- THIS IS THE CRITICAL FIX ---\n",
    "# Rename the coordinates of the climate data to match the yield data's mask\n",
    "coarsened_temp = coarsened_temp.rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "# --- END FIX ---\n",
    "\n",
    "# Now that the names match, we can align them perfectly to the yield grid\n",
    "regridded_temp = coarsened_temp.interp_like(regional_yield, method='nearest')\n",
    "\n",
    "# --- 5. Apply our final_mask ---\n",
    "# This will now work correctly because both datasets have 'lat' and 'lon' coordinates\n",
    "regional_temp_ts = regridded_temp.where(final_mask).mean(dim=['lat', 'lon'])\n",
    "\n",
    "# --- 6. Verification Plot ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "regional_temp_ts.plot()\n",
    "plt.title('Average Growing Season Temperature in Northern Italy (44 Grid Cells)')\n",
    "plt.ylabel('Temperature (C)')\n",
    "plt.xlabel('Year')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Final Processed Temperature Data for Analysis ---\")\n",
    "print(regional_temp_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3993df-0706-47a8-9c76-bbcdf6d86526",
   "metadata": {},
   "source": [
    "## Step 4: Combine and Analyze Yield vs. Temperature\n",
    "\n",
    "Now that we have clean, processed, and aligned time-series for both maize yield and growing season temperature, we can combine them to create our first scientifically sound vulnerability plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33a6b7-c6d3-4018-9e75-aff48e158aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create and Save the Final Compact Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Combine the Processed Data into a DataFrame ---\n",
    "# We use the raw, unprocessed mean_yield_trend from Cell 3\n",
    "# We use the processed regional_temp_ts from Cell 4\n",
    "\n",
    "final_df = pd.DataFrame({\n",
    "    'year': mean_yield_trend.time.values,\n",
    "    'maize_yield': mean_yield_trend.values,\n",
    "    'temperature': regional_temp_ts.values\n",
    "})\n",
    "\n",
    "# --- 2. Save the DataFrame to a CSV file ---\n",
    "# CSV is a simple, universal format (rows and columns) that is perfect for this.\n",
    "output_dir = '../data/analysis_ready/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'n_italy_maize_yearly_data.csv')\n",
    "\n",
    "final_df.to_csv(output_file, index=False)\n",
    "\n",
    "# --- 3. Display the Final DataFrame ---\n",
    "print(\"--- Final, Compact Dataset for Northern Italy Maize ---\")\n",
    "print(f\"Dataset saved to: {output_file}\")\n",
    "print(\"\\nHere is a preview of the first 5 rows:\")\n",
    "print(final_df.head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21516e29-6daa-4431-8667-2085adae283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Cell (Corrected for KeyError): Create the Complete, Unaggregated Dataset\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import detrend\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# --- 1. Define Regions ---\n",
    "yield_region = {'lon': slice(6, 13), 'lat': slice(43, 47)}\n",
    "climate_region = {'longitude': slice(6, 13), 'latitude': slice(47, 43)}\n",
    "\n",
    "# --- 2. Process Yield Data ---\n",
    "print(\"--- Processing Yield Data ---\")\n",
    "CROP_TO_ANALYZE = 'maize'\n",
    "data_path = f'../data/{CROP_TO_ANALYZE}/yield_*.nc4'\n",
    "filepaths = sorted(glob.glob(data_path))\n",
    "years = [int(re.search(r'(\\d{4})\\.nc4$', f).group(1)) for f in filepaths]\n",
    "with xr.open_mfdataset(data_path, combine='nested', concat_dim='time') as ds:\n",
    "    ds = ds.assign_coords(time=years)\n",
    "    regional_yield_lazy = ds['var'].sel(**yield_region)\n",
    "    regional_yield = regional_yield_lazy.compute()\n",
    "regional_yield = regional_yield.where(final_mask.sel(**yield_region))\n",
    "print(\"Yield data for the 44-cell region has been extracted.\")\n",
    "\n",
    "# --- 3. Process All Climate Stressors ---\n",
    "print(\"\\n--- Processing Climate Data ---\")\n",
    "STRESSORS = {\n",
    "    '2t': {'name': 'temperature', 'agg': 'mean'},\n",
    "    'tp': {'name': 'precipitation', 'agg': 'sum'},\n",
    "    'swvl1': {'name': 'soil_water_l1', 'agg': 'mean'},\n",
    "    'ssr': {'name': 'solar_radiation', 'agg': 'mean'},\n",
    "    'pev': {'name': 'potential_evaporation', 'agg': 'mean'}\n",
    "}\n",
    "processed_stressors = {}\n",
    "\n",
    "# Load the temperature data once to create the growing season mask\n",
    "with xr.open_dataset('../data/climate_monthly_full/era5_land_monthly_1995.grib', engine='cfgrib', backend_kwargs={'filter_by_keys': {'shortName': '2t'}}) as ds_temp_sample:\n",
    "    temp_celsius = ds_temp_sample['t2m'].sel(**climate_region) - 273.15\n",
    "    growing_season_mask = temp_celsius > 10\n",
    "\n",
    "for short_name, info in STRESSORS.items():\n",
    "    print(f\"Processing {info['name']}...\")\n",
    "    with xr.open_mfdataset(f'../data/climate_monthly_full/era5_land_monthly_*.grib', engine='cfgrib', combine='nested', concat_dim='time', backend_kwargs={'filter_by_keys': {'shortName': short_name}}) as ds_climate:\n",
    "        ds_climate = ds_climate.assign_coords(time=pd.to_datetime(ds_climate.time.values))\n",
    "        \n",
    "        # --- THIS IS THE CRITICAL FIX ---\n",
    "        # Get the actual variable name from the dataset (e.g., 't2m')\n",
    "        var_name = list(ds_climate.data_vars)[0]\n",
    "        # Now use this correct name to access the data\n",
    "        regional_climate = ds_climate[var_name].sel(**climate_region)\n",
    "        # --- END FIX ---\n",
    "        \n",
    "        seasonal_data = regional_climate.where(growing_season_mask)\n",
    "        if info['agg'] == 'sum':\n",
    "            yearly_agg = seasonal_data.groupby('time.year').sum(dim='time')\n",
    "        else: # 'mean'\n",
    "            yearly_agg = seasonal_data.groupby('time.year').mean(dim='time')\n",
    "            \n",
    "        if short_name == '2t': yearly_agg -= 273.15\n",
    "        if short_name == 'tp': yearly_agg *= 1000\n",
    "            \n",
    "        coarsened = yearly_agg.coarsen(latitude=5, longitude=5, boundary='trim').mean()\n",
    "        coarsened = coarsened.rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "        regridded = coarsened.interp_like(regional_yield, method='nearest')\n",
    "        \n",
    "        processed_stressors[info['name']] = regridded.where(final_mask.sel(**yield_region))\n",
    "\n",
    "# --- 4. Combine into a Single xarray Dataset and then DataFrame ---\n",
    "final_ds = xr.Dataset({'maize_yield': regional_yield, **processed_stressors})\n",
    "final_df = final_ds.to_dataframe().dropna().reset_index().rename(columns={'time': 'year'})\n",
    "\n",
    "# --- 5. Save and Display ---\n",
    "output_dir = '../data/analysis_ready/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'n_italy_maize_gridcell_all_stressors.csv')\n",
    "final_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n--- Final, Compact Dataset (with {len(final_df)} rows) ---\")\n",
    "print(f\"Dataset saved to: {output_file}\")\n",
    "print(\"\\nHere is a preview of the first 5 rows:\")\n",
    "print(final_df.head().to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 24-Final-Dataset-Pipeline.ipynb/24-Final-Dataset-Pipeline.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c99540-595e-411b-bed6-c27f2006e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Load Initial Data\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# --- Load the final_mask we created in notebook 23 ---\n",
    "# We need to recreate it here to be self-contained\n",
    "print(\"--- Recreating the Northern Italy Mask ---\")\n",
    "CROPS = ['maize', 'rice', 'wheat', 'soybean']\n",
    "avg_yields = {}\n",
    "for crop in CROPS:\n",
    "    with xr.open_mfdataset(f'../data/{crop}/yield_*.nc4', combine='nested', concat_dim='time') as ds:\n",
    "        avg_yields[crop] = ds['var'].mean(dim='time').compute()\n",
    "n_italy_bounds = {'lon': slice(6, 13), 'lat': slice(43, 47)}\n",
    "template_grid = avg_yields['maize']\n",
    "geo_mask = xr.ones_like(template_grid, dtype=bool).where((template_grid.lat >= n_italy_bounds['lat'].start) & (template_grid.lat <= n_italy_bounds['lat'].stop) & (template_grid.lon >= n_italy_bounds['lon'].start) & (template_grid.lon <= n_italy_bounds['lon'].stop), False)\n",
    "four_crop_mask = xr.ones_like(template_grid, dtype=bool)\n",
    "for crop_name, yield_data in avg_yields.items():\n",
    "    four_crop_mask = four_crop_mask & (yield_data > 0.1)\n",
    "final_mask = geo_mask & four_crop_mask\n",
    "print(f\"Mask created, selecting {int(final_mask.sum())} grid cells.\")\n",
    "\n",
    "# --- Load the full 36-year maize yield data ---\n",
    "print(\"\\n--- Loading Maize Yield Data ---\")\n",
    "data_path = f'../data/maize/yield_*.nc4'\n",
    "filepaths = sorted(glob.glob(data_path))\n",
    "years = [int(re.search(r'(\\d{4})\\.nc4$', f).group(1)) for f in filepaths]\n",
    "with xr.open_mfdataset(data_path, combine='nested', concat_dim='time') as ds:\n",
    "    ds = ds.assign_coords(time=years)\n",
    "    full_yield_data = ds['var']\n",
    "print(\"Maize yield data loaded.\")\n",
    "\n",
    "# --- Load the full 36-year monthly temperature data ---\n",
    "print(\"\\n--- Loading Temperature Data ---\")\n",
    "CLIMATE_PATH = '../data/climate_monthly_full/era5_land_monthly_*.grib'\n",
    "ds_temp_full = xr.open_mfdataset(\n",
    "    CLIMATE_PATH, engine='cfgrib', combine='nested', concat_dim='time',\n",
    "    backend_kwargs={'filter_by_keys': {'shortName': '2t'}}\n",
    ")\n",
    "ds_temp_full = ds_temp_full.assign_coords(time=pd.to_datetime(ds_temp_full.time.values))\n",
    "print(\"Temperature data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492fd66-a6e1-4249-a0e3-65dd92f2bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Process and Verify Yield Data\n",
    "\n",
    "# Define the yield region\n",
    "yield_region = {'lon': slice(6, 13), 'lat': slice(43, 47)}\n",
    "\n",
    "# 1. Select the region\n",
    "regional_yield_lazy = full_yield_data.sel(**yield_region)\n",
    "\n",
    "# 2. Apply the mask\n",
    "regional_yield = regional_yield_lazy.where(final_mask.sel(**yield_region)).compute()\n",
    "\n",
    "print(\"--- Processed Yield Data for Northern Italy ---\")\n",
    "print(\"This is a 3D data cube: (time, lat, lon)\")\n",
    "print(regional_yield)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012a06a-e3f3-49ed-92b9-3ada44fad650",
   "metadata": {},
   "source": [
    "### Step 2b: Process and Verify Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1508a2e-1fab-46ae-a626-67338023296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Process and Verify Temperature Data\n",
    "\n",
    "# --- 1. Define Growing Season and Region ---\n",
    "growing_season_months = [5, 6, 7, 8, 9]\n",
    "climate_region = {'longitude': slice(6, 13), 'latitude': slice(47, 43)}\n",
    "\n",
    "# --- 2. Slice the global dataset FIRST ---\n",
    "regional_temp_lazy = ds_temp_full['t2m'].sel(**climate_region)\n",
    "\n",
    "# --- 3. Perform calculations on the small, regional slice ---\n",
    "# Filter by season\n",
    "seasonal_temp = regional_temp_lazy.where(regional_temp_lazy['time.month'].isin(growing_season_months))\n",
    "# Aggregate to yearly values\n",
    "yearly_avg_temp = seasonal_temp.groupby('time.year').mean(dim='time')\n",
    "# Convert to Celsius\n",
    "yearly_avg_temp_c = yearly_avg_temp - 273.15\n",
    "\n",
    "# --- 4. Regrid to match the yield data grid ---\n",
    "# Rename coordinates to match for alignment\n",
    "yearly_avg_temp_c = yearly_avg_temp_c.rename({'latitude': 'lat', 'longitude': 'lon'})\n",
    "# Use interp_like for a smooth interpolation\n",
    "regridded_temp = yearly_avg_temp_c.interp_like(regional_yield)\n",
    "\n",
    "# --- 5. Apply the final mask ---\n",
    "regional_temp = regridded_temp.where(final_mask.sel(**yield_region)).compute()\n",
    "\n",
    "# --- 6. Verification ---\n",
    "print(\"--- Processed Temperature Data for Northern Italy ---\")\n",
    "print(\"This is a 3D data cube with the same shape as the yield data:\")\n",
    "print(regional_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab1369-58b6-4ba0-9182-86751d28ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Visual Verification of Processed Temperature Data (Corrected)\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# --- 1. Visual Verification Map ---\n",
    "# Let's plot the data for the year 2003, which was a major heatwave year.\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([5, 19, 42, 48], crs=ccrs.PlateCarree()) # Zoom in on N. Italy\n",
    "\n",
    "ax.add_feature(cfeature.LAND, facecolor='#f0f0f0')\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "# Plot the temperature data for our region for a single year\n",
    "regional_temp.sel(year=2003).plot(\n",
    "    ax=ax,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap='inferno', # A good colormap for temperature\n",
    "    cbar_kwargs={'label': 'Avg. Growing Season Temp (C)'}\n",
    ")\n",
    "plt.title('Verification Map: Processed Temperature for 2003')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 2. Visual Verification Time-Series ---\n",
    "# Calculate the average temperature across the 44 cells for each year\n",
    "mean_temp_trend = regional_temp.mean(dim=['lat', 'lon'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "mean_temp_trend.plot()\n",
    "plt.title('Average Growing Season Temperature Trend in Northern Italy (44 Grid Cells)')\n",
    "plt.ylabel('Temperature (C)')\n",
    "plt.xlabel('Year')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabaee90-a3ef-4715-ba91-252a7807a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Step (Corrected): Create, Verify, and Save the Unaggregated Dataset\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# --- 1. Combine our two processed xarray DataArrays into a single xarray Dataset ---\n",
    "# First, we clean the temperature data by dropping the useless GRIB coordinates.\n",
    "regional_temp_cleaned = regional_temp.drop_vars(['number', 'step', 'surface'], errors='ignore')\n",
    "\n",
    "# Second, we RENAME the 'year' coordinate in the temperature data to 'time' to match the yield data.\n",
    "regional_temp_aligned = regional_temp_cleaned.rename({'year': 'time'})\n",
    "\n",
    "# Now, because the coordinates and dimensions are identical, we can merge them correctly.\n",
    "final_ds = xr.Dataset({\n",
    "    'maize_yield': regional_yield,\n",
    "    'temperature': regional_temp_aligned\n",
    "})\n",
    "\n",
    "# --- 2. Convert the Correct 3D Cube to a Long-Format Pandas DataFrame ---\n",
    "final_df = final_ds.to_dataframe()\n",
    "\n",
    "# --- 3. Clean up the DataFrame ---\n",
    "final_df = final_df.dropna().reset_index()\n",
    "final_df = final_df.rename(columns={'time': 'year'})\n",
    "\n",
    "# --- 4. Save and Display the Final, Correct Dataset ---\n",
    "output_dir = '../data/analysis_ready/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'n_italy_maize_gridcell_temp_data.csv')\n",
    "final_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"--- Final, Compact Dataset for Northern Italy Maize (Cell by Cell) ---\")\n",
    "print(f\"Dataset saved to: {output_file}\")\n",
    "print(f\"The table has {len(final_df)} rows (44 cells * 36 years = 1584).\")\n",
    "print(\"\\nHere is the ENTIRE dataset for verification:\")\n",
    "\n",
    "# Set pandas to display all rows, as you requested.\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(final_df.to_string())\n",
    "pd.reset_option('display.max_rows') # Reset display option for future cells"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 25-Final-Data-Verification.ipynb/25-Final-Data-Verification.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36f6e3d-f68e-4095-b49d-86df0c628dcc",
   "metadata": {},
   "source": [
    "# Final Verification: Side-by-Side Maps of Yield and Temperature\n",
    "\n",
    "**Goal:** To visually inspect our final, cleaned dataset by plotting the maize yield and the corresponding growing season temperature for our Northern Italy region for every year from 1981 to 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b8a71e-2a1d-4864-bc98-92119569c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Verification (Corrected): Side-by-Side Maps of Yield and Temperature\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# --- 1. Load our final, clean dataset ---\n",
    "file_path = '../data/analysis_ready/n_italy_maize_gridcell_temp_data.csv'\n",
    "final_df = pd.read_csv(file_path)\n",
    "print(f\"Loaded the final dataset with {len(final_df)} rows.\")\n",
    "\n",
    "# --- 2. VERIFICATION 1: Count rows per year (as you suggested) ---\n",
    "print(\"\\n--- Verifying number of grid cells per year ---\")\n",
    "rows_per_year = final_df.groupby('year').size()\n",
    "print(rows_per_year.to_string())\n",
    "\n",
    "# --- 3. Clean the Data (as you suggested) ---\n",
    "# Exclude 1981 because it has very sparse data\n",
    "df_cleaned = final_df[final_df['year'] != 1981].copy()\n",
    "print(f\"\\nRemoved 1981 data. The dataset now has {len(df_cleaned)} rows.\")\n",
    "\n",
    "# --- 4. Convert the clean DataFrame back into a gridded xarray Dataset ---\n",
    "# This is the correct way to reconstruct the data cubes for plotting.\n",
    "ds_final = df_cleaned.set_index(['year', 'lat', 'lon']).to_xarray()\n",
    "\n",
    "# --- 5. Determine consistent color scales for plotting ---\n",
    "yield_vmin = ds_final['maize_yield'].min().item()\n",
    "yield_vmax = ds_final['maize_yield'].max().item()\n",
    "temp_vmin = ds_final['temperature'].min().item()\n",
    "temp_vmax = ds_final['temperature'].max().item()\n",
    "\n",
    "# --- 6. Loop through each year and create a pair of maps ---\n",
    "# We loop through the unique years present in our cleaned dataset\n",
    "for year in ds_final.year.values:\n",
    "    print(f\"\\n--- Generating maps for {year} ---\")\n",
    "    \n",
    "    data_for_year = ds_final.sel(year=year)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    \n",
    "    # --- Plot 1: Maize Yield ---\n",
    "    ax1.set_extent([5, 19, 42, 48], crs=ccrs.PlateCarree())\n",
    "    ax1.add_feature(cfeature.LAND, facecolor='#f0f0f0')\n",
    "    ax1.coastlines()\n",
    "    ax1.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    data_for_year['maize_yield'].plot(\n",
    "        ax=ax1, transform=ccrs.PlateCarree(), cmap='viridis',\n",
    "        vmin=yield_vmin, vmax=yield_vmax,\n",
    "        cbar_kwargs={'label': 'Maize Yield (t/ha)'}\n",
    "    )\n",
    "    ax1.set_title(f'Maize Yield in {year}')\n",
    "\n",
    "    # --- Plot 2: Temperature ---\n",
    "    ax2.set_extent([5, 19, 42, 48], crs=ccrs.PlateCarree())\n",
    "    ax2.add_feature(cfeature.LAND, facecolor='#f0f0f0')\n",
    "    ax2.coastlines()\n",
    "    ax2.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    data_for_year['temperature'].plot(\n",
    "        ax=ax2, transform=ccrs.PlateCarree(), cmap='inferno',\n",
    "        vmin=temp_vmin, vmax=temp_vmax,\n",
    "        cbar_kwargs={'label': 'Avg. Growing Season Temp (C)'}\n",
    "    )\n",
    "    ax2.set_title(f'Temperature in {year}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- All verification maps generated. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 26-Identify-Core-Grid-Cells.ipynb/26-Identify-Core-Grid-Cells.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5cab6a-de07-4f8f-9a87-8fda009101bb",
   "metadata": {},
   "source": [
    "# Step 1: Identify the 42 Core Grid Cells\n",
    "\n",
    "**Goal:** To find the exact coordinates of the grid cells that have complete data for the entire period from 1982 to 2016.\n",
    "\n",
    "**Methodology:**\n",
    "1.  Load the clean CSV dataset we created.\n",
    "2.  Exclude the year 1981.\n",
    "3.  Count the number of years each grid cell appears in the data.\n",
    "4.  Keep only the cells that appear in all 35 years (from 1982 to 2016).\n",
    "5.  Save the coordinates of these \"core\" cells to a new file.\n",
    "6.  Visually confirm the result by plotting only these core cells on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60294bce-49c3-4e67-bd02-0a9651b546a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Find, Save, and Visualize the 42 Core Grid Cells\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "\n",
    "# --- 1. Load the dataset ---\n",
    "file_path = '../data/analysis_ready/n_italy_maize_gridcell_temp_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# --- 2. Exclude 1981 data ---\n",
    "df_filtered = df[df['year'] != 1981].copy()\n",
    "\n",
    "# --- 3. Find the cells present in ALL 35 years (1982-2016) ---\n",
    "# Group by each grid cell and count how many years of data it has\n",
    "cell_counts = df_filtered.groupby(['lat', 'lon']).size()\n",
    "\n",
    "# The total number of years we are considering\n",
    "total_years = 1982 - 2016 + 1\n",
    "\n",
    "# Keep only the cells that have a count equal to the total number of years\n",
    "core_cells = cell_counts[cell_counts == 35].reset_index()[['lat', 'lon']]\n",
    "\n",
    "print(f\"--- Found {len(core_cells)} grid cells with complete data from 1982-2016 ---\")\n",
    "\n",
    "# --- 4. Save the coordinates of these core cells ---\n",
    "output_dir = '../data/analysis_ready/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "core_cells_path = os.path.join(output_dir, 'n_italy_core_42_cells.csv')\n",
    "core_cells.to_csv(core_cells_path, index=False)\n",
    "\n",
    "print(f\"Saved the coordinates of the core cells to: {core_cells_path}\")\n",
    "print(\"\\nPreview of the core cells coordinates:\")\n",
    "print(core_cells.head().to_string())\n",
    "\n",
    "# --- 5. VISUAL VERIFICATION ---\n",
    "print(\"\\n--- Visualizing the {len(core_cells)} Core Grid Cells ---\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([5, 19, 42, 48], crs=ccrs.PlateCarree())\n",
    "\n",
    "ax.add_feature(cfeature.LAND, facecolor='#f0f0f0')\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "# Use a scatter plot to show the exact location of each of the 42 core cells\n",
    "ax.scatter(x=core_cells['lon'], y=core_cells['lat'],\n",
    "           transform=ccrs.PlateCarree(),\n",
    "           color='red', s=50,  # s is size\n",
    "           label=f'{len(core_cells)} Core Cells')\n",
    "\n",
    "plt.title('Verification: The 42 Core Grid Cells in Northern Italy')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 27-italy-dataset.ipynb/27-italy-dataset.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65258f3-67b4-4f58-a2bd-ef185805eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Point to your original file (read-only)\n",
    "CSV = Path(r\"../italy_core_data/n_italy_core_42_cells.csv\")\n",
    "\n",
    "# 2) Load and standardize column names in memory\n",
    "df0 = pd.read_csv(CSV)\n",
    "df = df0.copy()\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "rename = {}\n",
    "if \"latitude\" in df.columns and \"lat\" not in df.columns: rename[\"latitude\"] = \"lat\"\n",
    "if \"longitude\" in df.columns and \"lon\" not in df.columns: rename[\"longitude\"] = \"lon\"\n",
    "if \"long\" in df.columns and \"lon\" not in df.columns: rename[\"long\"] = \"lon\"\n",
    "df = df.rename(columns=rename)\n",
    "\n",
    "# Keep only lat/lon and ensure numeric\n",
    "assert {\"lat\",\"lon\"} <= set(df.columns), f\"Expected columns lat, lon. Got: {list(df.columns)}\"\n",
    "df = df[[\"lat\",\"lon\"]].copy()\n",
    "df[\"lat\"] = pd.to_numeric(df[\"lat\"], errors=\"coerce\")\n",
    "df[\"lon\"] = pd.to_numeric(df[\"lon\"], errors=\"coerce\")\n",
    "\n",
    "# 3) Validations (handles 0.25-offset 0.5 grids)\n",
    "\n",
    "def infer_offset_deg(series, step=0.5):\n",
    "    \"\"\"Infer whether coords are aligned to n*step (+ 0) or n*step (+ step/2).\"\"\"\n",
    "    frac = (series / step) % 1.0  # in [0,1)\n",
    "    m = np.median(frac)\n",
    "    # decide between 0.00 and 0.50\n",
    "    return 0.0 if abs(m - 0.0) < 0.25 else step/2  # ~0.25 deg\n",
    "\n",
    "def off_grid_with_offset(series, offset_deg, step=0.5, tol=5e-4):\n",
    "    resid = ((series - offset_deg) / step) - np.round((series - offset_deg) / step)\n",
    "    return (np.abs(resid) > tol).sum()\n",
    "\n",
    "lat_off = infer_offset_deg(df[\"lat\"])\n",
    "lon_off = infer_offset_deg(df[\"lon\"])\n",
    "\n",
    "n_raw = len(df)\n",
    "n_nan = int(df.isna().sum().sum())\n",
    "n_unique = len(df.drop_duplicates([\"lat\",\"lon\"]))\n",
    "\n",
    "off_lat = off_grid_with_offset(df[\"lat\"], lat_off)\n",
    "off_lon = off_grid_with_offset(df[\"lon\"], lon_off)\n",
    "\n",
    "latmin, latmax = df[\"lat\"].min(), df[\"lat\"].max()\n",
    "lonmin, lonmax = df[\"lon\"].min(), df[\"lon\"].max()\n",
    "\n",
    "print(f\"Rows: {n_raw}\")\n",
    "print(f\"Unique (lat,lon) pairs: {n_unique}\")\n",
    "print(f\"Missing values total: {n_nan}\")\n",
    "print(f\"Inferred grid offsets: lat {lat_off:.2f}, lon {lon_off:.2f} (step=0.5)\")\n",
    "print(f\"Off-grid counts: lat {off_lat}, lon {off_lon}\")\n",
    "print(f\"BBox: lat [{latmin}, {latmax}], lon [{lonmin}, {lonmax}]\")\n",
    "\n",
    "# Hard checks\n",
    "assert n_nan == 0, \"Found missing lat/lon values.\"\n",
    "assert n_unique == 42, f\"Expected 42 unique cells; found {n_unique}.\"\n",
    "assert off_lat == 0 and off_lon == 0, \"Some coordinates are not aligned to the inferred 0.5 grid.\"\n",
    "\n",
    "# 4) Save canonical copies (original CSV untouched)\n",
    "OUTDIR = CSV.parent / \"derived\"\n",
    "OUTDIR.mkdir(exist_ok=True)\n",
    "\n",
    "df_unique = (\n",
    "    df.drop_duplicates([\"lat\",\"lon\"])\n",
    "      .sort_values([\"lat\",\"lon\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# (a) Verbatim copy: exactly as in the source\n",
    "# df_unique.to_parquet(OUTDIR / \"mask_core_42.parquet\", index=False)\n",
    "df_unique.to_csv(OUTDIR / \"mask_core_42.csv\", index=False)\n",
    "\n",
    "# (b) Normalized copy: snapped to the inferred 0.5 grid (useful for exact joins later)\n",
    "def snap_to_grid(series, offset_deg, step=0.5):\n",
    "    return offset_deg + step * np.round((series - offset_deg) / step)\n",
    "\n",
    "df_norm = df_unique.copy()\n",
    "df_norm[\"lat\"] = snap_to_grid(df_norm[\"lat\"], lat_off)\n",
    "df_norm[\"lon\"] = snap_to_grid(df_norm[\"lon\"], lon_off)\n",
    "df_norm.to_csv(OUTDIR / \"mask_core_42_normalized.csv\", index=False)\n",
    "\n",
    "print(f\"Saved canonical copies to: {OUTDIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f1eb9-3031-48fd-aa9d-a7474f6cda6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# --- Update these two paths ---\n",
    "GDHY_DIR = Path(r\"..\\data\\maize\")  # folder with yield_YYYY.nc4\n",
    "CORE_DIR = Path(r\"..\\italy_core_data\")  # folder with your CSV\n",
    "# ------------------------------\n",
    "\n",
    "# choose a representative file (use 1982 since 1981 is excluded downstream)\n",
    "gdhyds = xr.open_dataset(GDHY_DIR / \"yield_1982.nc4\")\n",
    "\n",
    "# extract grid coordinates (names should be 'lat'/'lon' in GDHY)\n",
    "lat = gdhyds[\"lat\"].values\n",
    "lon = gdhyds[\"lon\"].values\n",
    "\n",
    "# Quick grid report\n",
    "print(f\"GDHY grid: lat n={lat.size}, lon n={lon.size}\")\n",
    "print(f\"lat range [{lat.min()}, {lat.max()}]\")\n",
    "print(f\"lon range [{lon.min()}, {lon.max()}]\")\n",
    "\n",
    "# detect 0.5 step & 0.25 (or 0.00) offset\n",
    "def infer_step_and_offset(arr, guess_step=0.5):\n",
    "    step = float(np.round(np.median(np.diff(arr)), 3))\n",
    "    # infer offset relative to multiples of step\n",
    "    frac = (arr / guess_step) % 1.0\n",
    "    m = float(np.median(frac))\n",
    "    offset = 0.0 if abs(m - 0.0) < 0.25 else guess_step/2\n",
    "    return step, offset\n",
    "\n",
    "lat_step, lat_off = infer_step_and_offset(lat)\n",
    "lon_step, lon_off = infer_step_and_offset(lon)\n",
    "\n",
    "print(f\"Inferred: step lat={lat_step}, lon={lon_step}; offsets lat={lat_off}, lon={lon_off}\")\n",
    "\n",
    "# Save a tiny \"target grid\" artifact well reuse later (coords only)\n",
    "DERIVED = CORE_DIR / \"derived\"\n",
    "DERIVED.mkdir(exist_ok=True)\n",
    "\n",
    "grid_meta = {\n",
    "    \"source\": \"GDHY maize\",\n",
    "    \"file_used\": str((GDHY_DIR / \"yield_1982.nc4\").resolve()),\n",
    "    \"lat_len\": int(lat.size),\n",
    "    \"lon_len\": int(lon.size),\n",
    "    \"lat_min\": float(lat.min()),\n",
    "    \"lat_max\": float(lat.max()),\n",
    "    \"lon_min\": float(lon.min()),\n",
    "    \"lon_max\": float(lon.max()),\n",
    "    \"step_lat\": lat_step,\n",
    "    \"step_lon\": lon_step,\n",
    "    \"offset_lat\": lat_off,\n",
    "    \"offset_lon\": lon_off,\n",
    "}\n",
    "with open(DERIVED / \"grid_gdhy_0p5.json\", \"w\") as f:\n",
    "    json.dump(grid_meta, f, indent=2)\n",
    "\n",
    "# also save a NetCDF with just the 1D coords (handy for regridders)\n",
    "xr.Dataset(coords={\"lat\": ([\"lat\"], lat), \"lon\": ([\"lon\"], lon)}).to_netcdf(DERIVED / \"grid_gdhy_0p5.nc\")\n",
    "\n",
    "print(f\"Saved grid meta & coords to: {DERIVED}\")\n",
    "\n",
    "# ---- Verify the 42 cells sit on this grid and get their indices ----\n",
    "mask42 = pd.read_csv(DERIVED / \"mask_core_42_normalized.csv\")  # from Step 0\n",
    "# helper to convert lat/lon to integer indices using the inferred step/offset\n",
    "def coord_to_index(val, arr, offset, step):\n",
    "    # compute expected index analytically, then clip to bounds\n",
    "    idx = int(round((val - offset) / step))\n",
    "    # safety: ensure it maps to the actual array position\n",
    "    idx = int(np.clip(idx, 0, len(arr)-1))\n",
    "    # final sanity: if exact match fails due to tiny float drift, snap to nearest\n",
    "    if not np.isclose(arr[idx], val, atol=5e-4):\n",
    "        idx = int(np.argmin(np.abs(arr - val)))\n",
    "    return idx\n",
    "\n",
    "mask42[\"ilat\"] = mask42[\"lat\"].apply(lambda v: coord_to_index(v, lat, lat_off, lat_step))\n",
    "mask42[\"ilon\"] = mask42[\"lon\"].apply(lambda v: coord_to_index(v, lon, lon_off, lon_step))\n",
    "\n",
    "# check round-trip accuracy\n",
    "lat_miss = (~np.isclose(lat[mask42[\"ilat\"]], mask42[\"lat\"], atol=5e-4)).sum()\n",
    "lon_miss = (~np.isclose(lon[mask42[\"ilon\"]], mask42[\"lon\"], atol=5e-4)).sum()\n",
    "\n",
    "print(f\"Index mapping checks  lat mismatches: {lat_miss}, lon mismatches: {lon_miss}\")\n",
    "assert lat_miss == 0 and lon_miss == 0, \"Some 42 cells do not map cleanly onto the GDHY grid.\"\n",
    "\n",
    "# save the index mapping for later quick subsetting (new file; originals untouched)\n",
    "mask42[[\"lat\",\"lon\",\"ilat\",\"ilon\"]].to_csv(DERIVED / \"mask_core_42_on_gdhy.csv\", index=False)\n",
    "print(f\"Wrote 42-cell index mapping: {DERIVED/'mask_core_42_on_gdhy.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedec5c9-02a3-476f-ae10-3a5faeeff929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Update these two paths (same as you used in Step 1) ---\n",
    "GDHY_DIR = Path(r\"..\\data\\maize\")   # folder with yield_YYYY.nc4\n",
    "CORE_DIR = Path(r\"..\\italy_core_data\")\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "DERIVED = CORE_DIR / \"derived\"\n",
    "DERIVED.mkdir(exist_ok=True)\n",
    "\n",
    "# 1) Load the 42-cell mapping (created in Step 1). Well also sort for deterministic order.\n",
    "m42 = (\n",
    "    pd.read_csv(DERIVED / \"mask_core_42_on_gdhy.csv\")\n",
    "      .sort_values([\"lat\",\"lon\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "ilat = m42[\"ilat\"].to_numpy()\n",
    "ilon = m42[\"ilon\"].to_numpy()\n",
    "latc = m42[\"lat\"].to_numpy()\n",
    "lonc = m42[\"lon\"].to_numpy()\n",
    "\n",
    "# 2) Helper to open one year's GDHY file and return a 1x42 vector\n",
    "def load_yield_vector(year: int) -> np.ndarray:\n",
    "    f = GDHY_DIR / f\"yield_{year}.nc4\"\n",
    "    ds = xr.open_dataset(f)\n",
    "    # find the yield variable (often 'var')\n",
    "    varname = [v for v in ds.data_vars][0]\n",
    "    da = ds[varname].squeeze()  # -> (lat, lon) or (lat, lon) after dropping singleton time\n",
    "    # ensure we index in (lat, lon) order\n",
    "    if set((\"lat\",\"lon\")).issubset(da.dims):\n",
    "        da = da.transpose(\"lat\",\"lon\")\n",
    "    else:\n",
    "        raise ValueError(f\"{f} does not have expected lat/lon dims; got {da.dims}\")\n",
    "    arr = da.values  # shape [nlat, nlon]\n",
    "    # advanced indexing: pairs of (ilat[k], ilon[k]) -> (42,)\n",
    "    vals = arr[ilat, ilon]\n",
    "    return vals.astype(\"float32\")\n",
    "\n",
    "# 3) Loop years 19822016 (exclude 1981)\n",
    "years = list(range(1982, 2017))\n",
    "rows = []\n",
    "for y in years:\n",
    "    rows.append(load_yield_vector(y))\n",
    "data = np.vstack(rows)  # shape (35, 42)\n",
    "\n",
    "# 4) Build xarray object with coords and save\n",
    "yield_da = xr.DataArray(\n",
    "    data,\n",
    "    dims=(\"year\", \"cell\"),\n",
    "    coords={\n",
    "        \"year\": years,\n",
    "        \"cell\": np.arange(len(lonc)),\n",
    "        \"lat\": (\"cell\", latc),\n",
    "        \"lon\": (\"cell\", lonc),\n",
    "    },\n",
    "    name=\"yield_maize\",  # tonnes per hectare (per GDHY)\n",
    ")\n",
    "yield_ds = yield_da.to_dataset()\n",
    "\n",
    "# Write compact NetCDF (compressed)\n",
    "encoding = {\"yield_maize\": {\"zlib\": True, \"complevel\": 4}}\n",
    "out_nc = DERIVED / \"yield_maize_core42_1982_2016.nc\"\n",
    "yield_ds.to_netcdf(out_nc, encoding=encoding)\n",
    "\n",
    "# Also write a tidy CSV (rows = 42*35) for quick inspection\n",
    "out_csv = DERIVED / \"yield_maize_core42_1982_2016.csv\"\n",
    "\n",
    "# Build a tidy/long table; xarray includes 'lat' and 'lon' already\n",
    "df_long = yield_da.to_dataframe(name=\"yield_maize\").reset_index()\n",
    "\n",
    "# Keep columns and sort\n",
    "df_long = df_long[[\"lat\", \"lon\", \"year\", \"yield_maize\"]].sort_values([\"lat\", \"lon\", \"year\"])\n",
    "\n",
    "df_long.to_csv(out_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7dc75-453d-4268-8f9a-8825b4913c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- paths (edit ERA5_DIR to your location) ----\n",
    "ERA5_DIR = Path(r\"..\\data\\climate_monthly_full\")        # <-- folder with your .grib files\n",
    "CORE_DIR = Path(r\"..\\italy_core_data\")          # same as before\n",
    "DERIVED  = CORE_DIR / \"derived\"\n",
    "DERIVED.mkdir(exist_ok=True)\n",
    "\n",
    "# ---- load artifacts from prior steps ----\n",
    "m42 = pd.read_csv(DERIVED / \"mask_core_42_on_gdhy.csv\").sort_values([\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "ilat, ilon = m42[\"ilat\"].to_numpy(), m42[\"ilon\"].to_numpy()\n",
    "latc, lonc = m42[\"lat\"].to_numpy(), m42[\"lon\"].to_numpy()\n",
    "\n",
    "grid_ds = xr.open_dataset(DERIVED / \"grid_gdhy_0p5.nc\")  # has 1D lat/lon for GDHY\n",
    "lat_g, lon_g = grid_ds[\"lat\"].values, grid_ds[\"lon\"].values\n",
    "\n",
    "# ---- helpers ----\n",
    "def find_year_file(year: int, folder: Path) -> Path:\n",
    "    cand = folder / f\"era5_land_{year}.grib\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "    # fallback: any file that contains the year in the name\n",
    "    matches = list(folder.glob(f\"*{year}*.grib\"))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No GRIB for year {year} in {folder}\")\n",
    "    return matches[0]\n",
    "\n",
    "def open_era5_t2m_year(year: int) -> xr.DataArray:\n",
    "    f = find_year_file(year, ERA5_DIR)\n",
    "    ds = xr.open_dataset(\n",
    "        f,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs=dict(indexpath=\"\", filter_by_keys={\"shortName\": \"2t\"}),\n",
    "        decode_timedelta=True,  # <- silences the FutureWarning\n",
    "    )\n",
    "    ds = ds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "    if ds.lat[0] > ds.lat[-1]:\n",
    "        ds = ds.sortby(\"lat\")\n",
    "    if ds.lon.min() < 0:\n",
    "        ds = ds.assign_coords(lon=((ds.lon % 360 + 360) % 360)).sortby(\"lon\")\n",
    "    da = ds[\"t2m\"] - 273.15  # K -> C\n",
    "    return da\n",
    "\n",
    "def bin_to_half_degree(da: xr.DataArray, step=0.5, offset=0.25) -> xr.DataArray:\n",
    "    # Compute target bin centers as plain numpy arrays (not DataArrays)\n",
    "    lat_bins = (offset + step * np.round((da[\"lat\"].values - offset) / step)).astype(np.float64)\n",
    "    lon_bins = (offset + step * np.round((da[\"lon\"].values - offset) / step)).astype(np.float64)\n",
    "\n",
    "    # Attach as coordinates aligned to the respective dimensions\n",
    "    da = da.assign_coords(lat_bin=(\"lat\", lat_bins), lon_bin=(\"lon\", lon_bins))\n",
    "\n",
    "    # Block-average by those bins\n",
    "    da_c = da.groupby(\"lat_bin\").mean(\"lat\").groupby(\"lon_bin\").mean(\"lon\")\n",
    "    da_c = da_c.rename({\"lat_bin\": \"lat\", \"lon_bin\": \"lon\"})\n",
    "    return da_c\n",
    "\n",
    "def seasonal_mean_mjjas(da_monthly_05: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"Select MaySep and mean over months (per year).\"\"\"\n",
    "    sel = da_monthly_05.where(da_monthly_05[\"time\"].dt.month.isin([5,6,7,8,9]), drop=True)\n",
    "    out = sel.groupby(\"time.year\").mean(\"time\")\n",
    "    return out.rename({\"year\": \"year\"})\n",
    "\n",
    "# ---- process per year to keep memory low ----\n",
    "years = list(range(1982, 2016 + 1))  # exclude 1981\n",
    "pieces = []\n",
    "for y in years:\n",
    "    # 1) open monthly t2m for the year\n",
    "    da_m = open_era5_t2m_year(y)  # dims: time, lat, lon (0.1)\n",
    "\n",
    "    # 2) regrid monthly to 0.5 block means\n",
    "    da_m05 = bin_to_half_degree(da_m)  # dims: time, lat(0.5), lon(0.5)\n",
    "\n",
    "    # 3) seasonal (MaySep) mean for that year\n",
    "    da_y = seasonal_mean_mjjas(da_m05)  # dims: year(=1), lat, lon\n",
    "\n",
    "    # 4) align lat/lon exactly to GDHY grid (nearest) so indices match\n",
    "    da_y = da_y.sel(lat=lat_g, lon=lon_g, method=\"nearest\")\n",
    "\n",
    "    pieces.append(da_y)\n",
    "\n",
    "# ---- stack into one (year, lat, lon) cube on the GDHY grid ----\n",
    "t2m_mjjas = xr.concat(pieces, dim=\"year\")\n",
    "t2m_mjjas = t2m_mjjas.assign_coords(year=(\"year\", years))\n",
    "t2m_mjjas.name = \"t2m_MJJAS_C\"\n",
    "\n",
    "# ---- subset to the 42 cells by GDHY indices (vectorized) ----\n",
    "t2m_42 = t2m_mjjas.isel(\n",
    "    lat=xr.DataArray(ilat, dims=\"cell\"),\n",
    "    lon=xr.DataArray(ilon, dims=\"cell\"),\n",
    ")\n",
    "# add lat/lon as coords on 'cell'\n",
    "t2m_42 = t2m_42.assign_coords(cell=(\"cell\", np.arange(len(ilat))))\n",
    "t2m_42 = t2m_42.assign_coords(lat=(\"cell\", latc), lon=(\"cell\", lonc))\n",
    "\n",
    "# ---- save outputs ----\n",
    "out_nc = DERIVED / \"t2m_MJJAS_core42_1982_2016.nc\"\n",
    "enc = {\"t2m_MJJAS_C\": {\"zlib\": True, \"complevel\": 4}}\n",
    "t2m_42.to_dataset(name=\"t2m_MJJAS_C\").to_netcdf(out_nc, encoding=enc)\n",
    "\n",
    "out_csv = DERIVED / \"t2m_MJJAS_core42_1982_2016.csv\"\n",
    "df_t = t2m_42.to_dataframe(name=\"temperature\").reset_index()\n",
    "df_t = df_t[[\"lat\",\"lon\",\"year\",\"temperature\"]].sort_values([\"lat\",\"lon\",\"year\"])\n",
    "df_t.to_csv(out_csv, index=False)\n",
    "\n",
    "# ---- QA ----\n",
    "print(f\"Saved: {out_nc.name}, {out_csv.name}    {DERIVED}\")\n",
    "print(f\"Shape: years={t2m_42.sizes['year']} (expect 35), cells={t2m_42.sizes['cell']} (expect 42)\")\n",
    "print(f\"NANs: {int(np.isnan(t2m_42.values).sum())}\")\n",
    "print(f\"Year range: {t2m_42.year.values.min()}{t2m_42.year.values.max()}\")\n",
    "print(f\"Lat range: {t2m_42.lat.values.min()}{t2m_42.lat.values.max()} | Lon range: {t2m_42.lon.values.min()}{t2m_42.lon.values.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e238e69-388a-4c3f-878d-ae062757aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- paths ---\n",
    "DERIVED = Path(r\"..\\italy_core_data\\derived\")\n",
    "yield_csv = DERIVED / \"yield_maize_core42_1982_2016.csv\"\n",
    "temp_csv  = DERIVED / \"t2m_MJJAS_core42_1982_2016.csv\"\n",
    "out_csv   = DERIVED / \"maize_ITnorth_core42_1982_2016.csv\"\n",
    "\n",
    "# --- load ---\n",
    "y = pd.read_csv(yield_csv)   # cols: lat, lon, year, yield_maize\n",
    "t = pd.read_csv(temp_csv)    # cols: lat, lon, year, temperature\n",
    "\n",
    "# --- normalize dtypes ---\n",
    "for df, name in [(y,\"yield\"), (t,\"temp\")]:\n",
    "    # ensure keys exist\n",
    "    assert {\"lat\",\"lon\",\"year\"}.issubset(df.columns), f\"{name}: missing join keys\"\n",
    "    # consistent dtypes\n",
    "    df[\"year\"] = df[\"year\"].astype(int)\n",
    "    df[\"lat\"]  = df[\"lat\"].astype(float)\n",
    "    df[\"lon\"]  = df[\"lon\"].astype(float)\n",
    "\n",
    "# --- quick sanity on each input ---\n",
    "def quick_report(df, label):\n",
    "    n = len(df)\n",
    "    nunique = df[[\"lat\",\"lon\",\"year\"]].drop_duplicates().shape[0]\n",
    "    n_nans = int(df.isna().sum().sum())\n",
    "    print(f\"{label}: rows={n}, unique_keys={nunique}, NaNs={n_nans}\")\n",
    "    assert n == 42*35, f\"{label}: expected 1470 rows, got {n}\"\n",
    "    assert nunique == n, f\"{label}: duplicate (lat,lon,year) rows found\"\n",
    "    assert n_nans == 0, f\"{label}: contains missing values\"\n",
    "\n",
    "quick_report(y, \"Yield\")\n",
    "quick_report(t, \"Temperature\")\n",
    "\n",
    "# --- merge on exact keys ---\n",
    "df = y.merge(t, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "\n",
    "# --- final checks ---\n",
    "print(f\"Merged rows: {len(df)} (expected 1470)\")\n",
    "assert len(df) == 42*35, \"Merge did not produce 1470 rowscheck key alignment\"\n",
    "assert int(df.isna().sum().sum()) == 0, \"Merged table has missing values\"\n",
    "\n",
    "# order & save\n",
    "df = df[[\"lat\",\"lon\",\"year\",\"yield_maize\",\"temperature\"]].sort_values([\"lat\",\"lon\",\"year\"]).reset_index(drop=True)\n",
    "df.to_csv(out_csv, index=False)\n",
    "\n",
    "print(f\"Saved combined dataset  {out_csv}\")\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(df.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525042fc-9313-4c2b-b7ab-1c641600761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------ paths (edit if needed) ------------\n",
    "ERA5_DIR = Path(r\"..\\data\\climate_monthly_full\")   # contains era5_land_monthly_YYYY.grib\n",
    "CORE_DIR = Path(r\"..\\italy_core_data\")\n",
    "DERIVED  = CORE_DIR / \"derived\"\n",
    "DERIVED.mkdir(exist_ok=True)\n",
    "\n",
    "base_csv = DERIVED / \"maize_ITnorth_core42_1982_2016.csv\"  # yield + temperature (from Step 4)\n",
    "\n",
    "# ------------ prior artifacts ------------\n",
    "m42 = pd.read_csv(DERIVED / \"mask_core_42_on_gdhy.csv\").sort_values([\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "latc, lonc = m42[\"lat\"].to_numpy(), m42[\"lon\"].to_numpy()\n",
    "\n",
    "# Small safety margin so binning catches the edge cells\n",
    "BBOX = dict(\n",
    "    lat_min=float(latc.min() - 0.5),\n",
    "    lat_max=float(latc.max() + 0.5),\n",
    "    lon_min=float(lonc.min() - 0.5),\n",
    "    lon_max=float(lonc.max() + 0.5),\n",
    ")\n",
    "\n",
    "years = list(range(1982, 2016 + 1))\n",
    "season_months = [5,6,7,8,9]  # MJJAS\n",
    "\n",
    "# ------------ helpers ------------\n",
    "def find_year_file(year: int) -> Path:\n",
    "    cand = ERA5_DIR / f\"era5_land_monthly_{year}.grib\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "    matches = list(ERA5_DIR.glob(f\"*{year}*.grib\"))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No GRIB for year {year} in {ERA5_DIR}\")\n",
    "    return matches[0]\n",
    "\n",
    "def open_era5_var_year(year: int, short: str, varname: str, bbox: dict) -> xr.DataArray:\n",
    "    \"\"\"Open one ERA5-Land variable for a year, standardize coords, crop to Italy bbox, return DA.\"\"\"\n",
    "    f = find_year_file(year)\n",
    "    ds = xr.open_dataset(\n",
    "        f,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs=dict(indexpath=\"\", filter_by_keys={\"shortName\": short}),\n",
    "        decode_timedelta=True,\n",
    "    ).rename({\"latitude\":\"lat\",\"longitude\":\"lon\"})\n",
    "\n",
    "    # Ensure ascending latitude and 0..360 longitudes\n",
    "    if ds.lat[0] > ds.lat[-1]:\n",
    "        ds = ds.sortby(\"lat\")\n",
    "    if ds.lon.min() < 0:\n",
    "        ds = ds.assign_coords(lon=((ds.lon % 360 + 360) % 360)).sortby(\"lon\")\n",
    "\n",
    "    # Crop to Italy bbox BEFORE regridding (massive speed-up)\n",
    "    ds = ds.sel(lat=slice(bbox[\"lat_min\"], bbox[\"lat_max\"]),\n",
    "                lon=slice(bbox[\"lon_min\"], bbox[\"lon_max\"]))\n",
    "\n",
    "    da = ds[varname]\n",
    "    if \"expver\" in da.dims:\n",
    "        da = da.isel(expver=-1)  # pick analysis member if present\n",
    "\n",
    "    return da  # dims: time, lat, lon (regional)\n",
    "\n",
    "def bin_to_half_degree(da: xr.DataArray, step=0.5, offset=0.25) -> xr.DataArray:\n",
    "    \"\"\"Block-average 0.1 to 0.5 by labeling to nearest 0.25 + 0.5*k centers, then mean.\"\"\"\n",
    "    lat_bins = (offset + step * np.round((da[\"lat\"].values - offset) / step)).astype(np.float64)\n",
    "    lon_bins = (offset + step * np.round((da[\"lon\"].values - offset) / step)).astype(np.float64)\n",
    "    da = da.assign_coords(lat_bin=(\"lat\", lat_bins), lon_bin=(\"lon\", lon_bins))\n",
    "    da_c = da.groupby(\"lat_bin\").mean(\"lat\").groupby(\"lon_bin\").mean(\"lon\")\n",
    "    return da_c.rename({\"lat_bin\":\"lat\",\"lon_bin\":\"lon\"})  # dims: time, lat(0.5), lon(0.5)\n",
    "\n",
    "def seasonal_mean(da_m05: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"Mean over MJJAS months (for state variables).\"\"\"\n",
    "    sel = da_m05.where(da_m05[\"time\"].dt.month.isin(season_months), drop=True)\n",
    "    return sel.groupby(\"time.year\").mean(\"time\").rename({\"year\":\"year\"})\n",
    "\n",
    "def seasonal_total_from_daily_means(da_m05: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    ERA5 Monthly Averaged fluxes are DAILY means.\n",
    "    Convert to monthly totals by multiplying by days_in_month, then sum MJJAS.\n",
    "    \"\"\"\n",
    "    sel = da_m05.where(da_m05[\"time\"].dt.month.isin(season_months), drop=True)\n",
    "    days = xr.DataArray(\n",
    "        sel[\"time\"].dt.days_in_month,\n",
    "        coords={\"time\": sel[\"time\"]},\n",
    "        dims=[\"time\"]\n",
    "    ).astype(np.float64)\n",
    "    return (sel * days).groupby(\"time.year\").sum(\"time\").rename({\"year\":\"year\"})\n",
    "\n",
    "def process_var(short: str, xr_name: str, out_col: str, how: str, unit_conv=None, post_conv=None):\n",
    "    \"\"\"\n",
    "    short/xr_name : GRIB key & xarray variable name (often the same)\n",
    "    out_col       : output column name\n",
    "    how           : 'mean' (state) or 'sum' (flux) over season\n",
    "    unit_conv     : function applied to the **monthly** field (e.g., K->C, m->mm, sign flips)\n",
    "    post_conv     : function applied after seasonal reduce (e.g., convert J -> MJ)\n",
    "    \"\"\"\n",
    "    pieces = []\n",
    "    for y in years:\n",
    "        da = open_era5_var_year(y, short, xr_name, BBOX)   # regional 0.1\n",
    "        if unit_conv is not None:\n",
    "            da = unit_conv(da)                             # per-month conversion\n",
    "        da05 = bin_to_half_degree(da)                      # regional 0.5\n",
    "\n",
    "        if how == \"mean\":\n",
    "            ya = seasonal_mean(da05)                       # (year, lat, lon)\n",
    "        elif how == \"sum\":\n",
    "            ya = seasonal_total_from_daily_means(da05)    # (year, lat, lon)\n",
    "        else:\n",
    "            raise ValueError(\"how must be 'mean' or 'sum'\")\n",
    "\n",
    "        # Select the exact same 42 cells by value (no global expansion)\n",
    "        ya42 = ya.sel(lat=xr.DataArray(latc, dims=\"cell\"),\n",
    "                      lon=xr.DataArray(lonc, dims=\"cell\"),\n",
    "                      method=\"nearest\")\n",
    "        ya42 = ya42.assign_coords(cell=(\"cell\", np.arange(len(latc))),\n",
    "                                  lat=(\"cell\", latc), lon=(\"cell\", lonc))\n",
    "        pieces.append(ya42)\n",
    "\n",
    "        if (y - years[0]) % 5 == 0:\n",
    "            print(f\"{out_col}: processed {y}\", flush=True)\n",
    "\n",
    "    da_all = xr.concat(pieces, dim=\"year\").assign_coords(year=(\"year\", years))\n",
    "\n",
    "    if post_conv is not None:\n",
    "        da_all = post_conv(da_all)\n",
    "\n",
    "    out_nc  = DERIVED / f\"{out_col}_MJJAS_core42_1982_2016.nc\"\n",
    "    out_csv = DERIVED / f\"{out_col}_MJJAS_core42_1982_2016.csv\"\n",
    "    da_all.to_dataset(name=xr_name).to_netcdf(out_nc, encoding={xr_name: {\"zlib\": True, \"complevel\": 4}})\n",
    "    df = da_all.to_dataframe(name=out_col).reset_index()[[\"lat\",\"lon\",\"year\",out_col]]\n",
    "    df = df.sort_values([\"lat\",\"lon\",\"year\"])\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved {out_col}: {out_csv.name}  (rows={len(df)})\")\n",
    "    return df\n",
    "\n",
    "# ------------ recompute all stressors with correct handling ------------\n",
    "# precipitation: m -> mm (per month), then hours-weighted seasonal total\n",
    "df_tp   = process_var(short=\"tp\",    xr_name=\"tp\",    out_col=\"precipitation\",\n",
    "                      how=\"sum\",\n",
    "                      unit_conv=lambda da: da * 1000.0,   # mm per hour-mean\n",
    "                      post_conv=None)\n",
    "\n",
    "# soil water layer 1: mean MJJAS (m3/m3), no hours weighting\n",
    "df_swvl = process_var(short=\"swvl1\", xr_name=\"swvl1\", out_col=\"soil_water\",\n",
    "                      how=\"mean\",\n",
    "                      unit_conv=None,\n",
    "                      post_conv=None)\n",
    "\n",
    "# solar radiation: J/m per hour-mean -> hours-weighted seasonal total\n",
    "# (optional: convert to MJ/m for readability with post_conv=lambda da: da/1e6)\n",
    "df_ssr  = process_var(short=\"ssr\",   xr_name=\"ssr\",   out_col=\"solar_radiation\",\n",
    "                      how=\"sum\",\n",
    "                      unit_conv=None,\n",
    "                      post_conv=None)  # or post_conv=lambda da: da/1e6\n",
    "\n",
    "# potential evaporation: m (negative) -> **positive mm**, then hours-weighted seasonal total\n",
    "df_pev  = process_var(short=\"pev\",   xr_name=\"pev\",   out_col=\"potential_evaporation\",\n",
    "                      how=\"sum\",\n",
    "                      unit_conv=lambda da: -da * 1000.0,  # flip sign & mm per hour-mean\n",
    "                      post_conv=None)\n",
    "\n",
    "# ------------ merge with existing yield+temperature ------------\n",
    "base = pd.read_csv(base_csv)  # lat, lon, year, yield_maize, temperature\n",
    "df = (\n",
    "    base.merge(df_tp,   on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_swvl, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_ssr,  on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_pev,  on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .sort_values([\"lat\",\"lon\",\"year\"])\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Final checks and save\n",
    "assert len(df) == 42*35, f\"Expected 1470 rows, got {len(df)}\"\n",
    "assert int(df.isna().sum().sum()) == 0, \"Merged table has NaNs\"\n",
    "\n",
    "final_csv = DERIVED / \"maize_ITnorth_core42_1982_2016_allstressors.csv\"\n",
    "df.to_csv(final_csv, index=False)\n",
    "\n",
    "print(\"\\nFinal table saved \", final_csv)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(df.head(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2cafc8-5e00-41a6-9e7b-96dcd1d3244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- paths ----------\n",
    "ERA5_DIR = Path(r\"..\\data\\climate_monthly_full\")   # folder with era5_land_monthly_YYYY.grib\n",
    "CORE_DIR = Path(r\"..\\italy_core_data\")\n",
    "DERIVED  = CORE_DIR / \"derived\"\n",
    "DERIVED.mkdir(exist_ok=True)\n",
    "\n",
    "base_csv = DERIVED / \"maize_ITnorth_core42_1982_2016_allstressors.csv\"  # seasonal file (already created)\n",
    "out_csv  = DERIVED / \"maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv\"\n",
    "\n",
    "# ---------- constants ----------\n",
    "years = list(range(1982, 2016 + 1))\n",
    "months = [5, 6, 7, 8, 9]  # MaySep\n",
    "mon_name = {5: \"May\", 6: \"Jun\", 7: \"Jul\", 8: \"Aug\", 9: \"Sep\"}\n",
    "\n",
    "# ---------- 42-cell mask (values) for exact selection ----------\n",
    "m42 = pd.read_csv(DERIVED / \"mask_core_42_on_gdhy.csv\").sort_values([\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "latc, lonc = m42[\"lat\"].to_numpy(), m42[\"lon\"].to_numpy()\n",
    "\n",
    "# Small bbox so we only process Italy region\n",
    "BBOX = dict(\n",
    "    lat_min=float(latc.min() - 0.5),\n",
    "    lat_max=float(latc.max() + 0.5),\n",
    "    lon_min=float(lonc.min() - 0.5),\n",
    "    lon_max=float(lonc.max() + 0.5),\n",
    ")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_year_file(year: int) -> Path:\n",
    "    cand = ERA5_DIR / f\"era5_land_monthly_{year}.grib\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "    matches = list(ERA5_DIR.glob(f\"*{year}*.grib\"))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No GRIB for year {year} in {ERA5_DIR}\")\n",
    "    return matches[0]\n",
    "\n",
    "def open_era5_var_year(year: int, short: str, varname: str, bbox: dict) -> xr.DataArray:\n",
    "    f = find_year_file(year)\n",
    "    ds = xr.open_dataset(\n",
    "        f,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs=dict(indexpath=\"\", filter_by_keys={\"shortName\": short}),\n",
    "        decode_timedelta=True,\n",
    "    ).rename({\"latitude\":\"lat\",\"longitude\":\"lon\"})\n",
    "    # ensure ascending lat, 0..360 lon, and crop\n",
    "    if ds.lat[0] > ds.lat[-1]:\n",
    "        ds = ds.sortby(\"lat\")\n",
    "    if ds.lon.min() < 0:\n",
    "        ds = ds.assign_coords(lon=((ds.lon % 360 + 360) % 360)).sortby(\"lon\")\n",
    "    ds = ds.sel(lat=slice(bbox[\"lat_min\"], bbox[\"lat_max\"]),\n",
    "                lon=slice(bbox[\"lon_min\"], bbox[\"lon_max\"]))\n",
    "    da = ds[varname]\n",
    "    if \"expver\" in da.dims:\n",
    "        da = da.isel(expver=-1)\n",
    "    return da  # dims: time, lat, lon\n",
    "\n",
    "def bin_to_half_degree(da: xr.DataArray, step=0.5, offset=0.25) -> xr.DataArray:\n",
    "    \"\"\"Block-average 0.1 to 0.5: label to nearest 0.25+0.5*k centers, then mean.\"\"\"\n",
    "    lat_bins = (offset + step * np.round((da[\"lat\"].values - offset) / step)).astype(np.float64)\n",
    "    lon_bins = (offset + step * np.round((da[\"lon\"].values - offset) / step)).astype(np.float64)\n",
    "    da = da.assign_coords(lat_bin=(\"lat\", lat_bins), lon_bin=(\"lon\", lon_bins))\n",
    "    da_c = da.groupby(\"lat_bin\").mean(\"lat\").groupby(\"lon_bin\").mean(\"lon\")\n",
    "    return da_c.rename({\"lat_bin\":\"lat\",\"lon_bin\":\"lon\"})  # dims: time, lat(0.5), lon(0.5)\n",
    "\n",
    "def monthly_means(da_m05: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"Return monthly means for MJJAS (state variables).\"\"\"\n",
    "    return da_m05.where(da_m05[\"time\"].dt.month.isin(months), drop=True)\n",
    "\n",
    "def monthly_totals_from_daily_means(da_m05: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"For ERA5 Monthly Averaged fluxes: daily mean  days_in_month -> monthly totals.\"\"\"\n",
    "    sel = da_m05.where(da_m05[\"time\"].dt.month.isin(months), drop=True)\n",
    "    days = xr.DataArray(sel[\"time\"].dt.days_in_month, coords={\"time\": sel[\"time\"]}, dims=[\"time\"]).astype(np.float64)\n",
    "    return sel * days\n",
    "\n",
    "def make_monthly_wide(short: str, xr_name: str, out_col: str, how: str, unit_conv=None):\n",
    "    \"\"\"\n",
    "    Build a wide dataframe with columns {out_col}_May  {out_col}_Sep.\n",
    "    how = 'mean' for state vars; 'total' for flux vars.\n",
    "    unit_conv is applied to the **monthly** field prior to means/totals (e.g., K->C; m->mm and sign flip).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for y in years:\n",
    "        da = open_era5_var_year(y, short, xr_name, BBOX)\n",
    "        if unit_conv is not None:\n",
    "            da = unit_conv(da)\n",
    "        da05 = bin_to_half_degree(da)\n",
    "\n",
    "        if how == \"mean\":\n",
    "            dam = monthly_means(da05)                 # time (MJJAS), lat, lon\n",
    "        elif how == \"total\":\n",
    "            dam = monthly_totals_from_daily_means(da05)\n",
    "        else:\n",
    "            raise ValueError(\"how must be 'mean' or 'total'\")\n",
    "\n",
    "        # select the 42 cells by value (nearest to GDHY centers)\n",
    "        sub = dam.sel(lat=xr.DataArray(latc, dims=\"cell\"),\n",
    "                      lon=xr.DataArray(lonc, dims=\"cell\"),\n",
    "                      method=\"nearest\")\n",
    "        sub = sub.assign_coords(cell=(\"cell\", np.arange(len(latc))),\n",
    "                                lat=(\"cell\", latc), lon=(\"cell\", lonc))\n",
    "\n",
    "        df = sub.to_dataframe(name=out_col).reset_index()\n",
    "        df[\"year\"] = pd.to_datetime(df[\"time\"]).dt.year\n",
    "        df[\"month\"] = pd.to_datetime(df[\"time\"]).dt.month\n",
    "        df = df[df[\"month\"].isin(months)].copy()\n",
    "        df[\"month_name\"] = df[\"month\"].map(mon_name)\n",
    "        rows.append(df[[\"lat\",\"lon\",\"year\",\"month_name\",out_col]])\n",
    "\n",
    "        if (y - years[0]) % 5 == 0:\n",
    "            print(f\"{out_col}: processed {y}\", flush=True)\n",
    "\n",
    "    df_all = pd.concat(rows, ignore_index=True)\n",
    "    # pivot to wide: columns = month names\n",
    "    wide = df_all.pivot_table(index=[\"lat\",\"lon\",\"year\"], columns=\"month_name\", values=out_col, aggfunc=\"first\").reset_index()\n",
    "    # ensure month column order and rename with prefix\n",
    "    month_cols = [\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\"]\n",
    "    for m in month_cols:\n",
    "        if m not in wide.columns:\n",
    "            wide[m] = np.nan\n",
    "    wide = wide[[\"lat\",\"lon\",\"year\"] + month_cols]\n",
    "    wide = wide.rename(columns={m: f\"{out_col}_{m}\" for m in month_cols})\n",
    "    return wide\n",
    "\n",
    "# ---------- build monthly tables ----------\n",
    "# temperature: K -> C, monthly means\n",
    "df_t2m_mon = make_monthly_wide(short=\"2t\", xr_name=\"t2m\", out_col=\"temperature\",\n",
    "                               how=\"mean\", unit_conv=lambda da: da - 273.15)\n",
    "\n",
    "# soil water: monthly means (m3/m3)\n",
    "df_swvl_mon = make_monthly_wide(short=\"swvl1\", xr_name=\"swvl1\", out_col=\"soil_water\",\n",
    "                                how=\"mean\", unit_conv=None)\n",
    "\n",
    "# precipitation: m -> mm, daily mean  days -> monthly totals\n",
    "df_tp_mon = make_monthly_wide(short=\"tp\", xr_name=\"tp\", out_col=\"precipitation\",\n",
    "                              how=\"total\", unit_conv=lambda da: da * 1000.0)\n",
    "\n",
    "# solar radiation: J/m daily mean  days -> monthly totals (kept in J/m)\n",
    "df_ssr_mon = make_monthly_wide(short=\"ssr\", xr_name=\"ssr\", out_col=\"solar_radiation\",\n",
    "                               how=\"total\", unit_conv=None)\n",
    "\n",
    "# potential evaporation: m -> **mm** and flip sign to positive, daily mean  days -> monthly totals\n",
    "df_pev_mon = make_monthly_wide(short=\"pev\", xr_name=\"pev\", out_col=\"potential_evaporation\",\n",
    "                               how=\"total\", unit_conv=lambda da: -da * 1000.0)\n",
    "\n",
    "# ---------- merge with seasonal base and save ----------\n",
    "base = pd.read_csv(base_csv)  # lat, lon, year, seasonal columns already present\n",
    "\n",
    "enriched = (\n",
    "    base.merge(df_t2m_mon, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_tp_mon,   on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_swvl_mon, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_ssr_mon,  on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_pev_mon,  on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .sort_values([\"lat\",\"lon\",\"year\"])\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# sanity checks\n",
    "assert len(enriched) == 42*35, f\"Expected 1470 rows, got {len(enriched)}\"\n",
    "assert int(enriched.isna().sum().sum()) == 0, \"Found NaNs in enriched dataset\"\n",
    "\n",
    "enriched.to_csv(out_csv, index=False)\n",
    "print(f\"Saved enriched dataset with monthly columns  {out_csv}\")\n",
    "print(\"Columns:\", list(enriched.columns)[:12], \"...\")  # preview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 28-dataset-creation-pipeline.ipynb/28-dataset-creation-pipeline.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7dfde9-8d7a-457e-b995-0201930f645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === USER INPUTS ===\n",
    "\n",
    "# Project/crop tags\n",
    "crop          = \"wheat\"                 # e.g., \"maize\", \"wheat_winter\", \"soybean\", ...\n",
    "region_tag    = \"ITnorth_core42\"        # used in filenames (keep as-is if using same 42-core cells)\n",
    "\n",
    "# dynamic yield column name\n",
    "YIELD_COL = f\"yield_{crop}\"\n",
    "\n",
    "# Years and season\n",
    "start_year    = 1982\n",
    "end_year      = 2016                    # inclusive\n",
    "season_months=[3,4,5,6,7]         # MJJAS for Po Valley maize\n",
    "\n",
    "# Paths (edit to your structure)\n",
    "BASE_DIR      = r\"..\\italy_core_data\"\n",
    "DERIVED_DIR   = r\"..\\italy_core_data\\derived\"    # outputs will be written here\n",
    "MASK_CSV      = r\"..\\italy_core_data\\derived\\mask_core_42_on_gdhy.csv\"\n",
    "\n",
    "# GDHY yield files (per-year .nc4)\n",
    "GDHY_DIR      = r\"..\\data\\wheatg\"       # folder containing yield_YYYY.nc4 for the selected crop\n",
    "GDHY_FILE_TMPL= \"yield_{year}.nc4\"     # change if your files differ\n",
    "GDHY_VARNAME  = None                   # None -> auto-detect first data_var, or set e.g. \"var\"\n",
    "\n",
    "# ERA5-Land monthly averaged grib files (per-year)\n",
    "ERA5_DIR      = r\"..\\data\\climate_monthly_full\"\n",
    "ERA5_FILE_TMPL= \"era5_land_monthly_{year}.grib\"  # change if your files differ\n",
    "\n",
    "# Optional: convert SSR to MJ/m in monthly/seasonal outputs? (keeps J/m if False)\n",
    "SSR_TO_MJ     = False\n",
    "\n",
    "# === end user inputs ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd034c-0004-49c9-a9d9-5926043d0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Resolve paths\n",
    "DERIVED = Path(DERIVED_DIR); DERIVED.mkdir(exist_ok=True)\n",
    "MASK_CSV = Path(MASK_CSV)\n",
    "GDHY_DIR = Path(GDHY_DIR)\n",
    "ERA5_DIR = Path(ERA5_DIR)\n",
    "\n",
    "years = list(range(int(start_year), int(end_year) + 1))\n",
    "month_names = {1:\"Jan\",2:\"Feb\",3:\"Mar\",4:\"Apr\",5:\"May\",6:\"Jun\",7:\"Jul\",8:\"Aug\",9:\"Sep\",10:\"Oct\",11:\"Nov\",12:\"Dec\"}\n",
    "mon_order = [month_names[m] for m in season_months]  # dynamic order for monthly columns]\n",
    "\n",
    "# ---------- mask & bbox ----------\n",
    "m42 = (pd.read_csv(MASK_CSV).sort_values([\"lat\",\"lon\"]).reset_index(drop=True))\n",
    "latc, lonc = m42[\"lat\"].to_numpy(), m42[\"lon\"].to_numpy()\n",
    "ilat = m42[\"ilat\"].to_numpy() if \"ilat\" in m42.columns else None\n",
    "ilon = m42[\"ilon\"].to_numpy() if \"ilon\" in m42.columns else None\n",
    "\n",
    "# Small margin so binning catches edge cells\n",
    "BBOX = dict(\n",
    "    lat_min=float(latc.min() - 0.5),\n",
    "    lat_max=float(latc.max() + 0.5),\n",
    "    lon_min=float(lonc.min() - 0.5),\n",
    "    lon_max=float(lonc.max() + 0.5),\n",
    ")\n",
    "\n",
    "# ---------- file locators ----------\n",
    "def gdhyds_path(y:int)->Path:\n",
    "    p = GDHY_DIR / GDHY_FILE_TMPL.format(year=y)\n",
    "    if not p.exists():\n",
    "        # fallback: any file containing year\n",
    "        cands = list(GDHY_DIR.glob(f\"*{y}*.nc*\"))\n",
    "        if not cands:\n",
    "            raise FileNotFoundError(f\"GDHY file not found for {y} in {GDHY_DIR}\")\n",
    "        return cands[0]\n",
    "    return p\n",
    "\n",
    "def era5_path(y:int)->Path:\n",
    "    p = ERA5_DIR / ERA5_FILE_TMPL.format(year=y)\n",
    "    if not p.exists():\n",
    "        cands = list(ERA5_DIR.glob(f\"*{y}*.grib\"))\n",
    "        if not cands:\n",
    "            raise FileNotFoundError(f\"ERA5 file not found for {y} in {ERA5_DIR}\")\n",
    "        return cands[0]\n",
    "    return p\n",
    "\n",
    "# ---------- ERA5 open + crop ----------\n",
    "def open_era5_var_year(year:int, short:str, xr_name:str)->xr.DataArray:\n",
    "    f = era5_path(year)\n",
    "    ds = xr.open_dataset(\n",
    "        f,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs=dict(indexpath=\"\", filter_by_keys={\"shortName\": short}),\n",
    "        decode_timedelta=True,\n",
    "    ).rename({\"latitude\":\"lat\",\"longitude\":\"lon\"})\n",
    "    # ascending lat\n",
    "    if ds.lat[0] > ds.lat[-1]:\n",
    "        ds = ds.sortby(\"lat\")\n",
    "    # wrap lon to 0..360\n",
    "    if ds.lon.min() < 0:\n",
    "        ds = ds.assign_coords(lon=((ds.lon % 360 + 360) % 360)).sortby(\"lon\")\n",
    "    # crop to bbox\n",
    "    ds = ds.sel(lat=slice(BBOX[\"lat_min\"], BBOX[\"lat_max\"]),\n",
    "                lon=slice(BBOX[\"lon_min\"], BBOX[\"lon_max\"]))\n",
    "    da = ds[xr_name]\n",
    "    if \"expver\" in da.dims:\n",
    "        da = da.isel(expver=-1)\n",
    "    return da  # dims: time, lat, lon\n",
    "\n",
    "# ---------- 0.1 -> 0.5 block-average to GDHY centers (offset 0.25) ----------\n",
    "def bin_to_half_degree(da: xr.DataArray, step=0.5, offset=0.25) -> xr.DataArray:\n",
    "    lat_bins = (offset + step * np.round((da[\"lat\"].values - offset) / step)).astype(np.float64)\n",
    "    lon_bins = (offset + step * np.round((da[\"lon\"].values - offset) / step)).astype(np.float64)\n",
    "    da = da.assign_coords(lat_bin=(\"lat\", lat_bins), lon_bin=(\"lon\", lon_bins))\n",
    "    da_c = da.groupby(\"lat_bin\").mean(\"lat\").groupby(\"lon_bin\").mean(\"lon\")\n",
    "    return da_c.rename({\"lat_bin\":\"lat\",\"lon_bin\":\"lon\"})\n",
    "\n",
    "# ---------- monthly reductions ----------\n",
    "def monthly_means(da_m05: xr.DataArray, months:list)->xr.DataArray:\n",
    "    return da_m05.where(da_m05[\"time\"].dt.month.isin(months), drop=True)\n",
    "\n",
    "def monthly_totals_from_daily_means(da_m05: xr.DataArray, months:list)->xr.DataArray:\n",
    "    sel = da_m05.where(da_m05[\"time\"].dt.month.isin(months), drop=True)\n",
    "    days = xr.DataArray(sel[\"time\"].dt.days_in_month, coords={\"time\": sel[\"time\"]}, dims=[\"time\"]).astype(np.float64)\n",
    "    return sel * days\n",
    "\n",
    "# ---------- seasonal reductions ----------\n",
    "def seasonal_mean(da_m05: xr.DataArray, months:list)->xr.DataArray:\n",
    "    return monthly_means(da_m05, months).groupby(\"time.year\").mean(\"time\").rename({\"year\":\"year\"})\n",
    "\n",
    "def seasonal_total_from_daily_means(da_m05: xr.DataArray, months:list)->xr.DataArray:\n",
    "    return monthly_totals_from_daily_means(da_m05, months).groupby(\"time.year\").sum(\"time\").rename({\"year\":\"year\"})\n",
    "\n",
    "# ---------- select exact 42 cells by lat/lon value (nearest) ----------\n",
    "def select_core_cells(da: xr.DataArray)->xr.DataArray:\n",
    "    out = da.sel(lat=xr.DataArray(latc, dims=\"cell\"),\n",
    "                 lon=xr.DataArray(lonc, dims=\"cell\"),\n",
    "                 method=\"nearest\")\n",
    "    out = out.assign_coords(cell=(\"cell\", np.arange(len(latc))),\n",
    "                            lat=(\"cell\", latc), lon=(\"cell\", lonc))\n",
    "    return out\n",
    "\n",
    "# ---------- build yield panel ----------\n",
    "def build_yield_panel()->pd.DataFrame:\n",
    "    rows = []\n",
    "    for y in years:\n",
    "        f = gdhyds_path(y)\n",
    "        ds = xr.open_dataset(f)\n",
    "        varname = GDHY_VARNAME or [v for v in ds.data_vars][0]\n",
    "        da = ds[varname].squeeze()\n",
    "        if set((\"lat\",\"lon\")).issubset(da.dims):\n",
    "            da = da.transpose(\"lat\",\"lon\")\n",
    "        else:\n",
    "            raise ValueError(f\"{f} missing lat/lon dims; got {da.dims}\")\n",
    "        arr = da.values\n",
    "        if ilat is None or ilon is None:\n",
    "            # Fallback: map coords to indices\n",
    "            lat_vals = da[\"lat\"].values; lon_vals = da[\"lon\"].values\n",
    "            def idx(val, axis_vals): \n",
    "                i = int(np.argmin(np.abs(axis_vals - val)))\n",
    "                assert np.isclose(axis_vals[i], val, atol=5e-4)\n",
    "                return i\n",
    "            i_lat = np.array([idx(v, lat_vals) for v in latc])\n",
    "            i_lon = np.array([idx(v, lon_vals) for v in lonc])\n",
    "        else:\n",
    "            i_lat, i_lon = ilat, ilon\n",
    "        vals = arr[i_lat, i_lon].astype(\"float32\")\n",
    "        rows.append(pd.DataFrame({\"year\": y, \"lat\": latc, \"lon\": lonc, YIELD_COL: vals}))\n",
    "    out = pd.concat(rows, ignore_index=True).sort_values([\"lat\",\"lon\",\"year\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# make sure the yield column is present under the dynamic name\n",
    "if \"yield_maize\" in yield_df.columns and YIELD_COL not in yield_df.columns:\n",
    "    yield_df = yield_df.rename(columns={\"yield_maize\": YIELD_COL})\n",
    "\n",
    "\n",
    "# ---------- process a single ERA5 variable to seasonal + monthly ----------\n",
    "def process_era5_var(short:str, xr_name:str, out_col:str, vtype:str):\n",
    "    \"\"\"\n",
    "    short/xr_name : GRIB shortName and xarray variable name (often same)\n",
    "    out_col       : output column base name\n",
    "    vtype         : 'state' (mean) or 'flux' (total from daily means)\n",
    "    Returns: (seasonal_df, monthly_wide_df)\n",
    "    \"\"\"\n",
    "    # collect monthly for all years\n",
    "    monthly_rows = []\n",
    "    seasonal_rows = []\n",
    "    for y in years:\n",
    "        da = open_era5_var_year(y, short, xr_name)       # regional 0.1\n",
    "        # unit conversions (per variable)\n",
    "        if out_col == \"temperature\":\n",
    "            da = da - 273.15\n",
    "        if out_col == \"precipitation\":\n",
    "            da = da * 1000.0       # m -> mm (daily mean)\n",
    "        if out_col == \"potential_evaporation\":\n",
    "            da = -da * 1000.0      # flip sign, m -> mm (daily mean)\n",
    "        # block-average to 0.5\n",
    "        da05 = bin_to_half_degree(da)\n",
    "        # monthly slice\n",
    "        if vtype == \"state\":\n",
    "            dam = monthly_means(da05, season_months)\n",
    "        elif vtype == \"flux\":\n",
    "            dam = monthly_totals_from_daily_means(da05, season_months)\n",
    "        else:\n",
    "            raise ValueError(\"vtype must be 'state' or 'flux'\")\n",
    "        # select 42 cells\n",
    "        subm = select_core_cells(dam)\n",
    "        dft = subm.to_dataframe(name=out_col).reset_index()\n",
    "        dft[\"year\"] = pd.to_datetime(dft[\"time\"]).dt.year\n",
    "        dft[\"month\"] = pd.to_datetime(dft[\"time\"]).dt.month\n",
    "        dft[\"month_name\"] = dft[\"month\"].map(month_names)\n",
    "        dft = dft[dft[\"month\"].isin(season_months)]\n",
    "        monthly_rows.append(dft[[\"lat\",\"lon\",\"year\",\"month\",\"month_name\",out_col]])\n",
    "        # seasonal\n",
    "        if vtype == \"state\":\n",
    "            ya = seasonal_mean(da05, season_months)\n",
    "        else:\n",
    "            ya = seasonal_total_from_daily_means(da05, season_months)\n",
    "        suby = select_core_cells(ya)\n",
    "        dfs = suby.to_dataframe(name=out_col).reset_index()[[\"lat\",\"lon\",\"year\",out_col]]\n",
    "        seasonal_rows.append(dfs)\n",
    "        # progress\n",
    "        if (y - years[0]) % 5 == 0:\n",
    "            print(f\"{out_col}: processed {y}\", flush=True)\n",
    "    # concat\n",
    "    monthly_long = pd.concat(monthly_rows, ignore_index=True)\n",
    "    seasonal_df  = pd.concat(seasonal_rows, ignore_index=True).sort_values([\"lat\",\"lon\",\"year\"]).reset_index(drop=True)\n",
    "    # pivot monthly to wide (May..Sep)\n",
    "    wide = monthly_long.pivot_table(index=[\"lat\",\"lon\",\"year\"], columns=\"month_name\", values=out_col, aggfunc=\"first\").reset_index()\n",
    "    for m in mon_order:\n",
    "        if m not in wide.columns: wide[m] = np.nan\n",
    "    wide = wide[[\"lat\",\"lon\",\"year\"] + mon_order]\n",
    "    wide = wide.rename(columns={m: f\"{out_col}_{m}\" for m in mon_order})\n",
    "    # optional SSR MJ/m\n",
    "    if out_col == \"solar_radiation\" and SSR_TO_MJ:\n",
    "        # add converted copies with _MJm2 suffix (keep originals too)\n",
    "        for m in mon_order:\n",
    "            wide[f\"solar_radiation_{m}_MJm2\"] = wide[f\"solar_radiation_{m}\"] / 1e6\n",
    "        seasonal_df[\"solar_radiation_MJJAS_MJm2\"] = seasonal_df[\"solar_radiation\"] / 1e6\n",
    "    return seasonal_df, wide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf66916-1267-4aea-af2a-47f1c56ebade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) Build yield panel ----------\n",
    "yield_df = build_yield_panel()\n",
    "print(\"Yield panel:\", yield_df.shape, \"| years\", yield_df[\"year\"].min(), \"\", yield_df[\"year\"].max())\n",
    "\n",
    "# Identify your yield column dynamically (handles maize/rice/etc.)\n",
    "YIELD_COL = next(c for c in yield_df.columns if c.startswith(\"yield_\"))\n",
    "\n",
    "print(\"Yield NaNs by year (first 10 years shown):\")\n",
    "print(yield_df.assign(nan=yield_df[YIELD_COL].isna())\n",
    "                .groupby(\"year\")[\"nan\"].sum()\n",
    "                .head(10))\n",
    "\n",
    "print(\"\\nYield NaNs by cell (top 10):\")\n",
    "nan_cells = (yield_df[yield_df[YIELD_COL].isna()][[\"lat\",\"lon\"]]\n",
    "               .value_counts().reset_index(name=\"nan_years\"))\n",
    "print(nan_cells.head(10))\n",
    "\n",
    "print(\"\\nAny NaNs in climate seasonals individually (after you compute them)?\")\n",
    "# If you've already built seasonal_tables, check each one:\n",
    "try:\n",
    "    for tbl in seasonal_tables:\n",
    "        cname = [c for c in tbl.columns if c not in (\"lat\",\"lon\",\"year\")][0]\n",
    "        print(cname, \"NaNs:\", int(tbl[cname].isna().sum()))\n",
    "except NameError:\n",
    "    print(\"Seasonal tables not built yet; skip this until after process step.\")\n",
    "\n",
    "# Enforce a balanced panel for this crop (keep only cells with complete yield coverage)\n",
    "years_needed = len(years)\n",
    "coverage = (yield_df.assign(ok=~yield_df[YIELD_COL].isna())\n",
    "                      .groupby([\"lat\",\"lon\"])[\"ok\"].sum())\n",
    "complete_pairs = coverage[coverage == years_needed].index.to_list()\n",
    "\n",
    "print(f\"\\nBalanced-panel filter: keeping {len(complete_pairs)} cells with all {years_needed} years of yield.\")\n",
    "\n",
    "# If some cells are incomplete, shrink the dataset and update lat/lon list + bbox\n",
    "if len(complete_pairs) < yield_df[[\"lat\",\"lon\"]].drop_duplicates().shape[0]:\n",
    "    comp_df = pd.DataFrame(complete_pairs, columns=[\"lat\",\"lon\"])\n",
    "    yield_df = yield_df.merge(comp_df, on=[\"lat\",\"lon\"], how=\"inner\").sort_values([\"lat\",\"lon\",\"year\"]).reset_index(drop=True)\n",
    "\n",
    "    # Update the global latc/lonc so downstream ERA5 selection uses the same subset\n",
    "    latc = comp_df[\"lat\"].to_numpy()\n",
    "    lonc = comp_df[\"lon\"].to_numpy()\n",
    "\n",
    "    # Tighten bbox accordingly\n",
    "    BBOX = dict(\n",
    "        lat_min=float(latc.min() - 0.5),\n",
    "        lat_max=float(latc.max() + 0.5),\n",
    "        lon_min=float(lonc.min() - 0.5),\n",
    "        lon_max=float(lonc.max() + 0.5),\n",
    "    )\n",
    "\n",
    "# Optional: sanity check that yield_df is now fully non-missing\n",
    "assert yield_df[YIELD_COL].isna().sum() == 0, \"Yield still has NaNs after balancing.\"\n",
    "\n",
    "\n",
    "# ---------- 2) Process ERA5 variables ----------\n",
    "# Map of variables to process (shortName, xr_name, out_col, type)\n",
    "vars_cfg = [\n",
    "    (\"2t\",   \"t2m\",   \"temperature\",           \"state\"),\n",
    "    (\"tp\",   \"tp\",    \"precipitation\",         \"flux\"),\n",
    "    (\"swvl1\",\"swvl1\", \"soil_water\",            \"state\"),\n",
    "    (\"ssr\",  \"ssr\",   \"solar_radiation\",       \"flux\"),\n",
    "    (\"pev\",  \"pev\",   \"potential_evaporation\", \"flux\"),\n",
    "]\n",
    "\n",
    "seasonal_tables = []\n",
    "monthly_tables  = []\n",
    "\n",
    "for short, xr_name, out_col, vtype in vars_cfg:\n",
    "    seas, mon = process_era5_var(short, xr_name, out_col, vtype)\n",
    "    seasonal_tables.append(seas)\n",
    "    monthly_tables.append(mon)\n",
    "\n",
    "# ---------- 3) Merge seasonal into base ----------\n",
    "base = yield_df.copy()\n",
    "for tbl in seasonal_tables:\n",
    "    base = base.merge(tbl, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "\n",
    "# Basic checks\n",
    "assert len(base) == len(yield_df), \"Row count changed unexpectedly after seasonal merge\"\n",
    "assert base.isna().sum().sum() == 0, \"NaNs found after seasonal merge\"\n",
    "\n",
    "# ---------- 4) Save seasonal CSV ----------\n",
    "base_name = f\"{crop}_{region_tag}_{start_year}_{end_year}_allstressors.csv\"\n",
    "base_csv  = DERIVED / base_name\n",
    "base.to_csv(base_csv, index=False)\n",
    "print(\"Saved seasonal dataset \", base_csv)\n",
    "\n",
    "# ---------- 5) Merge monthly wide tables ----------\n",
    "enriched = base.copy()\n",
    "for mon in monthly_tables:\n",
    "    enriched = enriched.merge(mon, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "\n",
    "assert len(enriched) == len(base), \"Row count changed unexpectedly after monthly merge\"\n",
    "assert enriched.isna().sum().sum() == 0, \"NaNs found after monthly merge\"\n",
    "\n",
    "# ---------- 6) Save enriched CSV ----------\n",
    "enriched_name = f\"{crop}_{region_tag}_{start_year}_{end_year}_allstressors_with_monthly.csv\"\n",
    "enriched_csv  = DERIVED / enriched_name\n",
    "enriched.to_csv(enriched_csv, index=False)\n",
    "print(\"Saved enriched dataset with monthly columns \", enriched_csv)\n",
    "\n",
    "# Preview\n",
    "print(\"Columns (first 16):\", list(enriched.columns)[:16], \"\")\n",
    "print(\"Rows:\", len(enriched))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e108030-e074-4c74-9274-c447c61b9c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quick sanity checks on ranges (prints only)\n",
    "cols_check = [YIELD_COL,\"temperature\",\"precipitation\",\"soil_water\",\"solar_radiation\",\"potential_evaporation\"]\n",
    "print(\"Seasonal ranges:\")\n",
    "for c in cols_check:\n",
    "    lo, hi = enriched[c].min(), enriched[c].max()\n",
    "    print(f\"  {c:>22s}: min={lo:.3f}  max={hi:.3f}\")\n",
    "\n",
    "# Confirm MaySep columns exist\n",
    "must_have = [f\"temperature_{m}\" for m in mon_order] + [f\"precipitation_{m}\" for m in mon_order]\n",
    "missing = [c for c in must_have if c not in enriched.columns]\n",
    "print(\"Monthly cols missing:\", missing)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 29-wheat-dataset-creation-pipeline.ipynb/29-wheat-dataset-creation-pipeline.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98574e1b-82a4-47b5-b558-6925ffd7d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== USER INPUTS (edit) ====\n",
    "\n",
    "# Tag used in output filenames\n",
    "region_tag = \"ITnorth_core41\"      # if balancing drops a cell, feel free to rename to core41 manually\n",
    "\n",
    "# Years (inclusive)\n",
    "start_year = 1982\n",
    "end_year   = 2016\n",
    "years      = list(range(start_year, end_year + 1))\n",
    "\n",
    "# Seasons (list months in SEASON ORDER, startend)\n",
    "winter_months = [11,12,1,2,3,4,5,6]  # NovJun (wraps over DecJan)\n",
    "spring_months = [3,4,5,6,7]          # MarJul\n",
    "\n",
    "# Paths\n",
    "DERIVED     = r\"..\\italy_core_data\\derived\"\n",
    "MASK_CSV    = r\"..\\italy_core_data\\derived\\mask_core_42_on_gdhy.csv\"\n",
    "\n",
    "# GDHY yield directories (per-year yield_YYYY.nc4 files)\n",
    "GDHY_WINTER_DIR = r\"..\\data\\wheat_winter\"\n",
    "GDHY_SPRING_DIR = r\"..\\data\\wheat_spring\"\n",
    "GDHY_FILE_TMPL  = \"yield_{year}.nc4\"     # change if filenames differ\n",
    "GDHY_VARNAME    = None                   # None  auto-detect (first data_var)\n",
    "\n",
    "# ERA5 monthly averaged GRIBs (one file per year)\n",
    "ERA5_DIR        = r\"..\\data\\climate_monthly_full\"\n",
    "ERA5_FILE_TMPL  = \"era5_land_monthly_{year}.grib\"\n",
    "\n",
    "# Units convenience\n",
    "SSR_TO_MJ = False   # if True, also add solar_radiation_*_MJm2 columns\n",
    "\n",
    "# Month labels\n",
    "month_names = {1:\"Jan\",2:\"Feb\",3:\"Mar\",4:\"Apr\",5:\"May\",6:\"Jun\",7:\"Jul\",8:\"Aug\",9:\"Sep\",10:\"Oct\",11:\"Nov\",12:\"Dec\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98080181-7d29-4fde-af09-c6962577b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DERIVED = Path(DERIVED); DERIVED.mkdir(exist_ok=True)\n",
    "\n",
    "# --- mask & bbox from your 42-core cells ---\n",
    "m42 = pd.read_csv(MASK_CSV).sort_values([\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "LATC_ALL = m42[\"lat\"].to_numpy()\n",
    "LONC_ALL = m42[\"lon\"].to_numpy()\n",
    "\n",
    "def make_bbox(latc, lonc, pad=0.5):\n",
    "    return dict(\n",
    "        lat_min=float(latc.min() - pad),\n",
    "        lat_max=float(latc.max() + pad),\n",
    "        lon_min=float(lonc.min() - pad),\n",
    "        lon_max=float(lonc.max() + pad),\n",
    "    )\n",
    "\n",
    "# --- file finders ---\n",
    "def yield_path(gdhy_dir: Path, y: int) -> Path:\n",
    "    p = gdhy_dir / GDHY_FILE_TMPL.format(year=y)\n",
    "    if p.exists(): return p\n",
    "    cands = list(gdhy_dir.glob(f\"*{y}*.nc*\"))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No GDHY yield for {y} in {gdhy_dir}\")\n",
    "    return cands[0]\n",
    "\n",
    "def era5_path(y: int) -> Path:\n",
    "    p = Path(ERA5_DIR) / ERA5_FILE_TMPL.format(year=y)\n",
    "    if p.exists(): return p\n",
    "    cands = list(Path(ERA5_DIR).glob(f\"*{y}*.grib\"))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No ERA5 GRIB for {y} in {ERA5_DIR}\")\n",
    "    return cands[0]\n",
    "\n",
    "# --- ERA5 opener with cropping; returns DataArray (time, lat, lon) ---\n",
    "def open_era5_var_year(y: int, short: str, xr_name: str, bbox: dict) -> xr.DataArray:\n",
    "    f = era5_path(y)\n",
    "    ds = xr.open_dataset(\n",
    "        f, engine=\"cfgrib\",\n",
    "        backend_kwargs=dict(indexpath=\"\", filter_by_keys={\"shortName\": short}),\n",
    "        decode_timedelta=True,\n",
    "    ).rename({\"latitude\":\"lat\",\"longitude\":\"lon\"})\n",
    "    if ds.lat[0] > ds.lat[-1]:\n",
    "        ds = ds.sortby(\"lat\")\n",
    "    if ds.lon.min() < 0:\n",
    "        ds = ds.assign_coords(lon=((ds.lon % 360 + 360) % 360)).sortby(\"lon\")\n",
    "    ds = ds.sel(lat=slice(bbox[\"lat_min\"], bbox[\"lat_max\"]),\n",
    "                lon=slice(bbox[\"lon_min\"], bbox[\"lon_max\"]))\n",
    "    da = ds[xr_name]\n",
    "    if \"expver\" in da.dims:\n",
    "        da = da.isel(expver=-1)\n",
    "    return da\n",
    "\n",
    "# --- 0.1  0.5 block-average to GDHY centers (offset 0.25) ---\n",
    "def bin_to_half_degree(da: xr.DataArray, step=0.5, offset=0.25) -> xr.DataArray:\n",
    "    lat_bins = (offset + step * np.round((da[\"lat\"].values - offset) / step)).astype(np.float64)\n",
    "    lon_bins = (offset + step * np.round((da[\"lon\"].values - offset) / step)).astype(np.float64)\n",
    "    da = da.assign_coords(lat_bin=(\"lat\", lat_bins), lon_bin=(\"lon\", lon_bins))\n",
    "    da_c = da.groupby(\"lat_bin\").mean(\"lat\").groupby(\"lon_bin\").mean(\"lon\")\n",
    "    return da_c.rename({\"lat_bin\":\"lat\",\"lon_bin\":\"lon\"})\n",
    "\n",
    "# --- select a given set of cells by lat/lon value (nearest) ---\n",
    "def select_cells(da: xr.DataArray, latc: np.ndarray, lonc: np.ndarray) -> xr.DataArray:\n",
    "    out = da.sel(lat=xr.DataArray(latc, dims=\"cell\"),\n",
    "                 lon=xr.DataArray(lonc, dims=\"cell\"),\n",
    "                 method=\"nearest\")\n",
    "    return out.assign_coords(cell=(\"cell\", np.arange(len(latc))),\n",
    "                             lat=(\"cell\", latc), lon=(\"cell\", lonc))\n",
    "\n",
    "# --- wrap-around season helpers (DecJan) ---\n",
    "def season_wraps(months: list[int]) -> bool:\n",
    "    \"\"\"True if the season crosses DecJan (e.g., [11,12,1,...]). Assumes months are in season order.\"\"\"\n",
    "    return months != sorted(months)\n",
    "\n",
    "def crop_year_for_series(month: pd.Series, year: pd.Series, pivot_month: int) -> pd.Series:\n",
    "    \"\"\"Year + 1 for months >= pivot_month (e.g., Nov/Dec go to next harvest year).\"\"\"\n",
    "    return year + (month >= pivot_month).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559b03a-dcc0-47b0-9d7f-2c9e4ab25f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- monthly reducers ----\n",
    "def monthly_means(da_m05: xr.DataArray, months: list[int]) -> xr.DataArray:\n",
    "    return da_m05.where(da_m05[\"time\"].dt.month.isin(months), drop=True)\n",
    "\n",
    "def monthly_totals_from_daily_means(da_m05: xr.DataArray, months: list[int]) -> xr.DataArray:\n",
    "    sel = da_m05.where(da_m05[\"time\"].dt.month.isin(months), drop=True)\n",
    "    days = xr.DataArray(sel[\"time\"].dt.days_in_month, coords={\"time\": sel[\"time\"]}, dims=[\"time\"]).astype(np.float64)\n",
    "    return sel * days\n",
    "\n",
    "# ---- seasonal reducers (crop-year aware) ----\n",
    "def seasonal_reduce(da_m05: xr.DataArray, months: list[int], kind: str) -> xr.DataArray:\n",
    "    \"\"\"kind: 'mean' for state; 'sum' for flux monthly totals.\"\"\"\n",
    "    if kind == \"mean\":\n",
    "        sel = monthly_means(da_m05, months)\n",
    "    elif kind == \"sum\":\n",
    "        sel = monthly_totals_from_daily_means(da_m05, months)\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'mean' or 'sum'\")\n",
    "    if season_wraps(months):\n",
    "        pivot = months[0]\n",
    "        cy = xr.DataArray(sel[\"time\"].dt.year + (sel[\"time\"].dt.month >= pivot).astype(int),\n",
    "                          coords={\"time\": sel[\"time\"]}, dims=[\"time\"], name=\"year\")\n",
    "        out = sel.groupby(cy).mean(\"time\") if kind == \"mean\" else sel.groupby(cy).sum(\"time\")\n",
    "    else:\n",
    "        out = sel.groupby(\"time.year\").mean(\"time\") if kind == \"mean\" else sel.groupby(\"time.year\").sum(\"time\")\n",
    "        out = out.rename({\"year\":\"year\"})\n",
    "    return out\n",
    "\n",
    "# ---- process one ERA5 variable (returns seasonal_df, monthly_wide_df) ----\n",
    "def process_era5_var_cropyear(short: str, xr_name: str, out_col: str, vtype: str,\n",
    "                              season_months: list[int], latc: np.ndarray, lonc: np.ndarray,\n",
    "                              bbox: dict, years: list[int], month_names: dict, ssr_to_mj: bool=False):\n",
    "    \"\"\"\n",
    "    Crop-yearaware processor (safe for wrapping seasons like NovJun).\n",
    "    Returns: (seasonal_df, monthly_wide_df) with harvest-year 'year'.\n",
    "    vtype: 'state' (mean) or 'flux' (sum of monthly totals).\n",
    "    \"\"\"\n",
    "\n",
    "    wraps = season_wraps(season_months)\n",
    "    pivot = season_months[0] if wraps else None\n",
    "\n",
    "    # Calendar years to open:\n",
    "    # - non-wrap: exactly the requested years\n",
    "    # - wrap: include the *year before* start so NovDec for the first harvest year exist\n",
    "    y0 = years[0] - 1 if wraps else years[0]\n",
    "    y1 = years[-1]\n",
    "    cal_years = list(range(y0, y1 + 1))\n",
    "\n",
    "    monthly_rows = []\n",
    "\n",
    "    for y in cal_years:\n",
    "        da = open_era5_var_year(y, short, xr_name, bbox)\n",
    "\n",
    "        # unit conversions on monthly fields\n",
    "        if out_col == \"temperature\":\n",
    "            da = da - 273.15\n",
    "        if out_col == \"precipitation\":\n",
    "            da = da * 1000.0          # m -> mm (daily mean)\n",
    "        if out_col == \"potential_evaporation\":\n",
    "            da = -da * 1000.0         # flip sign, m -> mm (daily mean)\n",
    "\n",
    "        da05 = bin_to_half_degree(da)\n",
    "\n",
    "        # Build monthly values for requested months (state vs flux)\n",
    "        if vtype == \"state\":\n",
    "            dam = monthly_means(da05, season_months)                        # monthly means\n",
    "        elif vtype == \"flux\":\n",
    "            dam = monthly_totals_from_daily_means(da05, season_months)      # monthly totals\n",
    "        else:\n",
    "            raise ValueError(\"vtype must be 'state' or 'flux'\")\n",
    "\n",
    "        # Select analysis cells and go to a long table\n",
    "        sub = select_cells(dam, latc, lonc)\n",
    "        df = sub.to_dataframe(name=out_col).reset_index()\n",
    "\n",
    "        # Calendar month/year and days_in_month\n",
    "        t = pd.to_datetime(df[\"time\"])\n",
    "        df[\"month\"] = t.dt.month\n",
    "        df[\"month_name\"] = df[\"month\"].map(month_names)\n",
    "        df[\"cal_year\"] = t.dt.year\n",
    "        df[\"days\"] = t.dt.days_in_month.astype(float)\n",
    "\n",
    "        # Assign harvest year (crop-year)\n",
    "        if wraps:\n",
    "            df[\"year\"] = df[\"cal_year\"] + (df[\"month\"] >= pivot).astype(int)\n",
    "        else:\n",
    "            df[\"year\"] = df[\"cal_year\"]\n",
    "\n",
    "        # Keep only season months\n",
    "        df = df[df[\"month\"].isin(season_months)].copy()\n",
    "\n",
    "        monthly_rows.append(df[[\"lat\",\"lon\",\"year\",\"month\",\"month_name\",\"days\",out_col]])\n",
    "\n",
    "        if (y - cal_years[0]) % 5 == 0:\n",
    "            print(f\"{out_col}: processed {y}\", flush=True)\n",
    "\n",
    "    # All monthly rows across needed calendar years\n",
    "    m = pd.concat(monthly_rows, ignore_index=True)\n",
    "\n",
    "    # Keep only harvest years in the requested range\n",
    "    m = m[(m[\"year\"] >= years[0]) & (m[\"year\"] <= years[-1])].copy()\n",
    "\n",
    "    # ----- Seasonal from monthly (robust across wrap) -----\n",
    "    keys = [\"lat\",\"lon\",\"year\"]\n",
    "\n",
    "    if vtype == \"flux\":\n",
    "        # Already monthly totals  seasonal total = sum\n",
    "        seas = (m.groupby(keys, as_index=False)[out_col].sum())\n",
    "    else:\n",
    "        # State monthly means  seasonal days-weighted mean\n",
    "        num = (m.assign(wx=m[out_col]*m[\"days\"])\n",
    "                 .groupby(keys, as_index=False)[[\"wx\"]].sum()\n",
    "                 .rename(columns={\"wx\": \"num\"}))\n",
    "        den = (m.groupby(keys, as_index=False)[[\"days\"]].sum()\n",
    "                 .rename(columns={\"days\": \"den\"}))\n",
    "        seas = num.merge(den, on=keys, how=\"inner\")\n",
    "        seas[out_col] = seas[\"num\"] / seas[\"den\"]\n",
    "        seas = seas[keys + [out_col]]\n",
    "\n",
    "    # Consistent sort\n",
    "    seasonal_df = seas.sort_values(keys).reset_index(drop=True)\n",
    "\n",
    "    # ----- Monthly wide for just this season -----\n",
    "    mon_order = [month_names[mn] for mn in season_months]\n",
    "    wide = (m.pivot_table(index=keys, columns=\"month_name\", values=out_col, aggfunc=\"first\")\n",
    "              .reset_index())\n",
    "    for mn in mon_order:\n",
    "        if mn not in wide.columns:\n",
    "            wide[mn] = np.nan\n",
    "    wide = wide[keys + mon_order]\n",
    "    wide = wide.rename(columns={mn: f\"{out_col}_{mn}\" for mn in mon_order})\n",
    "\n",
    "    # Optional SSR copies in MJ/m\n",
    "    if out_col == \"solar_radiation\" and ssr_to_mj:\n",
    "        for mn in mon_order:\n",
    "            col = f\"solar_radiation_{mn}\"\n",
    "            if col in wide.columns:\n",
    "                wide[f\"{col}_MJm2\"] = wide[col] / 1e6\n",
    "\n",
    "    return seasonal_df, wide\n",
    "\n",
    "# ---- build yield panel for a given crop dir and cell list ----\n",
    "def build_yield_panel(gdhy_dir: str, crop_name: str, latc: np.ndarray, lonc: np.ndarray) -> pd.DataFrame:\n",
    "    gdhy_dir = Path(gdhy_dir)\n",
    "    rows = []\n",
    "    for y in years:\n",
    "        f = yield_path(gdhy_dir, y)\n",
    "        ds = xr.open_dataset(f)\n",
    "        varname = GDHY_VARNAME or [v for v in ds.data_vars][0]\n",
    "        da = ds[varname].squeeze()\n",
    "        if set((\"lat\",\"lon\")).issubset(da.dims):\n",
    "            da = da.transpose(\"lat\",\"lon\")\n",
    "        else:\n",
    "            raise ValueError(f\"{f} missing lat/lon dims; got {da.dims}\")\n",
    "        # map the requested lat/lon centers to indices\n",
    "        lat_vals = da[\"lat\"].values; lon_vals = da[\"lon\"].values\n",
    "        def idx(val, axis_vals):\n",
    "            i = int(np.argmin(np.abs(axis_vals - val)))\n",
    "            if not np.isclose(axis_vals[i], val, atol=5e-4):\n",
    "                raise ValueError(\"Grid mismatch locating indices.\")\n",
    "            return i\n",
    "        ilat = np.array([idx(v, lat_vals) for v in latc])\n",
    "        ilon = np.array([idx(v, lon_vals) for v in lonc])\n",
    "        vals = da.values[ilat, ilon].astype(\"float32\")\n",
    "        rows.append(pd.DataFrame({\"year\": y, \"lat\": latc, \"lon\": lonc, f\"yield_{crop_name}\": vals}))\n",
    "    return pd.concat(rows, ignore_index=True).sort_values([\"lat\",\"lon\",\"year\"]).reset_index(drop=True)\n",
    "\n",
    "# ---- enforce balanced panel by yield (keeps only cells with all years present) ----\n",
    "def balance_by_yield(yield_df: pd.DataFrame, yield_col: str) -> tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    yrs_needed = len(years)\n",
    "    cov = (yield_df.assign(ok=~yield_df[yield_col].isna())\n",
    "                   .groupby([\"lat\",\"lon\"])[\"ok\"].sum())\n",
    "    keep_pairs = cov[cov == yrs_needed].index.to_list()\n",
    "    keep = pd.DataFrame(keep_pairs, columns=[\"lat\",\"lon\"])\n",
    "    out = (yield_df.merge(keep, on=[\"lat\",\"lon\"], how=\"inner\")\n",
    "                  .sort_values([\"lat\",\"lon\",\"year\"]).reset_index(drop=True))\n",
    "    latc = keep[\"lat\"].to_numpy()\n",
    "    lonc = keep[\"lon\"].to_numpy()\n",
    "    return out, latc, lonc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878db6b-cb81-4270-94c6-1d98930892d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== WINTER WHEAT =====\n",
    "crop_name = \"wheat_winter\"\n",
    "latc = LATC_ALL.copy(); lonc = LONC_ALL.copy()\n",
    "bbox = make_bbox(latc, lonc)\n",
    "\n",
    "# 1) Yield panel\n",
    "yield_df = build_yield_panel(GDHY_WINTER_DIR, crop_name, latc, lonc)\n",
    "YIELD_COL = f\"yield_{crop_name}\"\n",
    "yield_df, latc, lonc = balance_by_yield(yield_df, YIELD_COL)\n",
    "bbox = make_bbox(latc, lonc)\n",
    "print(f\"Winter wheat: kept {len(np.unique(list(zip(latc,lonc)), axis=0))} cells after balancing.\")\n",
    "\n",
    "# 2) ERA5 variables\n",
    "vars_cfg = [\n",
    "    (\"2t\",\"t2m\",\"temperature\",\"state\"),\n",
    "    (\"tp\",\"tp\",\"precipitation\",\"flux\"),\n",
    "    (\"swvl1\",\"swvl1\",\"soil_water\",\"state\"),\n",
    "    (\"ssr\",\"ssr\",\"solar_radiation\",\"flux\"),\n",
    "    (\"pev\",\"pev\",\"potential_evaporation\",\"flux\"),\n",
    "]\n",
    "seasonal_tables = []; monthly_tables = []\n",
    "for short, xr_name, out_col, vtype in vars_cfg:\n",
    "    seas, mon = process_era5_var_cropyear(short, xr_name, out_col, vtype,\n",
    "                                          winter_months, latc, lonc, bbox, years, month_names, SSR_TO_MJ)\n",
    "    seasonal_tables.append(seas); monthly_tables.append(mon)\n",
    "\n",
    "# 3) Merge seasonal into base & save\n",
    "base = yield_df.copy()\n",
    "for tbl in seasonal_tables:\n",
    "    base = base.merge(tbl, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "assert len(base) == len(yield_df) and base.isna().sum().sum()==0\n",
    "\n",
    "out_seasonal = DERIVED / f\"{crop_name}_{region_tag}_{start_year}_{end_year}_allstressors.csv\"\n",
    "base.to_csv(out_seasonal, index=False)\n",
    "print(\"Saved \", out_seasonal)\n",
    "\n",
    "# 4) Add monthly & save\n",
    "enriched = base.copy()\n",
    "for mon in monthly_tables:\n",
    "    enriched = enriched.merge(mon, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "assert len(enriched) == len(base) and enriched.isna().sum().sum()==0\n",
    "\n",
    "out_monthly = DERIVED / f\"{crop_name}_{region_tag}_{start_year}_{end_year}_allstressors_with_monthly.csv\"\n",
    "enriched.to_csv(out_monthly, index=False)\n",
    "print(\"Saved \", out_monthly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccaff46-ca6e-4332-9cfd-c8c9c07d675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SPRING WHEAT =====\n",
    "crop_name = \"wheat_spring\"\n",
    "latc = LATC_ALL.copy(); lonc = LONC_ALL.copy()\n",
    "bbox = make_bbox(latc, lonc)\n",
    "\n",
    "# 1) Yield panel\n",
    "yield_df = build_yield_panel(GDHY_SPRING_DIR, crop_name, latc, lonc)\n",
    "YIELD_COL = f\"yield_{crop_name}\"\n",
    "yield_df, latc, lonc = balance_by_yield(yield_df, YIELD_COL)\n",
    "bbox = make_bbox(latc, lonc)\n",
    "print(f\"Spring wheat: kept {len(np.unique(list(zip(latc,lonc)), axis=0))} cells after balancing.\")\n",
    "\n",
    "# 2) ERA5 variables\n",
    "vars_cfg = [\n",
    "    (\"2t\",\"t2m\",\"temperature\",\"state\"),\n",
    "    (\"tp\",\"tp\",\"precipitation\",\"flux\"),\n",
    "    (\"swvl1\",\"swvl1\",\"soil_water\",\"state\"),\n",
    "    (\"ssr\",\"ssr\",\"solar_radiation\",\"flux\"),\n",
    "    (\"pev\",\"pev\",\"potential_evaporation\",\"flux\"),\n",
    "]\n",
    "seasonal_tables = []; monthly_tables = []\n",
    "for short, xr_name, out_col, vtype in vars_cfg:\n",
    "    seas, mon = process_era5_var_cropyear(short, xr_name, out_col, vtype,\n",
    "                                          spring_months, latc, lonc, bbox, years, month_names, SSR_TO_MJ)\n",
    "    seasonal_tables.append(seas); monthly_tables.append(mon)\n",
    "\n",
    "# 3) Merge seasonal into base & save\n",
    "base = yield_df.copy()\n",
    "for tbl in seasonal_tables:\n",
    "    base = base.merge(tbl, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "assert len(base) == len(yield_df) and base.isna().sum().sum()==0\n",
    "\n",
    "out_seasonal = DERIVED / f\"{crop_name}_{region_tag}_{start_year}_{end_year}_allstressors.csv\"\n",
    "base.to_csv(out_seasonal, index=False)\n",
    "print(\"Saved \", out_seasonal)\n",
    "\n",
    "# 4) Add monthly & save\n",
    "enriched = base.copy()\n",
    "for mon in monthly_tables:\n",
    "    enriched = enriched.merge(mon, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "assert len(enriched) == len(base) and enriched.isna().sum().sum()==0\n",
    "\n",
    "out_monthly = DERIVED / f\"{crop_name}_{region_tag}_{start_year}_{end_year}_allstressors_with_monthly.csv\"\n",
    "enriched.to_csv(out_monthly, index=False)\n",
    "print(\"Saved \", out_monthly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc627ece-3712-41be-961a-24206d801de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STACK WINTER + SPRING WHEAT INTO ONE LONG FILE (TYPE-LABELED) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- EDIT THESE 3 LINES ----\n",
    "DERIVED    = Path(r\"..\\italy_core_data\\derived\")\n",
    "region_tag = \"ITnorth_core41\"   # use the same tag you used for the per-type exports\n",
    "start_year, end_year = 1982, 2016\n",
    "# ----------------------------\n",
    "\n",
    "winter_csv = DERIVED / f\"wheat_winter_{region_tag}_{start_year}_{end_year}_allstressors_with_monthly.csv\"\n",
    "spring_csv = DERIVED / f\"wheat_spring_{region_tag}_{start_year}_{end_year}_allstressors_with_monthly.csv\"\n",
    "out_csv    = DERIVED / f\"wheat_both_{region_tag}_{start_year}_{end_year}_allstressors_with_monthly_long.csv\"\n",
    "\n",
    "# ---- helpers ----\n",
    "def load_and_tag(p: Path, wheat_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV, tag wheat_type, normalize yield_* -> yield_wheat.\"\"\"\n",
    "    df = pd.read_csv(p)\n",
    "    df[\"wheat_type\"] = wheat_type\n",
    "    ycols = [c for c in df.columns if c.startswith(\"yield_\")]\n",
    "    if not ycols:\n",
    "        raise ValueError(f\"No yield_* column found in {p.name}\")\n",
    "    if \"yield_wheat\" not in df.columns:\n",
    "        df[\"yield_wheat\"] = df[ycols[0]]\n",
    "        for c in ycols:\n",
    "            if c != \"yield_wheat\":\n",
    "                df.drop(columns=c, inplace=True)\n",
    "    return df\n",
    "\n",
    "dw = load_and_tag(winter_csv, \"winter\")\n",
    "ds = load_and_tag(spring_csv, \"spring\")\n",
    "\n",
    "# required keys & seasonals in each input\n",
    "key_cols = [\"lat\",\"lon\",\"year\",\"wheat_type\"]\n",
    "seasonal_cols = [\"yield_wheat\",\"temperature\",\"precipitation\",\"soil_water\",\"solar_radiation\",\"potential_evaporation\"]\n",
    "for req in seasonal_cols:\n",
    "    for d, name in [(dw, winter_csv.name), (ds, spring_csv.name)]:\n",
    "        if req not in d.columns:\n",
    "            raise ValueError(f\"Missing seasonal column '{req}' in {name}\")\n",
    "\n",
    "# ---- robust monthly union (no string parsing) ----\n",
    "mon_names = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "base_vars = [\"temperature\",\"precipitation\",\"soil_water\",\"solar_radiation\",\"potential_evaporation\"]\n",
    "\n",
    "def find_monthly_cols(df: pd.DataFrame):\n",
    "    cols = []\n",
    "    for b in base_vars:\n",
    "        for m in mon_names:\n",
    "            c = f\"{b}_{m}\"\n",
    "            if c in df.columns:\n",
    "                cols.append(c)\n",
    "    return cols\n",
    "\n",
    "cols_all = set(find_monthly_cols(dw)) | set(find_monthly_cols(ds))\n",
    "monthly_union = [f\"{b}_{m}\" for b in base_vars for m in mon_names if f\"{b}_{m}\" in cols_all]\n",
    "\n",
    "# ensure both frames have the full monthly schema (add NaNs where missing)\n",
    "for c in monthly_union:\n",
    "    if c not in dw.columns: dw[c] = np.nan\n",
    "    if c not in ds.columns: ds[c] = np.nan\n",
    "\n",
    "# order columns consistently\n",
    "ordered_cols = key_cols + seasonal_cols + monthly_union\n",
    "dw = dw[ordered_cols].sort_values([\"lat\",\"lon\",\"year\"]).reset_index(drop=True)\n",
    "ds = ds[ordered_cols].sort_values([\"lat\",\"lon\",\"year\"]).reset_index(drop=True)\n",
    "\n",
    "# stack long\n",
    "combined = pd.concat([dw, ds], ignore_index=True).sort_values([\"lat\",\"lon\",\"year\",\"wheat_type\"]).reset_index(drop=True)\n",
    "\n",
    "# sanity checks\n",
    "dups = combined.duplicated([\"lat\",\"lon\",\"year\",\"wheat_type\"]).sum()\n",
    "assert dups == 0, f\"Found duplicated keys: {dups}\"\n",
    "assert \"yield_wheat\" in combined.columns\n",
    "\n",
    "# save\n",
    "combined.to_csv(out_csv, index=False)\n",
    "print(\"Saved stacked long wheat dataset \", out_csv)\n",
    "print(\"Shape:\", combined.shape)\n",
    "print(\"Columns (first 20):\", combined.columns[:20].tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 30-QA-and-Metadata.ipynb/30-QA-and-Metadata.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b565dc-ab6a-48a3-ba24-192665cdcc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, time\n",
    "\n",
    "# ===== paths =====\n",
    "CORE_DIR = Path(r\"..\\italy_core_data\")\n",
    "DERIVED  = CORE_DIR / \"derived\"\n",
    "IN_CSV   = DERIVED / \"maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv\"\n",
    "MASK_CSV = DERIVED / \"mask_core_42_on_gdhy.csv\"\n",
    "OUT_DIR  = DERIVED / \"QA\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# (optional) plots\n",
    "PLOT = True\n",
    "\n",
    "# ===== load =====\n",
    "df   = pd.read_csv(IN_CSV)\n",
    "mask = pd.read_csv(MASK_CSV).sort_values([\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "latc, lonc = mask[\"lat\"].to_numpy(), mask[\"lon\"].to_numpy()\n",
    "\n",
    "# ===== basic structure checks =====\n",
    "assert {\"lat\",\"lon\",\"year\"}.issubset(df.columns)\n",
    "n_rows, n_cols = df.shape\n",
    "uniq_cells = df[[\"lat\",\"lon\"]].drop_duplicates().shape[0]\n",
    "uniq_years = df[\"year\"].nunique()\n",
    "dups = df.duplicated([\"lat\",\"lon\",\"year\"]).sum()\n",
    "nans_total = int(df.isna().sum().sum())\n",
    "\n",
    "print(f\"Rows: {n_rows}  (expect 1470)\")\n",
    "print(f\"Cols: {n_cols}\")\n",
    "print(f\"Unique cells: {uniq_cells} (expect 42)\")\n",
    "print(f\"Unique years: {uniq_years} (expect 35)\")\n",
    "print(f\"Duplicate (lat,lon,year): {dups} (expect 0)\")\n",
    "print(f\"Total NaNs: {nans_total} (expect 0)\")\n",
    "\n",
    "# fail-fast assertions\n",
    "assert n_rows == 42*35, \"Unexpected row count\"\n",
    "assert uniq_cells == 42, \"Unexpected number of cells\"\n",
    "assert uniq_years == 35, \"Unexpected number of years\"\n",
    "assert dups == 0, \"Duplicate keys detected\"\n",
    "assert nans_total == 0, \"Missing values found\"\n",
    "\n",
    "# ===== per-cell year coverage =====\n",
    "coverage = (df.groupby([\"lat\",\"lon\"])[\"year\"]\n",
    "              .agg([\"count\", \"min\", \"max\"])\n",
    "              .rename(columns={\"count\":\"years_present\"}))\n",
    "coverage.to_csv(OUT_DIR / \"qa_cell_year_counts.csv\")\n",
    "assert (coverage[\"years_present\"] == 35).all(), \"Some cells are missing years\"\n",
    "\n",
    "# ===== variable organization =====\n",
    "# identify seasonal columns (already present) and monthly columns (May..Sep)\n",
    "month_suffixes = [\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\"]\n",
    "is_month_col = df.columns.str.contains(\"_(\" + \"|\".join(month_suffixes) + \")$\", regex=True)\n",
    "monthly_cols = df.columns[is_month_col].tolist()\n",
    "\n",
    "seasonal_cols = [\n",
    "    \"yield_maize\",\"temperature\",\"precipitation\",\"soil_water\",\"solar_radiation\",\"potential_evaporation\"\n",
    "]\n",
    "present_seasonal = [c for c in seasonal_cols if c in df.columns]\n",
    "\n",
    "# ===== stats summary (per column) =====\n",
    "def summarize(col):\n",
    "    s = df[col]\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        return {\n",
    "            \"count\": int(s.count()),\n",
    "            \"min\": float(s.min()),\n",
    "            \"p05\": float(s.quantile(0.05)),\n",
    "            \"median\": float(s.median()),\n",
    "            \"p95\": float(s.quantile(0.95)),\n",
    "            \"max\": float(s.max()),\n",
    "        }\n",
    "    else:\n",
    "        return {\"count\": int(s.count()), \"unique\": int(s.nunique())}\n",
    "\n",
    "summary = {}\n",
    "for c in [\"yield_maize\",\"temperature\",\"precipitation\",\"soil_water\",\"solar_radiation\",\"potential_evaporation\"]:\n",
    "    if c in df.columns:\n",
    "        summary[c] = summarize(c)\n",
    "\n",
    "# add monthly summaries (group by variable base)\n",
    "def base_name(c):\n",
    "    for suf in month_suffixes:\n",
    "        if c.endswith(\"_\"+suf):\n",
    "            return c[:-(len(suf)+1)]\n",
    "    return None\n",
    "\n",
    "monthly_groups = {}\n",
    "for c in monthly_cols:\n",
    "    b = base_name(c)\n",
    "    monthly_groups.setdefault(b, []).append(c)\n",
    "\n",
    "for b, cols in monthly_groups.items():\n",
    "    cols_sorted = sorted(cols, key=lambda x: month_suffixes.index(x.split(\"_\")[-1]))\n",
    "    stats = {m: summarize(m) for m in cols_sorted}\n",
    "    summary[b + \"_monthly\"] = stats\n",
    "\n",
    "# write a flat CSV of simple stats for quick viewing\n",
    "flat_rows = []\n",
    "for k,v in summary.items():\n",
    "    if isinstance(v, dict) and all(isinstance(vv, dict) for vv in v.values()):\n",
    "        # monthly block\n",
    "        for mon, sv in v.items():\n",
    "            flat_rows.append({\"variable\": k, \"component\": mon, **sv})\n",
    "    else:\n",
    "        flat_rows.append({\"variable\": k, \"component\": \"seasonal\", **v})\n",
    "pd.DataFrame(flat_rows).to_csv(OUT_DIR / \"qa_summary.csv\", index=False)\n",
    "\n",
    "# ===== metadata (provenance) =====\n",
    "meta = {\n",
    "  \"dataset_name\": IN_CSV.name,\n",
    "  \"generated_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "  \"rows\": int(n_rows),\n",
    "  \"columns\": int(n_cols),\n",
    "  \"keys\": [\"lat\",\"lon\",\"year\"],\n",
    "  \"panel\": {\"cells\": int(uniq_cells), \"years\": int(uniq_years), \"balanced\": True},\n",
    "  \"region\": {\n",
    "      \"bbox\": {\n",
    "          \"lat_min\": float(latc.min()), \"lat_max\": float(latc.max()),\n",
    "          \"lon_min\": float(lonc.min()), \"lon_max\": float(lonc.max())\n",
    "      },\n",
    "      \"note\": \"Northern Italy (Po Valley); 42-core cells via geographic box  4-crop overlap; 1981 excluded.\"\n",
    "  },\n",
    "  \"grid\": {\n",
    "    \"resolution_deg\": 0.5,\n",
    "    \"center_offset_deg\": 0.25,\n",
    "    \"lon_convention\": \"0360\",\n",
    "    \"source\": \"GDHY maize grid\"\n",
    "  },\n",
    "  \"period\": {\"start_year\": 1982, \"end_year\": 2016, \"season\": \"MJJAS (MaySeptember)\"},\n",
    "  \"variables\": {\n",
    "    \"yield_maize\": {\"unit\": \"t/ha\", \"type\": \"state\", \"aggregation\": \"annual\"},\n",
    "    \"temperature\": {\"unit\": \"C\", \"type\": \"state\", \"aggregation\": \"MJJAS mean\"},\n",
    "    \"precipitation\": {\"unit\": \"mm\", \"type\": \"flux\", \"aggregation\": \"MJJAS total (daily mean  days per month)\"},\n",
    "    \"soil_water\": {\"unit\": \"m/m\", \"type\": \"state\", \"aggregation\": \"MJJAS mean\"},\n",
    "    \"solar_radiation\": {\"unit\": \"J/m\", \"type\": \"flux\", \"aggregation\": \"MJJAS total (daily mean  days per month)\"},\n",
    "    \"potential_evaporation\": {\"unit\": \"mm\", \"type\": \"flux\", \"aggregation\": \"MJJAS total (daily mean  days per month; sign flipped to +)\"},\n",
    "    \"monthly_columns\": {\n",
    "        \"months\": month_suffixes,\n",
    "        \"temperature_*\": {\"unit\": \"C\", \"aggregation\": \"monthly mean\"},\n",
    "        \"precipitation_*\": {\"unit\": \"mm\", \"aggregation\": \"monthly total (daily mean  days per month)\"},\n",
    "        \"soil_water_*\": {\"unit\": \"m/m\", \"aggregation\": \"monthly mean\"},\n",
    "        \"solar_radiation_*\": {\"unit\": \"J/m\", \"aggregation\": \"monthly total (daily mean  days per month)\"},\n",
    "        \"potential_evaporation_*\": {\"unit\": \"mm\", \"aggregation\": \"monthly total (daily mean  days per month; sign flipped to +)\"}\n",
    "    }\n",
    "  },\n",
    "  \"processing\": {\n",
    "    \"era5_monthly_product\": \"Monthly averaged (daily means for fluxes)\",\n",
    "    \"spatial_downsampling\": \"Block average 0.1  0.5 to GDHY centers (0.25 offset)\",\n",
    "    \"temporal_aggregation\": \"MJJAS per rules above\",\n",
    "    \"subset\": \"Exact 42 cells selected by value (nearest to centers); global crop to bbox before processing\",\n",
    "    \"exclusions\": \"Year 1981 excluded; cells with incomplete years removed\"\n",
    "  },\n",
    "  \"inputs\": {\n",
    "    \"seasonal_base_csv\": str((DERIVED / \"maize_ITnorth_core42_1982_2016_allstressors.csv\").resolve()),\n",
    "    \"enriched_with_monthly_csv\": str(IN_CSV.resolve()),\n",
    "    \"mask_core_42_csv\": str(MASK_CSV.resolve())\n",
    "  }\n",
    "}\n",
    "with open(OUT_DIR / \"meta.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "# ===== optional quick plots =====\n",
    "if PLOT:\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # regional means over time\n",
    "        reg = (df.groupby(\"year\")[[\"yield_maize\",\"temperature\",\"precipitation\",\"potential_evaporation\"]]\n",
    "                 .mean().reset_index())\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(reg[\"year\"], reg[\"yield_maize\"]); plt.xlabel(\"Year\"); plt.ylabel(\"Yield (t/ha)\")\n",
    "        plt.title(\"Regional mean yield (42 cells)\"); plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / \"fig_regional_yield_timeseries.png\", dpi=120)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(reg[\"year\"], reg[\"temperature\"]); plt.xlabel(\"Year\"); plt.ylabel(\"Temp (C)\")\n",
    "        plt.title(\"Regional mean temperature (MJJAS)\"); plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / \"fig_regional_temp_timeseries.png\", dpi=120)\n",
    "\n",
    "        # first scatter (for a sense-check)\n",
    "        plt.figure()\n",
    "        plt.scatter(df[\"temperature\"], df[\"yield_maize\"], s=6)\n",
    "        plt.xlabel(\"Temperature (C, MJJAS)\"); plt.ylabel(\"Yield (t/ha)\")\n",
    "        plt.title(\"Yield vs Temperature (all cells-years)\"); plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / \"fig_scatter_yield_vs_temp.png\", dpi=120)\n",
    "\n",
    "        print(\"Saved quick figures in:\", OUT_DIR)\n",
    "    except Exception as e:\n",
    "        print(\"Plotting skipped:\", e)\n",
    "\n",
    "# ===== final message =====\n",
    "print(\"\\nQA complete.\")\n",
    "print(\" Wrote:\", OUT_DIR / \"qa_summary.csv\")\n",
    "print(\" Wrote:\", OUT_DIR / \"qa_cell_year_counts.csv\")\n",
    "print(\" Wrote:\", OUT_DIR / \"meta.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 31-initial-modelling.ipynb/31-initial-modelling.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbfdd5f-3121-494c-9659-1dc09ed95b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 7.1  USER INPUTS ===\n",
    "from pathlib import Path\n",
    "\n",
    "# Folder with your final CSVs\n",
    "DERIVED = Path(r\"..\\italy_core_data\\derived\")\n",
    "\n",
    "# >>> Pick ONE of your datasets to prepare for modeling <<<\n",
    "# Examples:\n",
    "#INPUT_CSV = DERIVED / \"maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv\"\n",
    "#INPUT_CSV = DERIVED / \"rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv\"\n",
    "#INPUT_CSV = DERIVED / \"soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv\"\n",
    "#INPUT_CSV = DERIVED / \"wheat_spring_ITnorth_core42_1982_2016_allstressors_with_monthly.csv\"\n",
    "#INPUT_CSV = DERIVED / \"wheat_winter_ITnorth_core42_1982_2016_allstressors_with_monthly.csv\"\n",
    "#INPUT_CSV = DERIVED / \"wheat_both_ITnorth_core42_1982_2016_allstressors_with_monthly_long.csv\"\n",
    "\n",
    "#  choose one:\n",
    "INPUT_CSV = DERIVED / \"maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv\"\n",
    "\n",
    "# Where to save the modeling table\n",
    "OUT_DIR = DERIVED / \"modeling\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11fe49-6c92-4d0e-aa49-6729f574aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# -------- load --------\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# detect yield column dynamically (works for maize/rice/soy + wheat_* + stacked long)\n",
    "yield_cols = [c for c in df.columns if c.startswith(\"yield_\")]\n",
    "assert len(yield_cols) == 1, f\"Expected one yield_* column, found {yield_cols}\"\n",
    "YIELD_COL = yield_cols[0]\n",
    "\n",
    "# keys present\n",
    "assert {\"lat\",\"lon\",\"year\"}.issubset(df.columns)\n",
    "has_type = \"wheat_type\" in df.columns  # stacked long wheat\n",
    "\n",
    "# -------- add cell_id (stable across runs) --------\n",
    "# sort by lat,lon so IDs are reproducible\n",
    "cells = (df[[\"lat\",\"lon\"]]\n",
    "         .drop_duplicates()\n",
    "         .sort_values([\"lat\",\"lon\"])\n",
    "         .reset_index(drop=True))\n",
    "cells[\"cell_id\"] = np.arange(len(cells))\n",
    "\n",
    "df = df.merge(cells, on=[\"lat\",\"lon\"], how=\"left\")\n",
    "\n",
    "# -------- outcomes: log yield + two-way FE demeaned --------\n",
    "# sanity: yields should be > 0\n",
    "min_y = df[YIELD_COL].min()\n",
    "if not (min_y > 0):\n",
    "    raise ValueError(f\"{YIELD_COL} has non-positive values (min={min_y}). Cannot log-transform safely.\")\n",
    "\n",
    "df[\"log_yield\"] = np.log(df[YIELD_COL].values)\n",
    "\n",
    "def twoway_demean(s, ids, years):\n",
    "    mu = s.mean()\n",
    "    a = s.groupby(ids).transform(\"mean\")\n",
    "    t = s.groupby(years).transform(\"mean\")\n",
    "    return s - a - t + mu\n",
    "\n",
    "df[\"yield_fe\"]      = twoway_demean(df[YIELD_COL], df[\"cell_id\"], df[\"year\"])\n",
    "df[\"log_yield_fe\"]  = twoway_demean(df[\"log_yield\"], df[\"cell_id\"], df[\"year\"])\n",
    "\n",
    "# -------- seasonal predictors (keep raw + standardized) --------\n",
    "seasonal_vars = [\"temperature\",\"precipitation\",\"soil_water\",\"solar_radiation\",\"potential_evaporation\"]\n",
    "missing = [v for v in seasonal_vars if v not in df.columns]\n",
    "assert not missing, f\"Missing seasonal columns: {missing}\"\n",
    "\n",
    "# aridity index (mm/mm)  protect against zeros just in case\n",
    "pev = df[\"potential_evaporation\"].replace({0: np.nan})\n",
    "df[\"aridity_index\"] = df[\"precipitation\"] / pev\n",
    "\n",
    "# standardize (z-scores)  across the whole chosen dataset\n",
    "params = {\"standardization\": {}}\n",
    "for v in seasonal_vars + [\"aridity_index\"]:\n",
    "    s = df[v].astype(float)\n",
    "    mu = float(s.mean())\n",
    "    sd = float(s.std(ddof=0))\n",
    "    # avoid division by 0 if constant\n",
    "    if sd == 0 or np.isnan(sd):\n",
    "        z = pd.Series(np.zeros(len(s)), index=s.index, dtype=float)\n",
    "    else:\n",
    "        z = (s - mu) / sd\n",
    "    df[f\"{v}_z\"] = z\n",
    "    params[\"standardization\"][v] = {\"mean\": mu, \"std\": sd}\n",
    "\n",
    "# also store centered (mean-removed) versions for interpretable interactions\n",
    "for v in seasonal_vars:\n",
    "    df[f\"{v}_c\"] = df[v] - params[\"standardization\"][v][\"mean\"]\n",
    "\n",
    "# -------- light integrity checks --------\n",
    "n_rows = len(df)\n",
    "dups = df.duplicated([\"lat\",\"lon\",\"year\"] + ([\"wheat_type\"] if has_type else [])).sum()\n",
    "nans = int(df[[\"log_yield\",\"yield_fe\",\"log_yield_fe\"] + seasonal_vars].isna().sum().sum())\n",
    "\n",
    "print(f\"Rows: {n_rows}\")\n",
    "print(f\"Duplicate keys: {dups} (expect 0)\")\n",
    "print(f\"NaNs in core fields: {nans} (expect 0)\")\n",
    "print(\"Yield col:\", YIELD_COL)\n",
    "\n",
    "# -------- save --------\n",
    "stem = INPUT_CSV.stem.replace(\"_allstressors_with_monthly\",\"\").replace(\"_allstressors\",\"\")\n",
    "out_csv  = OUT_DIR / f\"{stem}_modeling.csv\"\n",
    "out_json = OUT_DIR / f\"{stem}_modeling_params.json\"\n",
    "\n",
    "df.to_csv(out_csv, index=False)\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"input_file\": str(INPUT_CSV),\n",
    "        \"yield_col\": YIELD_COL,\n",
    "        \"n_rows\": n_rows,\n",
    "        \"n_cells\": int(cells.shape[0]),\n",
    "        \"years\": {\"min\": int(df[\"year\"].min()), \"max\": int(df[\"year\"].max())},\n",
    "        \"has_wheat_type\": bool(has_type),\n",
    "        **params\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" \", out_csv)\n",
    "print(\" \", out_json)\n",
    "\n",
    "# preview a few columns youll model with\n",
    "show_cols = [\"lat\",\"lon\",\"cell_id\",\"year\"] + ([\"wheat_type\"] if has_type else []) + \\\n",
    "            [YIELD_COL,\"log_yield\",\"yield_fe\",\"log_yield_fe\"] + \\\n",
    "            [\"temperature_z\",\"precipitation_z\",\"soil_water_z\",\"solar_radiation_z\",\"potential_evaporation_z\",\"aridity_index_z\"]\n",
    "print(\"\\nPreview:\")\n",
    "print(df[show_cols].head(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b71f4-22d1-45fe-9108-bee9f09a3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 7.2  Baseline seasonal multiple regression (clustered by cell) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "DERIVED = Path(r\"..\\italy_core_data\\derived\")\n",
    "MODEL_DIR = DERIVED / \"modeling\"\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Pick the modeling CSV you just created in 7.1 (maize here, but any dataset works)\n",
    "model_csv = MODEL_DIR / \"maize_ITnorth_core42_1982_2016_modeling.csv\"\n",
    "df = pd.read_csv(model_csv)\n",
    "\n",
    "# Columns\n",
    "y = \"log_yield_fe\"   # two-way demeaned log yield\n",
    "X_base = [\"temperature_z\",\"precipitation_z\",\"soil_water_z\",\"solar_radiation_z\",\"potential_evaporation_z\"]\n",
    "\n",
    "# Add a simple interaction (centered vars so main effects are interpretable at avg climate)\n",
    "df[\"temp_x_precip\"] = (df[\"temperature_c\"] * df[\"precipitation_c\"]).astype(float)\n",
    "X = X_base + [\"temp_x_precip\"]\n",
    "\n",
    "# Build design matrix\n",
    "Xmat = sm.add_constant(df[X].astype(float))\n",
    "yvec = df[y].astype(float)\n",
    "\n",
    "# Fit OLS with cluster-robust SE by cell\n",
    "model = sm.OLS(yvec, Xmat, missing=\"drop\")\n",
    "res = model.fit(cov_type=\"cluster\", cov_kwds={\"groups\": df[\"cell_id\"]})\n",
    "\n",
    "# Print compact summary\n",
    "print(res.summary())\n",
    "\n",
    "# Tidy coefficient table with clustered SE and 95% CI\n",
    "coefs = (pd.DataFrame({\n",
    "    \"term\": [\"const\"] + X,\n",
    "    \"estimate\": res.params.values,\n",
    "    \"std_error\": res.bse.values,\n",
    "    \"t_value\": res.tvalues.values,\n",
    "    \"p_value\": res.pvalues.values,\n",
    "})\n",
    ".assign(ci_lo=lambda d: d[\"estimate\"] - 1.96*d[\"std_error\"])\n",
    ".assign(ci_hi=lambda d: d[\"estimate\"] + 1.96*d[\"std_error\"])\n",
    ")\n",
    "\n",
    "out_coef = MODEL_DIR / \"maize_baseline_coefficients.csv\"\n",
    "coefs.to_csv(out_coef, index=False)\n",
    "\n",
    "# Simple fit diagnostics\n",
    "print(\"\\nN =\", int(res.nobs), \"| R-squared (within-FE spec proxy):\", f\"{res.rsquared:.3f}\")\n",
    "print(\"Saved coef table \", out_coef)\n",
    "\n",
    "# Optional quick sanity: group means of yield_fe should be ~0\n",
    "cell_means = df.groupby(\"cell_id\")[\"yield_fe\"].mean().abs().max()\n",
    "year_means = df.groupby(\"year\")[\"yield_fe\"].mean().abs().max()\n",
    "print(f\"Max |mean(yield_fe)| by cell: {cell_means:.3e} ; by year: {year_means:.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0a76f-77d6-4d8c-ba5c-4198dada475d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 32-Multi-Crop-EDA-Italy.ipynb/32-Multi-Crop-EDA-Italy.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Initial Inspection of the Maize Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Load the Maize Dataset ---\n",
    "file_path = '../personal/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "\n",
    "    print(\"--- Maize Dataset Inspection ---\")\n",
    "    \n",
    "    # 1. Shape: How many rows and columns?\n",
    "    print(f\"\\nShape: {df_maize.shape}\")\n",
    "\n",
    "    # 2. Columns: What are all the variable names?\n",
    "    print(\"\\nColumns:\")\n",
    "    print(df_maize.columns.tolist())\n",
    "\n",
    "    # 3. Head: Show me the first few rows.\n",
    "    print(\"\\nData Preview (first 5 rows):\")\n",
    "    print(df_maize.head().to_string())\n",
    "\n",
    "    # 4. Missing Values: Are there any NaNs?\n",
    "    print(\"\\nMissing Value Check (sum of NaNs per column):\")\n",
    "    print(df_maize.isnull().sum())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "    print(\"Please ensure the 'personal' folder is in the root of the 'climarisc' directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Discover Growing Seasons for All Other Crops\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- List of all the dataset files ---\n",
    "file_paths = [\n",
    "    '../personal/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv',\n",
    "    '../personal/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv',\n",
    "    '../personal/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv',\n",
    "    '../personal/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "]\n",
    "\n",
    "print(\"--- Discovering Growing Seasons from Column Names ---\")\n",
    "\n",
    "for path in file_paths:\n",
    "    try:\n",
    "        # We use nrows=0 to read only the header row, which is very fast\n",
    "        df_header = pd.read_csv(path, nrows=0)\n",
    "        \n",
    "        # Extract the crop name from the filename for a clean title\n",
    "        crop_name = os.path.basename(path).replace('_ITnorth_core41_1982_2016_allstressors_with_monthly.csv', '')\n",
    "        \n",
    "        print(f\"\\n--- {crop_name.upper()} ---\")\n",
    "        print(\"Columns:\")\n",
    "        print(df_header.columns.tolist())\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nERROR: File not found: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR processing {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Generate Correlation Heatmaps for All Crops\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import detrend\n",
    "\n",
    "# --- List of all the dataset files and their yield column names ---\n",
    "file_info = {\n",
    "    'maize': {'path': '../personal/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv', 'yield_col': 'yield_maize'},\n",
    "    'rice': {'path': '../personal/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv', 'yield_col': 'yield_rice'},\n",
    "    'soybean': {'path': '../personal/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv', 'yield_col': 'yield_soybean'},\n",
    "    'wheat_spring': {'path': '../personal/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv', 'yield_col': 'yield_wheat_spring'},\n",
    "    'wheat_winter': {'path': '../personal/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv', 'yield_col': 'yield_wheat_winter'}\n",
    "}\n",
    "\n",
    "print(\"--- Generating Correlation Heatmaps for Each Crop ---\")\n",
    "\n",
    "for crop_name, info in file_info.items():\n",
    "    print(f\"\\n--- Processing: {crop_name.upper()} ---\")\n",
    "    \n",
    "    try:\n",
    "        # --- 1. Load the data ---\n",
    "        df = pd.read_csv(info['path'])\n",
    "        \n",
    "        # --- 2. Calculate Yield Anomaly (De-trending) ---\n",
    "        # We must de-trend the data within each grid cell group\n",
    "        df['yield_anomaly'] = df.groupby(['lat', 'lon'])[info['yield_col']].transform(lambda x: detrend(x))\n",
    "        \n",
    "        # --- 3. Select Columns for Correlation ---\n",
    "        # We only want the yield anomaly and the summary stressors\n",
    "        cols_to_correlate = [\n",
    "            'yield_anomaly', 'temperature', 'precipitation', 'soil_water',\n",
    "            'solar_radiation', 'potential_evaporation'\n",
    "        ]\n",
    "        df_corr = df[cols_to_correlate]\n",
    "        \n",
    "        # --- 4. Calculate and Plot the Heatmap ---\n",
    "        corr_matrix = df_corr.corr()\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(corr_matrix[['yield_anomaly']], annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
    "        plt.title(f'Correlation of Stressors with Yield Anomaly\\n({crop_name})')\n",
    "        plt.show()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found: {info['path']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {crop_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Monthly Temperature Correlation Analysis\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import detrend\n",
    "\n",
    "print(\"--- Generating Monthly Temperature Correlation Heatmaps ---\")\n",
    "\n",
    "for crop_name, info in file_info.items():\n",
    "    print(f\"\\n--- Processing: {crop_name.upper()} ---\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(info['path'])\n",
    "        \n",
    "        # Calculate Yield Anomaly\n",
    "        df['yield_anomaly'] = df.groupby(['lat', 'lon'])[info['yield_col']].transform(lambda x: detrend(x))\n",
    "        \n",
    "        # Find all columns that are monthly temperatures\n",
    "        monthly_temp_cols = [col for col in df.columns if 'temperature_' in col]\n",
    "        \n",
    "        # Select columns for correlation\n",
    "        cols_to_correlate = ['yield_anomaly'] + monthly_temp_cols\n",
    "        df_corr = df[cols_to_correlate]\n",
    "        \n",
    "        # Calculate and Plot the Heatmap\n",
    "        corr_matrix = df_corr.corr()\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(corr_matrix.loc[monthly_temp_cols, ['yield_anomaly']], annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
    "        plt.title(f'Correlation of Monthly Temperatures with Yield Anomaly\\n({crop_name})')\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {crop_name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 33-Maize-Heat-Stress-Model.ipynb/33-Maize-Heat-Stress-Model.ipynb
````
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ee2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Data Loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import detrend\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# --- Load the Maize Dataset ---\n",
    "file_path = '..\\personal\\maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    print(\"--- Maize Dataset Loaded Successfully ---\")\n",
    "    \n",
    "    # 1. Shape: How many rows and columns?\n",
    "    print(f\"\\nShape: {df.shape}\")\n",
    "\n",
    "    # 2. Columns: What are all the variable names?\n",
    "    print(\"\\nColumns:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    # 3. Head: Show me the first few rows.\n",
    "    print(\"\\nData Preview (first 5 rows):\")\n",
    "    print(df.head().to_string())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd67ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Calculate Yield Anomaly (De-trending)\n",
    "import pandas as pd\n",
    "from scipy.signal import detrend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- De-trend the Yield Data ---\n",
    "# We group the data by each unique grid cell and then apply the detrend function\n",
    "# to the 'yield_maize' column for each group.\n",
    "# 'transform' ensures the output is the same shape as the input, so we can add it as a new column.\n",
    "print(\"Calculating yield anomaly for each of the 42 grid cells...\")\n",
    "df['yield_anomaly'] = df.groupby(['lat', 'lon'])['yield_maize'].transform(lambda x: detrend(x))\n",
    "print(\"'yield_anomaly' column created successfully.\")\n",
    "\n",
    "# --- Verification Step ---\n",
    "# Let's visually check the result for a single, representative grid cell.\n",
    "# We'll pick one of the core cells from the middle of our region.\n",
    "example_cell = df[(df['lat'] == 45.25) & (df['lon'] == 9.75)]\n",
    "\n",
    "print(\"\\n--- Verification for a single grid cell (lat=45.25, lon=9.75) ---\")\n",
    "\n",
    "# Create a figure with two subplots, one above the other\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Plot 1: Raw Yield Trend\n",
    "ax1.plot(example_cell['year'], example_cell['yield_maize'], marker='o', linestyle='-', label='Raw Yield')\n",
    "ax1.set_title('1. Raw Yield Data (Technology Trend + Weather Signal)')\n",
    "ax1.set_ylabel('Yield (t/ha)')\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: De-trended Yield Anomaly\n",
    "ax2.plot(example_cell['year'], example_cell['yield_anomaly'], marker='o', linestyle='-', color='green', label='Yield Anomaly')\n",
    "ax2.axhline(0, color='black', linestyle='--') # Add a zero-line for reference\n",
    "ax2.set_title('2. De-trended Yield Anomaly (Weather Signal Only)')\n",
    "ax2.set_ylabel('Yield Anomaly (t/ha)')\n",
    "ax2.set_xlabel('Year')\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Final Preview ---\n",
    "print(\"\\nPreview of the DataFrame with the new 'yield_anomaly' column:\")\n",
    "print(df[['year', 'lat', 'lon', 'yield_maize', 'yield_anomaly']].head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c793b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Fit the Baseline Fixed Effects Model\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# --- Define and Fit the Model ---\n",
    "# We are building a Fixed Effects panel model. This is the standard for this type of analysis.\n",
    "# The formula reads: \"Explain the yield_anomaly using the July temperature, while controlling\n",
    "# for the fixed effects of each unique grid cell (EntityEffects).\"\n",
    "# We create a unique identifier for each grid cell to use in the model.\n",
    "df['cell_id'] = df.groupby(['lat', 'lon']).ngroup()\n",
    "\n",
    "print(\"--- Fitting Baseline Model: Yield Anomaly ~ July Temperature ---\")\n",
    "\n",
    "# Fit the model using the statsmodels library\n",
    "# C(cell_id) tells the model to treat each cell_id as a separate categorical variable\n",
    "model = smf.ols('yield_anomaly ~ temperature_Jul + C(cell_id)', data=df).fit() # C(cell_id) for fixed effects, which effectively controls for each grid cell.\n",
    "\n",
    "# --- Print the Model Summary ---\n",
    "print(\"\\n--- Model Results Summary ---\")\n",
    "print(model.summary())\n",
    "\n",
    "# We can also extract the key coefficient for our presentation\n",
    "temp_coefficient = model.params['temperature_Jul']\n",
    "p_value = model.pvalues['temperature_Jul']\n",
    "\n",
    "print(f\"\\n--- Key Finding ---\")\n",
    "print(f\"The coefficient for July Temperature is: {temp_coefficient:.4f}\")\n",
    "print(f\"The p-value for this coefficient is: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"\\nConclusion: The relationship is statistically significant.\")\n",
    "    print(f\"This means that for every 1C increase in average July temperature, we expect maize yield to decrease by approximately {-temp_coefficient:.2f} tonnes per hectare.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: The relationship is not statistically significant at the p < 0.05 level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78296cca",
   "metadata": {},
   "source": [
    "My Interpretation of the Model Results\n",
    "\n",
    "The \"OLS Regression Results\" table is the core output. Here's how to read it:\n",
    "\n",
    "The Overall Model:\n",
    "\n",
    "    R-squared: 0.185: This tells us that our simple model (July temperature + the unique characteristics of each grid cell) can explain about 18.5% of the variation in the yield_anomaly. This is a very respectable number for a simple baseline model in a noisy, real-world system like agriculture. It proves our model has meaningful explanatory power.\n",
    "\n",
    "    Prob (F-statistic): 5.80e-40: This is a very small number (essentially zero). It tells us that the model as a whole is highly statistically significant. It's extremely unlikely that our results are due to random chance.\n",
    "\n",
    "The Most Important Part (The Vulnerability Coefficient):\n",
    "The table is truncated, but my script also printed out the key finding separately.\n",
    "\n",
    "    The p-value for this coefficient is: 0.0000: This is the most important number. A p-value less than 0.05 is the standard for \"statistical significance.\" Our value is far below this threshold. This means we can be very confident that there is a real, non-random relationship between July temperature and maize yield anomaly.\n",
    "\n",
    "    Conclusion: The relationship is statistically significant. My script correctly interpreted this.\n",
    "\n",
    "The \"So What?\" (The Quantitative Impact):\n",
    "\n",
    "    \"for every 1C increase... yield to decrease by approximately 0.32 tonnes per hectare.\" This is the final, powerful conclusion. The model has quantified the risk. It has taken the messy cloud of data points and distilled it into a single, easy-to-understand number. This is the core finding you can present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Plot the Baseline Vulnerability Curve\n",
    "\n",
    "# --- Create the Scatter Plot ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Use seaborn's regplot to create the scatter plot and the regression line\n",
    "# This plot visually represents the model we just built.\n",
    "sns.regplot(\n",
    "    data=df,\n",
    "    x='temperature_Jul',\n",
    "    y='yield_anomaly',\n",
    "    scatter_kws={'alpha': 0.3, 's': 20}, # Make points slightly transparent and smaller\n",
    "    line_kws={'color': 'red', 'linewidth': 2}\n",
    ")\n",
    "\n",
    "# --- Formatting ---\n",
    "plt.title('Vulnerability of Maize to July Temperature in Northern Italy', fontsize=16)\n",
    "plt.xlabel('Average July Temperature (C)', fontsize=12)\n",
    "plt.ylabel('Yield Anomaly (tonnes per hectare)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1) # Add a zero-line\n",
    "\n",
    "# --- Add the Key Finding to the Plot ---\n",
    "# We'll add a text box with the main result from our model.\n",
    "results_text = (\n",
    "    f\"Model Result:\\n\"\n",
    "    f\"A 1C increase in July temperature\\n\"\n",
    "    f\"is associated with a yield loss of\\n\"\n",
    "    f\"~{-temp_coefficient:.2f} t/ha (p < 0.001)\"\n",
    ")\n",
    "plt.text(0.05, 0.95, results_text, transform=plt.gca().transAxes,\n",
    "         fontsize=12, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da732d4",
   "metadata": {},
   "source": [
    "## Baseline Model 2: Yield Anomaly vs. Full Growing Season Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc04ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Fit the Full Growing Season Temperature Model\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "print(\"--- Fitting Baseline Model: Yield Anomaly ~ Full Growing Season Temperature ---\")\n",
    "\n",
    "# The formula now uses the 'temperature' column, which is the seasonal average.\n",
    "model_season = smf.ols('yield_anomaly ~ temperature + C(cell_id)', data=df).fit()\n",
    "\n",
    "# --- Print the Model Summary ---\n",
    "print(\"\\n--- Model Results Summary (Full Season) ---\")\n",
    "print(model_season.summary())\n",
    "\n",
    "# --- Extract the Key Finding ---\n",
    "temp_coefficient_season = model_season.params['temperature']\n",
    "p_value_season = model_season.pvalues['temperature']\n",
    "\n",
    "print(f\"\\n--- Key Finding (Full Season) ---\")\n",
    "print(f\"The coefficient for Full Season Temperature is: {temp_coefficient_season:.4f}\")\n",
    "print(f\"The p-value for this coefficient is: {p_value_season:.4f}\")\n",
    "\n",
    "if p_value_season < 0.05:\n",
    "    print(\"\\nConclusion: The relationship is statistically significant.\")\n",
    "    print(f\"This means that for every 1C increase in average growing season temperature, we expect maize yield to decrease by approximately {-temp_coefficient_season:.2f} tonnes per hectare.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: The relationship is not statistically significant at the p < 0.05 level.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Plot the Full Growing Season Vulnerability Curve\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Use seaborn's regplot, but this time with the 'temperature' column on the x-axis\n",
    "sns.regplot(\n",
    "    data=df,\n",
    "    x='temperature',\n",
    "    y='yield_anomaly',\n",
    "    scatter_kws={'alpha': 0.3, 's': 20},\n",
    "    line_kws={'color': 'blue', 'linewidth': 2} # Using blue to distinguish from the July plot\n",
    ")\n",
    "\n",
    "# --- Formatting ---\n",
    "plt.title('Vulnerability of Maize to Full Growing Season Temperature in Northern Italy', fontsize=16)\n",
    "plt.xlabel('Average Growing Season Temperature (C)', fontsize=12)\n",
    "plt.ylabel('Yield Anomaly (tonnes per hectare)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "# --- Add the Key Finding to the Plot ---\n",
    "results_text_season = (\n",
    "    f\"Model Result:\\n\"\n",
    "    f\"A 1C increase in seasonal temperature\\n\"\n",
    "    f\"is associated with a yield loss of\\n\"\n",
    "    f\"~{-temp_coefficient_season:.2f} t/ha (p = {p_value_season:.3f})\"\n",
    ")\n",
    "plt.text(0.05, 0.95, results_text_season, transform=plt.gca().transAxes,\n",
    "         fontsize=12, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "dd65c0dc",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2340b",
   "metadata": {},
   "source": [
    "    The \"July Only\" model is a much better model. Its R-squared value (0.185) is nearly four times higher than the R-squared of the full-season model (0.048). This means that the temperature in the single month of July explains almost four times more of the year-to-year variation in maize yield than the average temperature of the entire season.\n",
    "\n",
    "    The impact is stronger in July. The coefficient for the July model is larger (-0.32 vs. -0.26), indicating that a 1C increase specifically in July has a more damaging effect on the final yield than a 1C increase spread across the whole season.\n",
    "\n",
    "This provides powerful, quantitative evidence for our hypothesis: it is the extreme heat during the critical month of July, not just a generally warm season, that poses the biggest threat to maize in this region. This is an excellent finding for your presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9163d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Fit the Non-Linear (Quadratic) Model\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "print(\"--- Fitting Non-Linear Model: Yield Anomaly ~ Temp_Jul + Temp_Jul^2 ---\")\n",
    "\n",
    "# The I(temperature_Jul**2) tells the model to use the square of the temperature as a predictor.\n",
    "# This allows us to fit a curved (parabolic) line.\n",
    "formula_quad = 'yield_anomaly ~ temperature_Jul + I(temperature_Jul**2) + C(cell_id)'\n",
    "model_quad = smf.ols(formula_quad, data=df).fit()\n",
    "\n",
    "# --- Print the Model Summary ---\n",
    "print(\"\\n--- Model Results Summary (Quadratic) ---\")\n",
    "print(model_quad.summary())\n",
    "\n",
    "# --- Check if the squared term is significant ---\n",
    "p_value_quad_term = model_quad.pvalues['I(temperature_Jul ** 2)']\n",
    "print(f\"\\n--- Key Finding (Non-Linearity) ---\")\n",
    "print(f\"The p-value for the squared temperature term is: {p_value_quad_term:.4f}\")\n",
    "\n",
    "if p_value_quad_term < 0.05:\n",
    "    print(\"\\nConclusion: The squared term is statistically significant. This provides strong evidence that the relationship is non-linear (curved).\")\n",
    "else:\n",
    "    print(\"\\nConclusion: The squared term is not statistically significant. A simple straight line is a sufficient model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c31e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Plot the Non-Linear Vulnerability Curve\n",
    "\n",
    "# --- Create a scatter plot of the raw data ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='temperature_Jul', y='yield_anomaly', alpha=0.3)\n",
    "\n",
    "# --- Generate and plot the curved regression line ---\n",
    "# We create a smooth range of temperature values to predict over\n",
    "x_range = np.linspace(df['temperature_Jul'].min(), df['temperature_Jul'].max(), 100)\n",
    "# We need to create a dataframe with the x_range and the average cell_id effect to predict correctly\n",
    "# For visualization, we can approximate this by using the intercept\n",
    "# A more robust method would be to plot predictions for each cell, but this is a good overview.\n",
    "y_preds = model_quad.predict(pd.DataFrame({'temperature_Jul': x_range, 'cell_id': 0}))\n",
    "\n",
    "# Plot the curved line\n",
    "plt.plot(x_range, y_preds, color='purple', linewidth=2, label='Quadratic Fit')\n",
    "\n",
    "# --- Formatting ---\n",
    "plt.title('Non-Linear Vulnerability of Maize to July Temperature', fontsize=16)\n",
    "plt.xlabel('Average July Temperature (C)', fontsize=12)\n",
    "plt.ylabel('Yield Anomaly (tonnes per hectare)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ff321",
   "metadata": {},
   "source": [
    "The Model Has Improved:\n",
    "\n",
    "    Look at the R-squared value: it is now 0.189. Our previous linear model had an R-squared of 0.185. This is not a huge jump, but it is a clear improvement. The curved line explains slightly more of the variation in the yield anomaly than the straight line did.\n",
    "\n",
    "    Look at the AIC (Akaike Information Criterion): it is now 3826. Our previous model had an AIC of 3831. In statistical modeling, a lower AIC is better. This confirms that even after penalizing the model for adding an extra term, the quadratic model is still considered a better fit to the data.\n",
    "\n",
    "The Curve is Statistically Significant:\n",
    "\n",
    "    This is the most important finding. The p-value for the squared temperature term is 0.0096.\n",
    "\n",
    "    Since this value is less than 0.05, it means we can be confident that the \"curviness\" of the relationship is a real feature of the data, not just a random fluke. My script correctly identified this.\n",
    "\n",
    "The Plot Tells the Story Perfectly:\n",
    "\n",
    "    The purple line is our new, more accurate vulnerability curve. It shows that the negative impact of heat accelerates as the temperature rises.\n",
    "\n",
    "    For example, the difference in yield loss between 18C and 19C is much smaller than the difference between 24C and 25C. This is a classic non-linear damage effect, and our model has successfully captured it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517e0c7",
   "metadata": {},
   "source": [
    "# STOPPPPPPPPPPPPPPPPPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Create the Scatterplot Matrix for Presentation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"--- Generating Scatterplot Matrix for Key Variables ---\")\n",
    "\n",
    "# --- 1. Select the columns we want to visualize ---\n",
    "# We'll choose our dependent variable and the most important summary stressors.\n",
    "columns_for_matrix = [\n",
    "    'yield_anomaly',\n",
    "    'temperature', # Full season average temp\n",
    "    'temperature_Jul', # July-specific temp\n",
    "    'precipitation', # Full season total precip\n",
    "    'soil_water' # Full season average soil water\n",
    "]\n",
    "\n",
    "# Create a subset of our main DataFrame\n",
    "df_subset = df[columns_for_matrix]\n",
    "\n",
    "# --- 2. Create the Scatterplot Matrix using seaborn.pairplot ---\n",
    "# This powerful function creates a grid of plots.\n",
    "# The diagonal shows the distribution (histogram) of each variable.\n",
    "# The off-diagonals show the scatter plot between each pair of variables.\n",
    "g = sns.pairplot(\n",
    "    df_subset,\n",
    "    kind='scatter', # Use scatter plots for the relationships\n",
    "    diag_kind='kde', # Use a Kernel Density Estimate for the distributions\n",
    "    plot_kws={'alpha': 0.2, 's': 10}, # Make scatter points transparent and small\n",
    "    diag_kws={'fill': True}\n",
    ")\n",
    "\n",
    "# Add regression lines to the scatter plots for clarity\n",
    "g.map_lower(sns.regplot, scatter_kws={'alpha': 0.0}) # Adds line to lower triangle\n",
    "g.map_upper(sns.regplot, scatter_kws={'alpha': 0.0}) # Adds line to upper triangle\n",
    "\n",
    "\n",
    "# --- Formatting ---\n",
    "g.fig.suptitle('Scatterplot Matrix of Key Variables for Maize in Northern Italy', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 34-Feature-Engineering.ipynb/34-Feature-Engineering.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83992928",
   "metadata": {},
   "source": [
    "# Phase 1: Data Preparation & Feature Engineering\n",
    "\n",
    "**Goal:** To load our five pre-processed crop datasets and systematically engineer all the necessary variables for our analysis, as outlined in the project plan. This includes calculating yield anomalies and creating both summary and extreme stressor variables. The final, enriched datasets will be saved to a new directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdcb090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: The Corrected and Simplified Feature Engineering Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import detrend\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "file_info = {\n",
    "    'maize': {'path': '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv', 'yield_col': 'yield_maize'},\n",
    "    'rice': {'path': '../data-cherry-pick/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv', 'yield_col': 'yield_rice'},\n",
    "    'soybean': {'path': '../data-cherry-pick/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv', 'yield_col': 'yield_soybean'},\n",
    "    'wheat_spring': {'path': '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv', 'yield_col': 'yield_wheat_spring'},\n",
    "    'wheat_winter': {'path': '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv', 'yield_col': 'yield_wheat_winter'}\n",
    "}\n",
    "\n",
    "output_dir = 'data/analysis_ready/feature_engineered/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"--- Starting Corrected Feature Engineering for All Crops ---\")\n",
    "\n",
    "# --- Main Loop to Process Each Crop Dataset ---\n",
    "for crop_name, info in file_info.items():\n",
    "    print(f\"\\n--- Processing: {crop_name.upper()} ---\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load the Core Dataset\n",
    "        df = pd.read_csv(info['path'])\n",
    "        \n",
    "        # 2. Calculate Yield Anomalies\n",
    "        df['yield_anomaly'] = df.groupby(['lat', 'lon'])[info['yield_col']].transform(lambda x: detrend(x))\n",
    "        df['yield_anomaly_diff'] = df.groupby(['lat', 'lon'])[info['yield_col']].diff()\n",
    "        \n",
    "        # 3. Create the ONE new useful stressor column (W/m^2)\n",
    "        # The 'solar_radiation' column is the sum of Joules for the growing season.\n",
    "        # We find the number of months to correctly average it.\n",
    "        num_months = len([col for col in df.columns if 'solar_radiation_' in col])\n",
    "        seconds_in_season = num_months * 30.44 * 24 * 3600\n",
    "        df['gs_solar_rad_w_m2'] = df['solar_radiation'] / seconds_in_season\n",
    "        \n",
    "        # 4. Add a unique cell_id for fixed effects modeling\n",
    "        df['cell_id'] = df.groupby(['lat', 'lon']).ngroup()\n",
    "\n",
    "        # 5. Define the final, clean column order\n",
    "        # Get all original columns first\n",
    "        final_cols = df.columns.tolist()\n",
    "        # Define our new/reordered columns\n",
    "        new_order = ['cell_id', 'lat', 'lon', 'year', info['yield_col'], 'yield_anomaly', 'yield_anomaly_diff']\n",
    "        # Remove them from the original list to avoid duplication\n",
    "        for col in new_order:\n",
    "            if col in final_cols: final_cols.remove(col)\n",
    "        # Combine the lists with our desired order at the front\n",
    "        final_cols = new_order + final_cols\n",
    "        # Apply the new order\n",
    "        df = df[final_cols]\n",
    "        \n",
    "        # 6. Save the Enriched and Cleaned Dataset\n",
    "        output_path = os.path.join(output_dir, f'{crop_name}_feature_engineered.csv')\n",
    "        df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"Successfully processed and saved enriched dataset to: {output_path}\")\n",
    "        print(f\"Final columns: {df.columns.tolist()}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found: {info['path']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {crop_name}: {e}\")\n",
    "\n",
    "print(\"\\n--- All feature engineering complete. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 35-Scatterplot-Matrices.ipynb/35-Scatterplot-Matrices.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0956e0e8",
   "metadata": {},
   "source": [
    "# Phase 2: Exploratory Analysis via Scatterplot Matrices\n",
    "\n",
    "**Goal:** To visually inspect the relationships in our feature-engineered datasets. For each of the five crops, we will generate two key scatterplot matrices:\n",
    "1.  **Summary Stressors Matrix:** To compare the high-level impact of temperature, precipitation, soil water, and solar radiation.\n",
    "2.  **Monthly Temperature Matrix:** To identify if a specific month's temperature is a stronger predictor than the seasonal average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7215717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Generate All Scatterplot Matrices\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# List of the new feature-engineered dataset files\n",
    "file_info = {\n",
    "    'maize': {'path': '../data-cherry-pick/feature_engineered/maize_feature_engineered.csv'},\n",
    "    'rice': {'path': '../data-cherry-pick/feature_engineered/rice_feature_engineered.csv'},\n",
    "    'soybean': {'path': '../data-cherry-pick/feature_engineered/soybean_feature_engineered.csv'},\n",
    "    'wheat_spring': {'path': '../data-cherry-pick/feature_engineered/wheat_spring_feature_engineered.csv'},\n",
    "    'wheat_winter': {'path': '../data-cherry-pick/feature_engineered/wheat_winter_feature_engineered.csv'}\n",
    "}\n",
    "\n",
    "print(\"--- Generating Scatterplot Matrices for All Crops ---\")\n",
    "\n",
    "# --- Main Loop to Process Each Crop Dataset ---\n",
    "for crop_name, info in file_info.items():\n",
    "    print(f\"\\n{'='*20} Processing: {crop_name.upper()} {'='*20}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load the data\n",
    "        df = pd.read_csv(info['path'])\n",
    "        \n",
    "        # --- MATRIX 1: SUMMARY STRESSORS ---\n",
    "        print(\"\\nGenerating Summary Stressors Matrix...\")\n",
    "        summary_cols = [\n",
    "            'yield_anomaly', 'temperature', 'precipitation', \n",
    "            'soil_water', 'gs_solar_rad_w_m2'\n",
    "        ]\n",
    "        \n",
    "        g1 = sns.pairplot(\n",
    "            df[summary_cols],\n",
    "            kind='scatter',\n",
    "            diag_kind='kde',\n",
    "            plot_kws={'alpha': 0.3, 's': 10},\n",
    "            diag_kws={'fill': True}\n",
    "        )\n",
    "        g1.fig.suptitle(f'Summary Stressor Matrix for {crop_name.capitalize()}', y=1.03, fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "        # --- MATRIX 2: MONTHLY TEMPERATURE DEEP DIVE ---\n",
    "        print(\"\\nGenerating Monthly Temperature Matrix...\")\n",
    "        monthly_temp_cols = [col for col in df.columns if 'temperature_' in col]\n",
    "        \n",
    "        # Check if there are any monthly temp columns before trying to plot\n",
    "        if monthly_temp_cols:\n",
    "            cols_for_monthly_matrix = ['yield_anomaly'] + monthly_temp_cols\n",
    "            \n",
    "            g2 = sns.pairplot(\n",
    "                df[cols_for_monthly_matrix],\n",
    "                kind='scatter',\n",
    "                diag_kind='kde',\n",
    "                plot_kws={'alpha': 0.3, 's': 10},\n",
    "                diag_kws={'fill': True}\n",
    "            )\n",
    "            g2.fig.suptitle(f'Monthly Temperature Matrix for {crop_name.capitalize()}', y=1.03, fontsize=16)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No monthly temperature columns found for this crop.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found: {info['path']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {crop_name}: {e}\")\n",
    "\n",
    "print(\"\\n--- All scatterplot matrices generated. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Generate the Combined, Multi-Crop Scatterplot Matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- 1. Load and Combine All Feature-Engineered Datasets ---\n",
    "# This is the most important step. We will create one large DataFrame.\n",
    "\n",
    "# List of the new feature-engineered dataset files\n",
    "file_info = {\n",
    "    'maize': {'path': '../data-cherry-pick/feature_engineered/maize_feature_engineered.csv', 'yield_col': 'yield_maize'},\n",
    "    'rice': {'path': '../data-cherry-pick/feature_engineered/rice_feature_engineered.csv', 'yield_col': 'yield_rice'},\n",
    "    'soybean': {'path': '../data-cherry-pick/feature_engineered/soybean_feature_engineered.csv', 'yield_col': 'yield_soybean'},\n",
    "    'wheat_spring': {'path': '../data-cherry-pick/feature_engineered/wheat_spring_feature_engineered.csv', 'yield_col': 'yield_wheat_spring'},\n",
    "    #'wheat_winter': {'path': '../data-cherry-pick/feature_engineered/wheat_winter_feature_engineered.csv', 'yield_col': 'yield_wheat_winter'}\n",
    "}\n",
    "\n",
    "all_dfs = []\n",
    "print(\"--- Loading and combining all crop datasets ---\")\n",
    "\n",
    "for crop_name, info in file_info.items():\n",
    "    try:\n",
    "        df = pd.read_csv(info['path'])\n",
    "        \n",
    "        # Add a new column to identify the crop for each row\n",
    "        df['crop_type'] = crop_name\n",
    "        \n",
    "        # Rename the specific yield column to a generic name for plotting\n",
    "        df = df.rename(columns={info['yield_col']: 'yield_raw'})\n",
    "        \n",
    "        all_dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File not found for {crop_name}. Skipping.\")\n",
    "\n",
    "# Concatenate all the individual DataFrames into one master DataFrame\n",
    "master_df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"Master DataFrame created with {len(master_df)} total rows.\")\n",
    "\n",
    "\n",
    "# --- 2. Create the Combined Scatterplot Matrix ---\n",
    "print(\"\\n--- Generating the final combined matrix ---\")\n",
    "\n",
    "# Define the summary columns we want to plot\n",
    "summary_cols = [\n",
    "    'yield_anomaly', 'temperature', 'precipitation', \n",
    "    'soil_water', 'gs_solar_rad_w_m2'\n",
    "]\n",
    "\n",
    "# Create the pairplot, using the 'crop_type' column to color the points\n",
    "g = sns.pairplot(\n",
    "    master_df,\n",
    "    vars=summary_cols, # The columns to plot\n",
    "    hue='crop_type', # The column that determines the color\n",
    "    kind='scatter',\n",
    "    diag_kind='kde',\n",
    "    plot_kws={'alpha': 0.2, 's': 10},\n",
    "    diag_kws={'fill': True}\n",
    ")\n",
    "\n",
    "# Add regression lines to the scatter plots for clarity\n",
    "#g.map_lower(sns.regplot, scatter_kws={'alpha': 0.0})\n",
    "#g.map_upper(sns.regplot, scatter_kws={'alpha': 0.0})\n",
    "\n",
    "# --- Formatting ---\n",
    "g.fig.suptitle('Combined Summary Stressor Matrix for All Crops in Northern Italy', y=1.02, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# If you don't want trendlines in the scatter plots, comment out the two g.map_lower and g.map_upper lines above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 36-glm-models.ipynb/36-glm-models.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71a6103",
   "metadata": {},
   "source": [
    "## GLM approach using gamma models\n",
    "\n",
    "In this notebook, we focused on moving away from OLS and into GLMs. \n",
    "\n",
    "To start this, we decided to refine our previous approaches, and perform a step-by-step data driven approach to chose which stressors are most impactful when it comes to yield. \n",
    "\n",
    "We also decided to not go trhough with the detrending we used before, and rather, use year as a cofactor. This is due to the fact that our previous detrending approach resulted in negative numbers, and a gamma model cannot handle these negative numbers. \n",
    "\n",
    "To start out with, we want to loop over all potential stressors, and see which ones are the most impactful on our yield. We do this to better understand how we want to build our end model. We extract AIC scores and rank the top 10 influencial stressor per crop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries:\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028559a",
   "metadata": {},
   "source": [
    "### Stressor looping and AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe471a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress convergence warnings for this screening step, as some models may not fit perfectly\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "print(\"--- Starting Univariate Stressor Screening for Maize ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We'll use the same pre-processed maize dataset from your EDA.\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset: {file_path}\")\n",
    "\n",
    "    # --- 2. Prepare Data for Modeling ---\n",
    "    # As per our plan, we'll use raw yield as the dependent variable.\n",
    "    # The Gamma GLM requires all dependent variable values to be > 0.\n",
    "    if (df_maize['yield_maize'] <= 0).any():\n",
    "        print(\"Warning: Found non-positive yield values. These will be excluded for the Gamma GLM.\")\n",
    "        df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "\n",
    "    # Create the unique identifier for each grid cell for our fixed effects model.\n",
    "    df_maize['cell_id'] = df_maize.groupby(['lat', 'lon']).ngroup()\n",
    "    \n",
    "    # --- 3. Identify Stressor Variables ---\n",
    "    # Programmatically create a list of all potential climate stressor columns to test.\n",
    "    # We exclude identifiers, the outcome variable, and control variables.\n",
    "    excluded_cols = ['lat', 'lon', 'year', 'yield_maize', 'cell_id']\n",
    "    stressor_variables = [col for col in df_maize.columns if col not in excluded_cols]\n",
    "    \n",
    "    print(f\"\\nFound {len(stressor_variables)} potential stressor variables to screen.\")\n",
    "\n",
    "    # --- 4. Screening Loop ---\n",
    "    # This loop will fit a separate GLM for each stressor and store the results.\n",
    "    results = []\n",
    "\n",
    "    for stressor in stressor_variables:\n",
    "        try:\n",
    "            # Construct the formula for each model.\n",
    "            # `year` and `C(cell_id)` are included as controls in every model.\n",
    "            formula = f\"yield_maize ~ year + {stressor} + C(cell_id)\"\n",
    "            \n",
    "            # Fit the Gamma GLM. We use a log link, which is common and robust.\n",
    "            # It ensures predictions are positive and makes coefficients easier to interpret.\n",
    "            model = smf.glm(\n",
    "                formula=formula, \n",
    "                data=df_maize, \n",
    "                family=sm.families.Gamma(link=sm.families.links.log()) #log linking might not be that important here !! \n",
    "            ).fit()\n",
    "            \n",
    "            # Extract AIC, coefficient, and p-value for the stressor\n",
    "            aic = model.aic\n",
    "            coef = model.params[stressor]\n",
    "            p_value = model.pvalues[stressor]\n",
    "            \n",
    "            # Store the results\n",
    "            results.append({\n",
    "                'Stressor': stressor,\n",
    "                'AIC': aic,\n",
    "                'p_value': p_value,\n",
    "                'Coefficient': coef\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not fit model for {stressor}. Error: {e}\")\n",
    "\n",
    "    # --- 5. Analyze and Display Results ---\n",
    "    # Convert the results to a DataFrame and sort by AIC (lower is better).\n",
    "    results_df = pd.DataFrame(results).sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n--- Top 10 Stressors for Maize (Ranked by AIC) ---\")\n",
    "    # Display the results, formatted for clarity\n",
    "    print(results_df.head(10).to_string(formatters={\n",
    "        'AIC': '{:,.2f}'.format,\n",
    "        'p_value': '{:,.4f}'.format,\n",
    "        'Coefficient': '{:,.4f}'.format\n",
    "    }))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715718ab",
   "metadata": {},
   "source": [
    "## building a multivariate GLM using stressors identified above\n",
    "\n",
    "To avoid multicolinearity, we wanna chose stressors that we dont think are hihgly correlated with each other. The top 5 stressors we already identified are: \n",
    "\n",
    "1. temperature_Jul (Heat)\n",
    "\n",
    "2. soil_water_Jul (Water Availability)\n",
    "\n",
    "3. potential_evaporation_Jul (Water Loss, driven by heat)\n",
    "\n",
    "4. temperature_May (Early Season Heat)\n",
    "\n",
    "5. precipitation_Jul (Water Input)\n",
    "\n",
    "Physically, we know these are linked. A hot July (temperature_Jul ) causes more evaporation (potential_evaporation_Jul ), which in turn dries out the soil (soil_water_Jul ). They aren't independent pieces of information adn so we need to be careful when chosing which stressors to use.\n",
    "\n",
    "So for the following models, we are going to start by just using our most inlfuencial stressor, before adding new info, that is not highly corrolated with the already in use stressors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1eb65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress convergence warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "print(\"--- Step 2: Building Multivariate GLM for Maize ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "    df_maize['cell_id'] = df_maize.groupby(['lat', 'lon']).ngroup()\n",
    "    print(\"Data prepared successfully.\")\n",
    "\n",
    "    # --- 2. Fit a Sequence of Models ---\n",
    "    \n",
    "    # We'll store the AIC of each model to compare them at the end\n",
    "    model_comparison = {}\n",
    "    \n",
    "    # --- Model 1: Our New Baseline (Top Predictor Only) ---\n",
    "    print(\"\\nFitting Model 1: Baseline with temperature_Jul...\")\n",
    "    formula1 = \"yield_maize ~ year + temperature_Jul + C(cell_id)\"\n",
    "    model1 = smf.glm(formula=formula1, data=df_maize, family=sm.families.Gamma(link=sm.families.links.log())).fit()\n",
    "    model_comparison['Model 1 (Temp Jul)'] = model1.aic\n",
    "    # print(model1.summary()) # Optional: uncomment to see full summary\n",
    "    \n",
    "    # --- Model 2: Adding the Next Best Predictor (Soil Water) ---\n",
    "    # From our screening, soil_water_Jul was #2. Let's see if it adds explanatory power.\n",
    "    print(\"Fitting Model 2: Adding soil_water_Jul...\")\n",
    "    formula2 = \"yield_maize ~ year + temperature_Jul + soil_water_Jul + C(cell_id)\"\n",
    "    model2 = smf.glm(formula=formula2, data=df_maize, family=sm.families.Gamma(link=sm.families.links.log())).fit()\n",
    "    model_comparison['Model 2 (Temp Jul + Soil Jul)'] = model2.aic\n",
    "    # print(model2.summary())\n",
    "\n",
    "    # --- Model 3: Adding an Early-Season Predictor (May Temperature) ---\n",
    "    # Temperature in May was ranked #4. Does an early-season effect matter on top of the July stress?\n",
    "    print(\"Fitting Model 3: Adding temperature_May...\")\n",
    "    formula3 = \"yield_maize ~ year + temperature_Jul + soil_water_Jul + temperature_May + C(cell_id)\"\n",
    "    model3 = smf.glm(formula=formula3, data=df_maize, family=sm.families.Gamma(link=sm.families.links.log())).fit()\n",
    "    model_comparison['Model 3 (Temp Jul + Soil Jul + Temp May)'] = model3.aic\n",
    "    # print(model3.summary())\n",
    "\n",
    "    # --- Model 4: Adding Non-Linearity with Splines ---\n",
    "    # This is our key hypothesis: the effect of July temperature is non-linear.\n",
    "    # We replace 'temperature_Jul' with a basis spline function 'bs()'.\n",
    "    # df=4 means a cubic spline with one internal knot - a good, flexible start.\n",
    "    print(\"Fitting Model 4: Adding Spline on temperature_Jul...\")\n",
    "    formula4 = \"yield_maize ~ year + bs(temperature_Jul, df=4) + soil_water_Jul + C(cell_id)\"\n",
    "    model4 = smf.glm(formula=formula4, data=df_maize, family=sm.families.Gamma(link=sm.families.links.log())).fit()\n",
    "    model_comparison['Model 4 (Spline Temp Jul + Soil Jul)'] = model4.aic\n",
    "    print(model4.summary()) # Let's look at the full summary for this one.\n",
    "\n",
    "    # --- 3. Compare Model Performance ---\n",
    "    print(\"\\n--- Model Comparison (lower AIC is better) ---\")\n",
    "    aic_df = pd.DataFrame.from_dict(model_comparison, orient='index', columns=['AIC'])\n",
    "    print(aic_df.sort_values(by='AIC').to_string(formatters={'AIC': '{:,.2f}'.format}))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77862768",
   "metadata": {},
   "source": [
    "### interpretation:\n",
    "\n",
    "First thing that sticks out to me is the Pseudo R-squared value. It is very high. The combo of year and individual cell_id fixed effects are explaining almost all the variance in the absolute yield values. This however makes sense, as a cell in a really ideal area should always have a higher average yield compared to a cell in a less fertile area. So our fixed effects are correctly capturing baseline dofferences, which means we can be more confident that the climate coeffeicants are truly explainig the deviations from the average in each cell. \n",
    "\n",
    "Looking at the coefficients, we need to remember that we used a log link, so they reprsent a multiplicative or precentage change. \n",
    "\n",
    "The intercept and c(cell_id) are our fixed effects. \n",
    "- The cell_id individual coeficcients are not really that intresting. These are simply herer to control for the baseline differences in location. \n",
    "- The year coeficcient: coef = 0.0107, P>|z| = 0.000 is our technology/management trend. It's positive and highly significant. It means that, holding all else constant, maize yields increased by about 1.07% per year on average across this period.\n",
    "\n",
    "bs(temperature_Jul, df=4)[0] through [3]: These are the coefficients for our spline. Crucially, we do not interpret these numbers individually. They are the weights for the basis functions that, together, create the flexible curve for the effect of July temperature. The key takeaway here is that their p-values are all extremely low (0.000). This tells us that the curve they collectively define is a highly statistically significant predictor of yield.\n",
    "\n",
    "soil_water_Jul: coef = 0.1651, P>|z| = 0.052. This is the effect of soil water in July. It's positive, as we'd expect (more water is generally good). The p-value of 0.052 is \"marginally significant.\" It's right on the edge of the standard 0.05 cutoff. In Model 2, it was highly significant, but its importance has been slightly reduced by the addition of the temperature spline, suggesting some overlap in the information they provide.\n",
    "\n",
    "\n",
    "#### Comparing the 4 models !!\n",
    "\n",
    "All of the new models are an improvement from the original single stressor models, which was expected. \n",
    "\n",
    "1. Model 1 -> Model 2: Adding soil_water_Jul lowers the AIC (from 3999.78 to 3998.33). The improvement is small but real. It tells us that knowing the soil moisture adds a little bit of explanatory power on top of knowing the temperature.\n",
    "\n",
    "2. Model 2 -> Model 4: Allowing temperature_Jul to be a flexible curve (spline) instead of a straight line lowers the AIC again (from 3998.33 to 3997.27). This confirms your non-linear hypothesis! The curve is a better fit.\n",
    "\n",
    "3. The Big Finding (Model 2 -> Model 3): Your observation is spot on. Adding temperature_May to the model with linear July temperature and soil water causes a huge drop in AIC (from 3998.33 down to 3947.66). This is a much larger improvement than we got from adding the spline. It's a very strong signal that conditions at the start of the growing season have a major impact on the final yield, even after accounting for the critical stress period in July.\n",
    "\n",
    "This suggests that a warmer May can lead to a better early season growth and root development for Maize, making it more resiliant to the heat and water stress it can face in July. And then the oposite is also true, a bad start in Mayb can make the plant more vulnerable to any stress that comes later. \n",
    "\n",
    "However, these findings just further prove our original interraction hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3b4d05",
   "metadata": {},
   "source": [
    "#### Using model 3, lets test for interractions and make a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b885f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Final Multivariate Model with Interaction for Maize\n",
    "print(\"\\n--- Step 2b: Fitting Best Model with Spline and Interaction ---\")\n",
    "\n",
    "try:\n",
    "    # --- Model 5: Spline on July Temp *INTERACTING WITH* May Temp ---\n",
    "    # The '*' in the formula tells statsmodels to include the main effects of both\n",
    "    # variables AND their interaction term. This is a powerful and concise way\n",
    "    # to test our new hypothesis.\n",
    "    formula5 = \"yield_maize ~ year + bs(temperature_Jul, df=4) * temperature_May + C(cell_id)\"\n",
    "    formula5 = \"yield_maize ~ year + temperature_Jul * temperature_May + bs(lat, df=4) + bs(lon, df=4)\" #try lat/lon interraction? \n",
    "    \n",
    "    model5 = smf.glm(formula=formula5, data=df_maize, family=sm.families.Gamma(link=sm.families.links.log())).fit()\n",
    "\n",
    "    print(\"\\n--- Final Model (Model 5) Summary ---\")\n",
    "    print(model5.summary())\n",
    "\n",
    "    print(f\"\\nComparing AICs:\")\n",
    "    print(f\"  Previous Best AIC (Model 3): {model_comparison['Model 3 (Temp Jul + Soil Jul + Temp May)']:,.2f}\")\n",
    "    print(f\"  New Model AIC (Model 5):     {model5.aic:,.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58adc1",
   "metadata": {},
   "source": [
    "#### model 5 analysis\n",
    "\n",
    "We see a decrease in our AIC score when adding the interraction term !! nice. We also see that each individual part of our model is significant, but is it a significant improvment to the fit when we add the inteaction term? To test this we need to perform a likelihopod ratio test. This simply tests our current \"full-model\" to a \"reduced model\", where we dont have the interraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a49e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Likelihood Ratio Test for the Interaction Term\n",
    "\n",
    "print(\"\\n--- Testing Significance of the Full Interaction Term ---\")\n",
    "\n",
    "try:\n",
    "    # First, we need to create the \"reduced\" model. This model includes both\n",
    "    # temperature variables as main effects, but NOT their interaction.\n",
    "    formula_reduced = \"yield_maize ~ year + bs(temperature_Jul, df=4) + temperature_May + C(cell_id)\"\n",
    "    \n",
    "    model_reduced = smf.glm(\n",
    "        formula=formula_reduced, \n",
    "        data=df_maize, \n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    ).fit()\n",
    "    \n",
    "    # The Likelihood Ratio Test\n",
    "    lr_statistic = 2 * (model5.llf - model_reduced.llf)\n",
    "    degrees_of_freedom = model5.df_model - model_reduced.df_model\n",
    "    \n",
    "    # We use a chi-squared distribution to get the p-value\n",
    "    from scipy.stats import chi2\n",
    "    p_value_interaction = chi2.sf(lr_statistic, degrees_of_freedom)\n",
    "\n",
    "    print(f\"Likelihood Ratio Test Statistic: {lr_statistic:.4f}\")\n",
    "    print(f\"Degrees of Freedom: {degrees_of_freedom}\")\n",
    "    print(f\"P-value for the interaction term as a group: {p_value_interaction:.4f}\")\n",
    "    \n",
    "    if p_value_interaction < 0.05:\n",
    "        print(\"\\nConclusion: The interaction is statistically significant. The effect of July temperature on yield DEPENDS on the May temperature.\")\n",
    "    else:\n",
    "        print(\"\\nConclusion: The interaction is not statistically significant.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the Likelihood Ratio Test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5eb4d",
   "metadata": {},
   "source": [
    "#### more analysis on model 5 and visualizations\n",
    "\n",
    "The individual coefficients for the interaction terms are difficult to interpret directly. The best way to understand an interaction is to visualize it.\n",
    "What does the interaction we added mean? It means the vulnerability curve for July temperature is not fixed. It changes its shape depending on the temperature in May.\n",
    "\n",
    "Let's plot this. We will generate the vulnerability curve for July temperature for three different scenarios:\n",
    "- A cool May (e.g., the 10th percentile of May temperatures)\n",
    "- An average May (the 50th percentile, or median)\n",
    "- A warm May (the 90th percentile)\n",
    "\n",
    "If our hypothesis is correct, we should see three different curves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31816dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Visualize the Interaction Effect with Confidence Intervals\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n--- Visualizing the Interaction with Confidence Intervals ---\")\n",
    "\n",
    "# --- 1. Get the levels for May temperature ---\n",
    "cool_may = df_maize['temperature_May'].quantile(0.10)\n",
    "avg_may = df_maize['temperature_May'].quantile(0.50)\n",
    "warm_may = df_maize['temperature_May'].quantile(0.90)\n",
    "\n",
    "# --- 2. Create a prediction grid ---\n",
    "x_range_jul = np.linspace(df_maize['temperature_Jul'].min(), df_maize['temperature_Jul'].max(), 100)\n",
    "scenarios = {\n",
    "    f'Cool May ({cool_may:.1f}C)': cool_may,\n",
    "    f'Average May ({avg_may:.1f}C)': avg_may,\n",
    "    f'Warm May ({warm_may:.1f}C)': warm_may\n",
    "}\n",
    "colors = {'Cool': 'blue', 'Average': 'green', 'Warm': 'red'}\n",
    "\n",
    "# --- 3. Plot the base scatter plot ---\n",
    "plt.figure(figsize=(14, 9))\n",
    "plt.scatter(df_maize['temperature_Jul'], df_maize['yield_maize'], alpha=0.1, label='Raw Data Points')\n",
    "\n",
    "# --- 4. Loop through scenarios to predict and plot ---\n",
    "for label, may_temp in scenarios.items():\n",
    "    # Create a dataframe with all necessary variables for prediction\n",
    "    pred_df = pd.DataFrame({\n",
    "        'temperature_Jul': x_range_jul,\n",
    "        'temperature_May': may_temp,\n",
    "        'year': df_maize['year'].mean(),\n",
    "        'cell_id': 0  # Predict for the reference cell\n",
    "    })\n",
    "    \n",
    "    # Get predictions and confidence intervals\n",
    "    preds = model5.get_prediction(pred_df)\n",
    "    pred_summary = preds.summary_frame(alpha=0.05) # alpha=0.05 for 95% CI\n",
    "    \n",
    "    # Extract key parts from the summary\n",
    "    y_preds = pred_summary['mean']\n",
    "    ci_lower = pred_summary['mean_ci_lower']\n",
    "    ci_upper = pred_summary['mean_ci_upper']\n",
    "    \n",
    "    # Plot the regression line and the confidence interval\n",
    "    color_key = label.split(' ')[0]\n",
    "    plt.plot(x_range_jul, y_preds, color=colors[color_key], linewidth=3, label=f'Vulnerability Curve for an {label}')\n",
    "    plt.fill_between(x_range_jul, ci_lower, ci_upper, color=colors[color_key], alpha=0.1)\n",
    "\n",
    "# --- 5. Formatting ---\n",
    "plt.title('Maize Vulnerability to July Temperature is MODERATED by May Temperature', fontsize=18)\n",
    "plt.xlabel('Average July Temperature (C)', fontsize=14)\n",
    "plt.ylabel('Predicted Maize Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Raw Data Points\", loc=\"upper right\")\n",
    "# Adjust y-axis to a more realistic range to avoid showing extreme extrapolations\n",
    "plt.ylim(bottom=5, top=35) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e9b36e",
   "metadata": {},
   "source": [
    "#### visalization analysis\n",
    "\n",
    "- The X-axis is our primary stressor: temperature_Jul.\n",
    "- The Y-axis is the outcome we care about: Predicted Yield.\n",
    "- The three separate lines show how the relationship between the X and Y axes changes depending on our third variable, temperature_May.\n",
    "\n",
    "1. A warm start is a good start: The red curve (Warm May) starts at a much higher predicted yield than the blue curve (Cool May). This suggests that warmer May temperatures allow the maize to establish itself better and achieve a higher yield potential.\n",
    "\n",
    "2. The Interaction Effect (The \"So What?\"): The vulnerability to July heat is not static.\n",
    "    - After a warm May (red line), the yield potential is high, but the crop is also more sensitive to increasing July heat, showing a steep and continuous decline as temperatures rise.\n",
    "    - After a cool May (blue line), the yield potential is lower, but the crop appears somewhat more resilient to moderate increases in July temperature (the curve is flatter in the 15-23C range).\n",
    "\n",
    "3. The \"Cliff Edge\": The model is uncertain what happens to cool-spring crops when they are hit with extreme (>25C) July heat because it rarely happens in the historical data. However, for average and warm springs, the evidence clearly shows that high July temperatures lead to a strong, non-linear decline in yield."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c14e2",
   "metadata": {},
   "source": [
    "### What we can say so far\n",
    "\n",
    "1. We've Quantified the Long-Term Trend: \"After controlling for location-specific advantages, we found that maize yields in Northern Italy have been increasing by approximately 1.1% per year from 1982-2016. This is likely due to a combination of technological and management improvements.\" (This comes from the year coefficient).\n",
    "\n",
    "2. We've Confirmed a Critical Vulnerability Period: \"Our analysis confirms that the single month of July is a critical period for maize yield. Climate stressors during this month have a disproportionately large impact compared to the growing season as a whole.\"\n",
    "\n",
    "3. We've Modeled a Non-Linear \"Damage Curve\": \"The relationship between July heat and maize yield is not a straight line. Our model shows that the damage accelerates at higher temperatures. The difference between a 24C and 25C July is significantly more harmful than the difference between an 18C and 19C July.\" (This is the contribution of the spline).\n",
    "\n",
    "4. The Main Finding - We've Discovered a Resilience Factor: \"Most importantly, we discovered that the vulnerability of maize to July heat is not fixed. It is significantly moderated by conditions earlier in the season. A warmer May leads to higher overall yield potential, making the crop more robust and resilient against subsequent heat stress in July.\" (This is the story told by your interaction plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48aeb5",
   "metadata": {},
   "source": [
    "#### EXPLORING PRECIPITATION IN JULY IN RELATION TO OUR CURRENT BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d586283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress convergence warnings that might arise during model fitting\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "print(\"--- Step A: Finalizing the Best Maize Model ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    # Ensure yield is positive for the Gamma model\n",
    "    df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "    # Create the cell_id for fixed effects\n",
    "    df_maize['cell_id'] = df_maize.groupby(['lat', 'lon']).ngroup()\n",
    "    print(\"Data prepared successfully.\")\n",
    "\n",
    "    # --- 2. Fit the Sequence of Competing Models ---\n",
    "    \n",
    "    # Dictionary to store the AIC of each model for comparison\n",
    "    model_comparison = {}\n",
    "    # Dictionary to store the fitted model objects\n",
    "    models = {}\n",
    "\n",
    "    # --- Model 5: Our Current Best (from the previous step) ---\n",
    "    print(\"\\nFitting Model 5: Spline(Temp Jul) * Temp May (Our Current Best)...\")\n",
    "    formula5 = \"yield_maize ~ year + bs(temperature_Jul, df=4) * temperature_May + C(cell_id)\"\n",
    "    model5 = smf.glm(formula=formula5, data=df_maize, family=sm.families.Gamma(link=sm.families.links.log())).fit()\n",
    "    model_comparison['Model 5 (TempJul*TempMay)'] = model5.aic\n",
    "    models['Model 5 (TempJul*TempMay)'] = model5\n",
    "    \n",
    "    # --- Model 6: Adding Precipitation as a Main Effect ---\n",
    "    # Here, we test if adding rainfall in July improves the model.\n",
    "    print(\"Fitting Model 6: Adding Precipitation_Jul...\")\n",
    "    formula6 = \"yield_maize ~ year + bs(temperature_Jul, df=4) * temperature_May + precipitation_Jul + C(cell_id)\"\n",
    "    model6 = smf.glm(formula=formula6, data=df_maize, family=sm.families.Gamma(link=sm.families.links.log())).fit()\n",
    "    model_comparison['Model 6 (Add PrecipJul)'] = model6.aic\n",
    "    models['Model 6 (Add PrecipJul)'] = model6\n",
    "\n",
    "    # --- Model 7: Testing the Heat x Drought Interaction ---\n",
    "    # This model tests if the effect of July temperature is made worse by a lack of rain in the same month.\n",
    "    # The formula `A * B + A * C` ensures we keep the first interaction while adding the second.\n",
    "    print(\"Fitting Model 7: Testing Temp_Jul * Precipitation_Jul Interaction...\")\n",
    "    formula7 = \"yield_maize ~ year + bs(temperature_Jul, df=4) * temperature_May + bs(temperature_Jul, df=4) * precipitation_Jul + C(cell_id)\"\n",
    "    model7 = smf.glm(formula=formula7, data=df_maize, family=sm.families.Gamma(link=sm.families.links.log())).fit()\n",
    "    model_comparison['Model 7 (Add TempJul*PrecipJul Interaction)'] = model7.aic\n",
    "    models['Model 7 (Add TempJul*PrecipJul Interaction)'] = model7\n",
    "    \n",
    "    # --- 3. Compare Model Performance ---\n",
    "    print(\"\\n--- Final Model Comparison (lower AIC is better) ---\")\n",
    "    aic_df = pd.DataFrame.from_dict(model_comparison, orient='index', columns=['AIC'])\n",
    "    # Sort the results to easily see the best model\n",
    "    aic_df = aic_df.sort_values(by='AIC')\n",
    "    print(aic_df.to_string(formatters={'AIC': '{:,.2f}'.format}))\n",
    "\n",
    "    # --- 4. Display Summary of the Winning Model ---\n",
    "    # Programmatically find the name of the model with the lowest AIC\n",
    "    best_model_name = aic_df.index[0]\n",
    "    print(f\"\\n--- Summary for the Best Model: {best_model_name} ---\")\n",
    "    best_model = models[best_model_name]\n",
    "    print(best_model.summary())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892cdf5",
   "metadata": {},
   "source": [
    "#### testing if adding the bs(temperature_Jul, df=4) * precipitation_Jul interaction term provides a significant improvement over Model 5.\n",
    "\n",
    "- Null Hypothesis (H): The interaction term adds no significant explanatory power. The simpler model (Model 5) is sufficient.\n",
    "- Alternative Hypothesis (H): The interaction term is significant. The more complex model (Model 7) is a statistically better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fda696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Likelihood Ratio Test for the Heat x Drought Interaction\n",
    "\n",
    "print(\"\\n--- Testing Significance of the TempJul*PrecipJul Interaction Term ---\")\n",
    "\n",
    "try:\n",
    "    # Model 7 is our \"full\" model. Model 5 is our \"reduced\" model.\n",
    "    lr_statistic = 2 * (model7.llf - model5.llf)\n",
    "    degrees_of_freedom = model7.df_model - model5.df_model\n",
    "    \n",
    "    # We use a chi-squared distribution to get the p-value\n",
    "    from scipy.stats import chi2\n",
    "    p_value_interaction = chi2.sf(lr_statistic, degrees_of_freedom)\n",
    "\n",
    "    print(f\"Likelihood Ratio Test Statistic: {lr_statistic:.4f}\")\n",
    "    print(f\"Degrees of Freedom difference: {degrees_of_freedom}\")\n",
    "    print(f\"P-value for the interaction as a group: {p_value_interaction:.4f}\")\n",
    "    \n",
    "    if p_value_interaction < 0.05:\n",
    "        print(\"\\nConclusion: The improvement from adding the heat x precipitation interaction is statistically significant.\")\n",
    "    else:\n",
    "        print(\"\\nConclusion: The interaction is not statistically significant; the simpler Model 5 is preferred.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the Likelihood Ratio Test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4195bf04",
   "metadata": {},
   "source": [
    "#### interpretation of model 7\n",
    "\n",
    "AIC Comparison: Model 7's AIC (3,878.16) is the best yet. This is our first clue that we've found a very good combination of predictors.\n",
    "\n",
    "Pseudo R-squ.: 0.9994: Still extremely high, which is great. It confirms that the fixed effects (cell_id) and the overall trend (year) are successfully capturing the vast majority of the baseline yield variation.\n",
    "\n",
    "1. Main Effect of Precipitation (precipitation_Jul)\n",
    "    - coef = -0.0009, P>|z| = 0.167\n",
    "    - Interpretation: On its own, the main effect of July precipitation is not statistically significant and is very close to zero. This seems counter-intuitive at first. Doesn't more rain lead to more yield? What this tells us is that, on average, the effect of rain is not consistent. Sometimes it helps, sometimes it doesn't. This is because its importance depends on another factor. \n",
    "\n",
    "2. The Interaction Effect (bs(temperature_Jul...):precipitation_Jul)\n",
    "    - This is the most important new finding. Let's look at the coefficients:\n",
    "        - bs(...)[0]:precipitation_Jul: p=0.067 (marginally significant)\n",
    "        - bs(...)[3]:precipitation_Jul: p=0.005 (highly significant!)\n",
    "    - Interpretation: The fact that these interaction terms are significant (especially the one for the highest temperature part of the spline) tells us that the effect of July precipitation DEPENDS on the July temperature.\n",
    "    - What is the direction? The coefficient for the significant interaction ([3]) is positive (+0.0056). Remember that the main effect of very high July temperature is strongly negative (as seen by the large negative spline coefficients in our previous models). This positive interaction coefficient means that as precipitation_Jul increases, it makes the negative effect of temperature less negative.\n",
    "\n",
    "While July precipitation on its own is not a universally strong predictor of maize yield, our model reveals a significant heat-drought interaction. The data shows that adequate rainfall in July acts as a crucial buffer against the damaging effects of extreme heat. The negative impact of a very hot July is significantly less severe in years that also have higher precipitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed5ea9",
   "metadata": {},
   "source": [
    "#### visualizing a wet vs dry july in relation to the interractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed56d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Visualize the Heat x Drought Interaction Effect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n--- Visualizing the Heat x Drought (TempJul * PrecipJul) Interaction ---\")\n",
    "\n",
    "# --- 1. Get the levels for July precipitation ---\n",
    "dry_jul = df_maize['precipitation_Jul'].quantile(0.10)\n",
    "avg_jul = df_maize['precipitation_Jul'].quantile(0.50)\n",
    "wet_jul = df_maize['precipitation_Jul'].quantile(0.90)\n",
    "\n",
    "print(f\"Plotting for a Dry July ({dry_jul:.1f} mm), Average July ({avg_jul:.1f} mm), and Wet July ({wet_jul:.1f} mm)\")\n",
    "\n",
    "# --- 2. Create a prediction grid ---\n",
    "x_range_jul_temp = np.linspace(df_maize['temperature_Jul'].min(), df_maize['temperature_Jul'].max(), 100)\n",
    "\n",
    "# Create a base dataframe for predictions\n",
    "pred_df_base = pd.DataFrame({\n",
    "    'temperature_Jul': x_range_jul_temp,\n",
    "    'temperature_May': df_maize['temperature_May'].mean(), # Hold May temp constant\n",
    "    'year': df_maize['year'].mean(),\n",
    "    'cell_id': 0\n",
    "})\n",
    "\n",
    "# Create dataframes for each precipitation scenario\n",
    "pred_df_dry = pred_df_base.copy()\n",
    "pred_df_dry['precipitation_Jul'] = dry_jul\n",
    "\n",
    "pred_df_avg = pred_df_base.copy()\n",
    "pred_df_avg['precipitation_Jul'] = avg_jul\n",
    "\n",
    "pred_df_wet = pred_df_base.copy()\n",
    "pred_df_wet['precipitation_Jul'] = wet_jul\n",
    "\n",
    "\n",
    "# --- 3. Generate Predictions using our best model (Model 7) ---\n",
    "y_preds_dry = model7.predict(pred_df_dry)\n",
    "y_preds_avg = model7.predict(pred_df_avg)\n",
    "y_preds_wet = model7.predict(pred_df_wet)\n",
    "\n",
    "# --- 4. Plot the Results ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the vulnerability curves\n",
    "plt.plot(x_range_jul_temp, y_preds_dry, color='orange', linewidth=3, linestyle='--', label=f'Vulnerability Curve for a Dry July ({dry_jul:.1f} mm)')\n",
    "plt.plot(x_range_jul_temp, y_preds_avg, color='blue', linewidth=2, label=f'Vulnerability Curve for an Average July ({avg_jul:.1f} mm)')\n",
    "plt.plot(x_range_jul_temp, y_preds_wet, color='green', linewidth=3, linestyle=':', label=f'Vulnerability Curve for a Wet July ({wet_jul:.1f} mm)')\n",
    "\n",
    "# --- Formatting ---\n",
    "plt.title('Maize Vulnerability to July Heat is Buffered by July Precipitation', fontsize=16)\n",
    "plt.xlabel('Average July Temperature (C)', fontsize=12)\n",
    "plt.ylabel('Predicted Maize Yield (tonnes per hectare)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Advanced Visualization of the Final Maize Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"--- Advanced Visualization for the Champion Maize Model ---\")\n",
    "\n",
    "# --- 1. Load Data and Fit the Best Model (Model 7) ---\n",
    "# This ensures this cell can be run independently\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "df_maize = pd.read_csv(file_path)\n",
    "df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "df_maize['cell_id'] = df_maize.groupby(['lat', 'lon']).ngroup()\n",
    "\n",
    "formula7 = \"yield_maize ~ year + bs(temperature_Jul, df=4) * temperature_May + bs(temperature_Jul, df=4) * precipitation_Jul + C(cell_id)\"\n",
    "model7 = smf.glm(formula=formula7, data=df_maize, family=sm.families.Gamma(link=sm.families.links.log())).fit()\n",
    "print(\"Champion model (Model 7) has been fitted.\")\n",
    "\n",
    "\n",
    "# --- 2. Define Scenarios and Prediction Grid ---\n",
    "dry_jul = df_maize['precipitation_Jul'].quantile(0.10)\n",
    "avg_jul = df_maize['precipitation_Jul'].quantile(0.50)\n",
    "wet_jul = df_maize['precipitation_Jul'].quantile(0.90)\n",
    "\n",
    "scenarios = {\n",
    "    f'Dry July ({dry_jul:.1f} mm)': dry_jul,\n",
    "    f'Average July ({avg_jul:.1f} mm)': avg_jul,\n",
    "    f'Wet July ({wet_jul:.1f} mm)': wet_jul\n",
    "}\n",
    "colors = {'Dry': 'orange', 'Average': 'blue', 'Wet': 'green'}\n",
    "\n",
    "# Define the temperature range for plotting\n",
    "x_range_jul_temp = np.linspace(df_maize['temperature_Jul'].min(), df_maize['temperature_Jul'].max(), 200)\n",
    "\n",
    "# Create a base dataframe for predictions\n",
    "pred_df_base = pd.DataFrame({\n",
    "    'temperature_Jul': x_range_jul_temp,\n",
    "    'temperature_May': df_maize['temperature_May'].mean(),\n",
    "    'year': df_maize['year'].mean(),\n",
    "    'cell_id': 0\n",
    "})\n",
    "\n",
    "# --- VISUALIZATION 1: SINGLE PLOT WITH CONFIDENCE INTERVALS ---\n",
    "print(\"\\nGenerating single plot with transparent confidence intervals...\")\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "plt.scatter(df_maize['temperature_Jul'], df_maize['yield_maize'], alpha=0.1, label='Raw Data Points', color='gray')\n",
    "\n",
    "for label, precip_val in scenarios.items():\n",
    "    pred_df = pred_df_base.copy()\n",
    "    pred_df['precipitation_Jul'] = precip_val\n",
    "    \n",
    "    preds = model7.get_prediction(pred_df)\n",
    "    pred_summary = preds.summary_frame(alpha=0.05)\n",
    "    \n",
    "    color_key = label.split(' ')[0]\n",
    "    plt.plot(x_range_jul_temp, pred_summary['mean'], color=colors[color_key], linewidth=3, label=f'Curve for a {label}')\n",
    "    plt.fill_between(x_range_jul_temp, pred_summary['mean_ci_lower'], pred_summary['mean_ci_upper'], color=colors[color_key], alpha=0.15)\n",
    "\n",
    "plt.title('Maize Vulnerability to July Heat is Buffered by July Precipitation', fontsize=18)\n",
    "plt.xlabel('Average July Temperature (C)', fontsize=14)\n",
    "plt.ylabel('Predicted Maize Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylim(bottom=5, top=30) # Set a realistic Y-axis\n",
    "plt.xlim(right=26)        # IMPORTANT: Limit X-axis to avoid showing extrapolation\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Cell: REFINED Facet Plot with Adjusted Y-Axis\n",
    "\n",
    "print(\"\\n--- Generating REFINED faceted plot with adjusted axes ---\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15), sharex=True) # Removed sharey=True to set manually\n",
    "fig.suptitle('Maize Vulnerability Curves by July Precipitation Scenario', fontsize=18, y=0.93)\n",
    "\n",
    "for i, (label, precip_val) in enumerate(scenarios.items()):\n",
    "    ax = axes[i]\n",
    "    color_key = label.split(' ')[0]\n",
    "\n",
    "    # Plot raw data on each subplot for context\n",
    "    ax.scatter(df_maize['temperature_Jul'], df_maize['yield_maize'], alpha=0.1, color='gray')\n",
    "\n",
    "    # Create predictions for this specific scenario\n",
    "    pred_df = pred_df_base.copy()\n",
    "    pred_df['precipitation_Jul'] = precip_val\n",
    "    preds = model7.get_prediction(pred_df)\n",
    "    pred_summary = preds.summary_frame(alpha=0.05)\n",
    "    \n",
    "    # Plot the line and confidence interval\n",
    "    ax.plot(x_range_jul_temp, pred_summary['mean'], color=colors[color_key], linewidth=3, label='Mean Prediction')\n",
    "    ax.fill_between(x_range_jul_temp, pred_summary['mean_ci_lower'], pred_summary['mean_ci_upper'], color=colors[color_key], alpha=0.2, label='95% Confidence Interval')\n",
    "    \n",
    "    ax.set_title(f'Scenario: {label}', fontsize=14)\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    # --- KEY IMPROVEMENT ---\n",
    "    # Set the axis limits to a more meaningful range\n",
    "    ax.set_xlim(df_maize['temperature_Jul'].min(), 26) # Focus on the data-rich region\n",
    "    ax.set_ylim(5, 30)                               # Adjust the Y-axis as you suggested!\n",
    "\n",
    "# Common labels\n",
    "fig.text(0.5, 0.06, 'Average July Temperature (C)', ha='center', va='center', fontsize=14)\n",
    "fig.text(0.06, 0.5, 'Predicted Maize Yield (tonnes per hectare)', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "plt.tight_layout(rect=[0.08, 0.08, 1, 0.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb80fd",
   "metadata": {},
   "source": [
    "The final model reveals a complex, non-linear relationship between July temperature and maize yield. Yields are highest at cooler July temperatures and decrease as heat stress intensifies, reaching a point of maximum stress (lowest predicted yield) around 23-25C. The model's key finding is a statistically significant interaction with precipitation: while the yield difference between wet and dry years is minimal at cooler temperatures, a clear separation emerges under high heat stress. For example, at a stressful 24C, a wet July (182 mm) is predicted to yield approximately 1 tonne per hectare more than a dry July (28 mm), demonstrating that adequate rainfall is a key resilience factor that mitigates the most damaging effects of extreme heat. The widening confidence intervals at the temperature extremes responsibly highlight that the model's predictions are most certain within the core 12-24C range where the historical data is most dense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed36fb1",
   "metadata": {},
   "source": [
    "\n",
    "### **Summary of Progress: Maize Vulnerability Model**\n",
    "\n",
    "**Our Goal:** To move beyond our initial baseline OLS models and develop a more statistically robust, interpretable model to quantify the impact of climate stressors on maize yield in Northern Italy.\n",
    "\n",
    "---\n",
    "\n",
    "### **What We Did & Why (Our Process)**\n",
    "\n",
    "*   **Selected a Better Model:** We moved from a standard linear model (OLS) to a **Gamma Generalized Linear Model (GLM)**.\n",
    "    *   **Why:** A Gamma GLM is statistically more appropriate for crop yield data, which is always positive and often has a skewed distribution.\n",
    "\n",
    "*   **Refined the Target Variable:** We switched from using the detrended `yield_anomaly` to using the raw `yield_maize` as our outcome. We then added `year` as a predictor in the model.\n",
    "    *   **Why:** This allows us to use the Gamma GLM (which requires positive values) while still controlling for long-term trends like improvements in technology and management practices.\n",
    "\n",
    "*   **Systematically Screened for Key Stressors:** Instead of guessing, we ran a loop to test every single climate variable (both seasonal and monthly) in its own simple GLM.\n",
    "    *   **Why:** To objectively rank every stressor based on its statistical power (using the AIC metric) and find the most important drivers of yield *after* accounting for trends and location.\n",
    "\n",
    "*   **Built the Best Model Step-by-Step:** We used a \"forward selection\" approach, starting with our best single predictor and carefully adding new variables.\n",
    "    *   **Why:** This avoids the statistical pitfalls of multicollinearity (using redundant variables) and ensures our final model is both powerful and easy to interpret.\n",
    "\n",
    "---\n",
    "\n",
    "### **What We Found (Our Results for Maize)**\n",
    "\n",
    "*   **Key Stressors Identified:** The screening process confirmed our initial hypothesis: **July temperature** is the single most powerful predictor of maize yield. However, it also uncovered a new, highly significant factor: **May temperature**.\n",
    "\n",
    "*   **The Main Finding - Complex Interactions:** Our best model(Model 7) revealed two critical interaction effects:\n",
    "    1.  **Spring Conditions Matter:** We found a significant interaction between May and July temperatures. The model shows that a **warm May makes maize more resilient** to the negative impacts of heat stress later in July.\n",
    "    2.  **Rainfall is a Buffer:** We found a significant interaction between July temperature and July precipitation. The model demonstrates that adequate rainfall **mitigates the most severe damage** caused by extreme July heat.\n",
    "\n",
    "*   **Non-Linear Damage Confirmed:** The model uses splines to flexibly fit the data, confirming that the negative impact of heat is non-linearthe damage **accelerates as temperatures get hotter**, especially under dry conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps (Our Plan Forward)**\n",
    "\n",
    "Our successful workflow for maize now serves as the blueprint for the rest of the project.\n",
    "\n",
    "*   **1. Finalize Maize:** We have now established our current best model and a robust analytical process for maize.\n",
    "\n",
    "*   **2. Extend to Other Crops:** Our immediate next step is to **apply this exact same process** to the other four crops:\n",
    "    *   Run the stressor screening for **Rice**, then **Soybean**, **Spring Wheat**, and **Winter Wheat**.\n",
    "    *   Build the best multivariate, interactive model for each one.\n",
    "\n",
    "*   **3. Synthesize and Compare:** The final goal is to bring all the results together. We will create summary tables and comparative plots to answer the core research question: **How do the specific climate vulnerabilities and resilience factors differ across the major crops of Northern Italy?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 37-maize-gamma-models-vol2.ipynb/37-maize-gamma-models-vol2.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b236669b",
   "metadata": {},
   "source": [
    "## Updated GLM models\n",
    "\n",
    "This notebook focuses on adressing comments we recieved on our gamma models.\n",
    "\n",
    "The main strokes of the comments were the following: \n",
    "\n",
    "When we used C(cell_id) in our previous attempt, we are creating a seperate parameter for every single grid cell. This is a fixed effect approach, which works, but it is using a lot of the models power to describe baseline differences. So instead of treating each cell as a unique, independant category, we want to try modeling the spatial variation as a smooth surface using bsplines for the lat and lon columns. This is essentially us saying that yields don't just randomly vary from cell to cell; they vary smoothly across latitude and longitude. A cell in the Po Valley will have a similar baseline yield to its immediate neighbor, and this effect will gradually change as you move towards the mountains. \n",
    "\n",
    "We should focus on using splines for things we want to control for, and not our explanitory variables. Our inital model, we added a bspline to our tempature variable. This adds a lot of flexability and makes the coefciients hard to interpret. So in this attempt, we would use splines on things like lat and long, while for temperature_Jul (and other stressors), go back to using parametric functions that have a clear mathematical form.\n",
    "\n",
    "Our earlier attmept added stressors one by one using a form of forward selection. However, we got the suggestion that we should start by having all the stressors, and drop one out at a time. This approach is a form of \"backward elimination\" grouped by stressor type. We will start with a \"full\" model containing all plausible monthly stressors. Then, we will systematically remove one entire group of stressors at a time (e.g., all 5 monthly temperature variables) and see how much the model's performance (AIC) suffers. The group whose removal hurts the model the most is the most important group of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370741a",
   "metadata": {},
   "source": [
    "### **Our New, Refined Plan of Attack**\n",
    "\n",
    "This is a significant but very positive pivot. Here is the new, improved workflow based on our advisor's guidance.\n",
    "\n",
    "**Step 1: The New \"Full\" Model Definition**\n",
    "\n",
    "*   First, we need to define our comprehensive starting model.\n",
    "*   **Formula:** `yield_maize ~ year + [ALL_MONTHLY_STRESSORS] + bs(lat, df=4) + bs(lon, df=4)`\n",
    "    *   `[ALL_MONTHLY_STRESSORS]` will include all monthly temperature, precipitation, and soil water variables for the maize growing season (e.g., `temperature_May`, `temperature_Jun`... `precipitation_May`, etc.).\n",
    "\n",
    "**Step 2: Leave-One-Group-Out (LOGO) Feature Selection**\n",
    "\n",
    "*   We will fit the \"Full\" model from Step 1 and record its AIC as our baseline.\n",
    "*   Then, we will create a loop:\n",
    "    1.  **Remove Temperature Group:** Fit the model again without *any* of the `temperature_*` variables. Record the new, higher AIC.\n",
    "    2.  **Remove Precipitation Group:** Go back to the full model, but this time remove all `precipitation_*` variables. Record the AIC.\n",
    "    3.  **Remove Soil Water Group:** Repeat for `soil_water_*`.\n",
    "*   **Outcome:** We will have a clear ranking. The stressor group that causes the biggest *increase* in AIC when removed is the most important one.\n",
    "\n",
    "**Step 3: Build and Refine the Final Explanatory Model**\n",
    "\n",
    "*   Using the most important stressor group(s) identified in Step 2, we will build our final, parsimonious model.\n",
    "*   This is where we follow the advisor's other key pieces of advice:\n",
    "    *   **Test Parametric Non-Linearity:** For the most important single variable (e.g., `temperature_Jul`), we will test different functional forms like quadratic `(temp + I(temp**2))`, logarithmic `(log(temp))`, etc., and use AIC to choose the best one.\n",
    "    *   **Test Interactions:** We will explicitly test the `temperature_Jul * temperature_May` interaction, as it was a strong signal in our previous work.\n",
    "    *   **The new model idea your advisor gave is a perfect starting point for this step:** `yield_maize ~ year + temperature_Jul * temperature_May + bs(lat, df=4) + bs(lon, df=4)`\n",
    "\n",
    "**Step 4: Extend to Other Crops & Synthesize**\n",
    "\n",
    "*   Once we have this new, highly robust workflow finalized for maize, we will then apply the *exact same process* (Steps 1-3) to the other four crops.\n",
    "*   This will give you an even stronger and more directly comparable set of final results for your presentation.\n",
    "\n",
    "This is an excellent research plan. It's more rigorous, addresses potential issues with our previous approach, and will lead to conclusions you can be extremely confident in.\n",
    "\n",
    "How does this new plan sound to you? If you're on board, we can start by writing the code for **Step 1 and 2: Defining the full model and implementing the LOGO feature selection loop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac878313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and warning:\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Suppress convergence warnings that might arise during model fitting\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 1 & 2: LOGO Feature Selection for Maize ---\")\n",
    "\n",
    "# --- Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "    print(f\"Successfully loaded and filtered dataset from: {file_path}\")\n",
    "\n",
    "    # --- Step 1a: Define Core Growing Season and Filter Variables ---\n",
    "    # For Maize, the key months are May through September.\n",
    "    core_months = ['May', 'Jun', 'Jul', 'Aug', 'Sep']\n",
    "    \n",
    "    # Get all monthly stressors from the dataframe\n",
    "    all_monthly_stressors = [col for col in df_maize.columns if '_' in col and 'yield' not in col]\n",
    "    \n",
    "    # Filter the list to include ONLY those from our core months\n",
    "    filtered_stressors = []\n",
    "    for col in all_monthly_stressors:\n",
    "        month = col.split('_')[-1]\n",
    "        if month in core_months:\n",
    "            filtered_stressors.append(col)\n",
    "            \n",
    "    # Now, group the filtered stressors\n",
    "    stressor_groups = defaultdict(list)\n",
    "    for stressor in filtered_stressors:\n",
    "        group_name = stressor.split('_')[0]\n",
    "        stressor_groups[group_name].append(stressor)\n",
    "\n",
    "    print(\"\\nStandardized groups of monthly stressors (May-Sep):\")\n",
    "    for group, variables in stressor_groups.items():\n",
    "        print(f\"- {group.capitalize()}: {len(variables)} variables\")\n",
    "\n",
    "    # --- Step 1b: Define and Fit the \"Full\" Model with Filtered Stressors ---\n",
    "    all_stressors_str = ' + '.join(filtered_stressors)\n",
    "    formula_full = f\"yield_maize ~ year + {all_stressors_str} + bs(lat, df=4) + bs(lon, df=4)\"\n",
    "    \n",
    "    print(\"\\nFitting the 'Full' model with STANDARDIZED stressors...\")\n",
    "    model_full = smf.glm(\n",
    "        formula=formula_full,\n",
    "        data=df_maize,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    ).fit()\n",
    "    \n",
    "    aic_full = model_full.aic\n",
    "    print(f\"--- Baseline AIC of the Full Model: {aic_full:,.2f} ---\")\n",
    "\n",
    "    # --- Step 2: Leave-One-Group-Out (LOGO) Feature Selection Loop ---\n",
    "    print(\"\\nStarting LOGO feature selection...\")\n",
    "    logo_results = []\n",
    "\n",
    "    for group_to_drop, vars_in_group in stressor_groups.items():\n",
    "        print(f\"  Dropping group: {group_to_drop}...\")\n",
    "        \n",
    "        predictors_reduced = [v for v in filtered_stressors if v not in vars_in_group]\n",
    "        predictors_reduced_str = ' + '.join(predictors_reduced)\n",
    "        \n",
    "        formula_reduced = f\"yield_maize ~ year + {predictors_reduced_str} + bs(lat, df=4) + bs(lon, df=4)\"\n",
    "        \n",
    "        model_reduced = smf.glm(\n",
    "            formula=formula_reduced,\n",
    "            data=df_maize,\n",
    "            family=sm.families.Gamma(link=sm.families.links.log())\n",
    "        ).fit()\n",
    "        \n",
    "        logo_results.append({\n",
    "            'Dropped_Group': group_to_drop.capitalize(),\n",
    "            'AIC_of_Reduced_Model': model_reduced.aic\n",
    "        })\n",
    "\n",
    "    # --- Analyze and Display LOGO Results ---\n",
    "    results_df = pd.DataFrame(logo_results)\n",
    "    results_df['Increase_in_AIC'] = results_df['AIC_of_Reduced_Model'] - aic_full\n",
    "    results_df = results_df.sort_values(by='Increase_in_AIC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n--- Importance of Stressor Groups (Ranked by AIC Increase) ---\")\n",
    "    print(results_df.to_string(formatters={\n",
    "        'AIC_of_Reduced_Model': '{:,.2f}'.format,\n",
    "        'Increase_in_AIC': '{:,.2f}'.format\n",
    "    }))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2794d9c",
   "metadata": {},
   "source": [
    "### Analysis of some of the AIC changes\n",
    "\n",
    "The one that looks most suprizing is the decrease seen in the solar group. This is usually a sign of multicolinearity, and I want to try and test that just to verify. \n",
    "\n",
    "VIF is the standard way to diagnose multicollinearity. For each predictor variable, it calculates a score that measures how much its variance is \"inflated\" by its relationship with all the other predictors in the model.\n",
    "\n",
    "How to interpret the score:\n",
    "- VIF = 1: Not correlated with any other predictors.\n",
    "- VIF between 1 and 5: Generally considered acceptable.\n",
    "- VIF > 5 or 10: Indicates high multicollinearity that can be problematic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7936b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Calculating VIF to Diagnose Multicollinearity in the Full Model ---\")\n",
    "\n",
    "try:\n",
    "    # --- 1. Prepare Data and Formula (as before) ---\n",
    "    file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "\n",
    "    core_months = ['May', 'Jun', 'Jul', 'Aug', 'Sep']\n",
    "    all_monthly_stressors = [col for col in df_maize.columns if '_' in col and 'yield' not in col]\n",
    "    filtered_stressors = [col for col in all_monthly_stressors if col.split('_')[-1] in core_months]\n",
    "    all_stressors_str = ' + '.join(filtered_stressors)\n",
    "    \n",
    "    formula_full = f\"yield_maize ~ year + {all_stressors_str} + bs(lat, df=4) + bs(lon, df=4)\"\n",
    "\n",
    "    # --- 2. Create the Design Matrix ---\n",
    "    # This is a necessary step to prepare the data for the VIF calculation\n",
    "    print(\"Creating design matrix from formula...\")\n",
    "    y, X = dmatrices(formula_full, data=df_maize, return_type='dataframe')\n",
    "    print(\"Matrix created. Calculating VIF for each predictor...\")\n",
    "\n",
    "    # --- 3. Calculate VIF for each variable ---\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = X.columns\n",
    "    # The [1:] is to exclude the Intercept, which always has a high VIF\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "\n",
    "    # Filter out the spline terms for a cleaner summary, as they are complex by nature\n",
    "    vif_summary = vif_data[~vif_data['Variable'].str.contains('bs|Intercept|C\\(')]\n",
    "    \n",
    "    # Sort by VIF to see the most problematic variables\n",
    "    vif_summary = vif_summary.sort_values(by='VIF', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n--- Variance Inflation Factor (VIF) Results ---\")\n",
    "    print(\"Rule of thumb: VIF > 5 is a concern, VIF > 10 is a serious problem.\")\n",
    "    print(vif_summary.to_string())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ce144",
   "metadata": {},
   "source": [
    "### VIF Scores Analysis\n",
    "\n",
    "This table is a map of the redundant information in your dataset.\n",
    "\n",
    "*   **Extreme Multicollinearity (VIF > 20):**\n",
    "    *   `temperature_Aug` (36.4), `temperature_Jun` (32.2), `temperature_May` (21.1)\n",
    "    *   All `potential_evaporation` variables (ranging from 20 to 28)\n",
    "    *   **The Culprit:** These variables are all different ways of measuring the same underlying phenomenon: **heat**. Potential evaporation is a direct physical consequence of temperature and solar energy. The high VIF scores confirm they are so tightly interlinked that the model cannot distinguish their individual effects. It's statistically impossible to interpret the coefficient for `temperature_Aug` when `potential_evaporation_Aug` is also in the model.\n",
    "\n",
    "*   **Serious Multicollinearity (VIF 10-20):**\n",
    "    *   `temperature_Jul` (19.9), `temperature_Sep` (12.6)\n",
    "    *   Most `soil_water` variables (July, Aug, Jun)\n",
    "    *   Some `solar_radiation` variables (May, Aug, Jun)\n",
    "    *   **The Story:** This is the next layer of redundancy. Soil water is clearly a function of heat (evaporation) and rain. Solar radiation is linked to heat. The model is telling us that once it knows the temperature and precipitation, it already has a very good idea of what the soil water and solar radiation values will be.\n",
    "\n",
    "*   **Acceptable / Independent Variables (VIF < 10):**\n",
    "    *   Most `precipitation` variables.\n",
    "    *   `year`.\n",
    "    *   **The Insight:** This is fantastic news. It shows that **precipitation provides unique information** that is *not* captured by the other variables. And `year` is, as expected, independent of the annual weather fluctuations.\n",
    "\n",
    "\n",
    "### Justification for Our New Modeling Strategy\n",
    "\n",
    "Based on this VIF analysis and the LOGO results, we can now justify our final modeling plan with very strong evidence. \n",
    "\n",
    "**Our Justification:**\n",
    "The initial LOGO analysis showed that the `Temperature` group was the most important predictor, while the `Solar` group was redundant. A subsequent VIF analysis on the full model confirmed severe multicollinearity among the heat-related variables. Specifically, all `potential_evaporation` variables and most `temperature` and `solar_radiation` variables showed VIF scores far exceeding the problematic threshold of 10. This indicates that these variables carry overlapping information, making their individual coefficients unstable and uninterpretable. In contrast, the `precipitation` variables showed low VIF scores, indicating they provide unique explanatory power.\n",
    "\n",
    "Therefore, to build a parsimonious and robust explanatory model, we will proceed by:\n",
    "1.  **Selecting one primary heat variable** (e.g., `temperature_Jul`, based on our previous findings) to represent the dominant effect of temperature stress.\n",
    "2.  **Excluding** the other highly collinear heat-related groups (`potential_evaporation`, `solar_radiation`, and other monthly temperatures) as separate, independent main effects to ensure model stability.\n",
    "3.  **Including precipitation** as a key secondary stressor and testing its interaction with temperature to model the critical heat-drought effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb907bcf",
   "metadata": {},
   "source": [
    "### Building the Final Explanatory Model\n",
    "We are now ready to put this all together and build our champion model. Based on all the evidence we've gathered, the model your advisor initially suggested is the perfect place to start. It embodies all of these principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b646c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Building the Final Explanatory Model ---\")\n",
    "\n",
    "# --- Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "df_maize = pd.read_csv(file_path)\n",
    "df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "print(\"Data prepared.\")\n",
    "\n",
    "# --- Model A ---\n",
    "# This is our new, robust baseline. It controls for space with splines,\n",
    "# includes the year trend, and tests the interaction between our two\n",
    "# most promising, non-collinear temperature variables.\n",
    "print(\"\\nFitting Model A: The primary interaction model...\")\n",
    "formula_A = \"yield_maize ~ year + temperature_Jul + temperature_May + temperature_Jun + temperature_May:temperature_Jun + temperature_Jun:temperature_Jul + temperature_Jul:temperature_May:temperature_Jun + bs(lat, df=4) + bs(lon, df=4)\"\n",
    "model_A = smf.glm(\n",
    "    formula=formula_A,\n",
    "    data=df_maize,\n",
    "    family=sm.families.Gamma(link=sm.families.links.log())\n",
    ").fit_regularized(refit = True)\n",
    "print(model_A.summary())\n",
    "\n",
    "# --- Model B: Adding the Heat x Drought Interaction ---\n",
    "# Now we add the interaction with precipitation, which our VIF analysis\n",
    "# showed provides unique information.\n",
    "print(\"\\nFitting Model B: Adding the heat x drought interaction...\")\n",
    "# Note: We use quadratic for temperature now, as per the plan to use parametric forms.\n",
    "# We also test a quadratic for precipitation to allow for a non-linear water response.\n",
    "formula_B = \"yield_maize ~ year + temperature_Jul + temperature_May + temperature_Jun + temperature_May:temperature_Jun + temperature_Jun:temperature_Jul + temperature_Jul:temperature_May:temperature_Jun + precipitation_Jul + precipitation_May + precipitation_Jun + bs(lat, df=4) + bs(lon, df=4)\"\n",
    "model_B = smf.glm(\n",
    "    formula=formula_B,\n",
    "    data=df_maize,\n",
    "    family=sm.families.Gamma(link=sm.families.links.log())\n",
    ").fit_regularized(refit = True)\n",
    "print(model_B.summary())\n",
    "\n",
    "# --- Final AIC Comparison ---\n",
    "print(\"\\n--- Final Model Comparison ---\")\n",
    "print(f\"Model A (Temp*Temp) AIC:        {model_A.aic:,.2f}\")\n",
    "print(f\"Model B (Added Precip & Interactions) AIC: {model_B.aic:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_A.__dir__()\n",
    "model_A.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248be22",
   "metadata": {},
   "source": [
    "### Analysis of the Final Models\n",
    "\n",
    "This output gives us a very clear winner.\n",
    "\n",
    "#### Model Comparison:\n",
    "*   **Model A AIC:** 5,433.84\n",
    "*   **Model B AIC:** 5,406.88\n",
    "\n",
    "The AIC for **Model B is substantially lower** (a drop of ~27 points), which is a huge and decisive improvement. This tells us that incorporating the precipitation variables and the heat x drought interaction has made our model significantly better. **Model B is our new champion.**\n",
    "\n",
    "#### Interpreting the Champion (Model B):\n",
    "\n",
    "Let's look at the coefficients in the Model B summary.\n",
    "\n",
    "*   **`year`:** Still positive and highly significant. The trend is robust.\n",
    "*   **`temperature_May`:** `coef=0.1001`, `p=0.000`. This effect is still here and is very strong. A warmer May is associated with higher yields.\n",
    "*   **`temperature_Jul * temperature_May` interaction:** `coef=-0.0051`, `p=0.000`. The interaction is also still highly significant and negative. This confirms our previous finding: the damaging effect of July heat is *stronger* (more negative) when May is also warmer. *(This is a slight refinement of our previous interpretation - we'll need to re-visualize to be sure).*\n",
    "\n",
    "**Now for the new terms:**\n",
    "\n",
    "*   **`precipitation_Jul`:** `coef=0.0034`, `p=0.000`. The main effect of rain is now **positive and highly significant**. This is great! In this more correctly specified model, we see that, on average, more rain in July is good for yield.\n",
    "*   **`I(precipitation_Jul ** 2)`:** `coef=-3.823e-06`, `p=0.000`. This is the quadratic term for precipitation. It's significant and negative. This tells us the relationship with rain is non-linear: the benefit of rain is positive, but there are diminishing returns. Too much rain eventually leads to a slight decrease in the benefit (a classic \"hump shape\").\n",
    "*   **`temperature_Jul:precipitation_Jul`:** `coef=-0.0001`, `p=0.000`. This is the heat x drought interaction. It's **negative and highly significant.** This is the buffering effect! The negative coefficient means that as precipitation increases, it counteracts the positive coefficient on `temperature_Jul`, making the overall effect of heat less damaging.\n",
    "\n",
    "### A New, More Complete Story\n",
    "\n",
    "We can now tell an even richer story than before:\n",
    "\n",
    "1.  A warm start to the season (`temperature_May`) is beneficial for setting a high yield potential.\n",
    "2.  However, this creates a \"live fast, die young\" scenario, as the negative interaction term shows these high-potential crops are then *more sensitive* to the heat that comes in July.\n",
    "3.  The damage from July heat is itself a complex function, moderated by rainfall. The negative interaction between `temperature_Jul` and `precipitation_Jul` proves that adequate rainfall acts as a buffer, mitigating the damage from high temperatures.\n",
    "4.  The relationship with rainfall itself is non-linear, with diminishing returns at very high levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d6165",
   "metadata": {},
   "source": [
    "### Visualizing our temp interraction and our heat x drought interraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Final Visualizations for the Champion Maize Model (Corrected) ---\")\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "df_maize = pd.read_csv(file_path)\n",
    "df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "print(\"Data prepared successfully.\")\n",
    "\n",
    "# --- 2. Create the Design Matrix with Splines ---\n",
    "# This is the new, robust way to create our predictors.\n",
    "# We define a formula with all our non-stressor variables.\n",
    "design_formula = \"yield_maize ~ year + bs(lat, df=4) + bs(lon, df=4)\"\n",
    "\n",
    "# dmatrices creates the outcome (y) and predictor (X) dataframes\n",
    "y, X = dmatrices(design_formula, data=df_maize, return_type='dataframe')\n",
    "\n",
    "# Now, we combine our original dataframe with the newly created spline columns from X\n",
    "df_model_ready = pd.concat([df_maize, X.drop(columns=['Intercept', 'year'])], axis=1)\n",
    "\n",
    "\n",
    "# --- 3. Fit the Best Model (Model B) using the new data ---\n",
    "# We build the formula string by hand now, which is safer for prediction.\n",
    "# First, get the names of the spline columns that were just created.\n",
    "spline_cols = [col for col in X.columns if 'bs(' in col]\n",
    "spline_str = ' + '.join(spline_cols)\n",
    "\n",
    "# Our final champion model formula, now using the pre-built spline columns\n",
    "formula_B = f\"yield_maize ~ year + temperature_Jul * temperature_May + I(temperature_Jul**2) + I(precipitation_Jul**2) + temperature_Jul*precipitation_Jul + {spline_str}\"\n",
    "\n",
    "print(\"Fitting Champion model (Model B)...\")\n",
    "model_B = smf.glm(\n",
    "    formula=formula_B,\n",
    "    data=df_model_ready,\n",
    "    family=sm.families.Gamma(link=sm.families.links.log())\n",
    ").fit()\n",
    "print(\"Model fitted successfully.\")\n",
    "\n",
    "\n",
    "# --- VISUALIZATION 1: The Temperature Interaction (Temp_Jul * Temp_May) ---\n",
    "# (This section remains largely the same, but uses the new model object)\n",
    "print(\"\\nGenerating plot 1: The interaction between May and July temperatures...\")\n",
    "\n",
    "cool_may = df_maize['temperature_May'].quantile(0.10)\n",
    "avg_may = df_maize['temperature_May'].quantile(0.50)\n",
    "warm_may = df_maize['temperature_May'].quantile(0.90)\n",
    "temp_scenarios = {f'Cool May ({cool_may:.1f}C)': cool_may, f'Average May ({avg_may:.1f}C)': avg_may, f'Warm May ({warm_may:.1f}C)': warm_may}\n",
    "colors = {'Cool': 'blue', 'Average': 'green', 'Warm': 'red'}\n",
    "\n",
    "x_range_jul = np.linspace(df_maize['temperature_Jul'].min(), df_maize['temperature_Jul'].max(), 200)\n",
    "pred_df_temp_base = pd.DataFrame({\n",
    "    'temperature_Jul': x_range_jul,\n",
    "    'precipitation_Jul': df_maize['precipitation_Jul'].mean(),\n",
    "    'year': df_maize['year'].mean(),\n",
    "    'lat': df_maize['lat'].mean(), # Use mean lat/lon for spline prediction\n",
    "    'lon': df_maize['lon'].mean()\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "plt.scatter(df_maize['temperature_Jul'], df_maize['yield_maize'], alpha=0.1, label='Raw Data Points', color='gray')\n",
    "\n",
    "for label, may_temp in temp_scenarios.items():\n",
    "    pred_df = pred_df_temp_base.copy()\n",
    "    pred_df['temperature_May'] = may_temp\n",
    "    \n",
    "    preds = model_B.get_prediction(pred_df)\n",
    "    pred_summary = preds.summary_frame(alpha=0.05)\n",
    "    \n",
    "    color_key = label.split(' ')[0]\n",
    "    plt.plot(x_range_jul, pred_summary['mean'], color=colors[color_key], linewidth=3, label=f'Curve for a {label}')\n",
    "    plt.fill_between(x_range_jul, pred_summary['mean_ci_lower'], pred_summary['mean_ci_upper'], color=colors[color_key], alpha=0.15)\n",
    "\n",
    "plt.title('Vulnerability to July Heat Depends on May Temperature', fontsize=18)\n",
    "plt.xlabel('Average July Temperature (C)', fontsize=14)\n",
    "plt.ylabel('Predicted Maize Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylim(5, 30); plt.xlim(right=26)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- VISUALIZATION 2: The Heat x Drought Interaction (Facet Plot) ---\n",
    "# (This section also remains the same, just using the new model object)\n",
    "print(\"\\nGenerating plot 2: The interaction between July temperature and precipitation...\")\n",
    "\n",
    "dry_jul = df_maize['precipitation_Jul'].quantile(0.10)\n",
    "avg_jul = df_maize['precipitation_Jul'].quantile(0.50)\n",
    "wet_jul = df_maize['precipitation_Jul'].quantile(0.90)\n",
    "precip_scenarios = {f'Dry July ({dry_jul:.1f} mm)': dry_jul, f'Average July ({avg_jul:.1f} mm)': avg_jul, f'Wet July ({wet_jul:.1f} mm)': wet_jul}\n",
    "precip_colors = {'Dry': 'orange', 'Average': 'blue', 'Wet': 'green'}\n",
    "\n",
    "pred_df_precip_base = pd.DataFrame({\n",
    "    'temperature_Jul': x_range_jul,\n",
    "    'temperature_May': df_maize['temperature_May'].mean(),\n",
    "    'year': df_maize['year'].mean(),\n",
    "    'lat': df_maize['lat'].mean(),\n",
    "    'lon': df_maize['lon'].mean()\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n",
    "fig.suptitle('Maize Vulnerability Curves by July Precipitation Scenario', fontsize=18, y=0.93)\n",
    "\n",
    "for i, (label, precip_val) in enumerate(precip_scenarios.items()):\n",
    "    ax = axes[i]\n",
    "    color_key = label.split(' ')[0]\n",
    "\n",
    "    ax.scatter(df_maize['temperature_Jul'], df_maize['yield_maize'], alpha=0.1, color='gray')\n",
    "\n",
    "    pred_df = pred_df_precip_base.copy()\n",
    "    pred_df['precipitation_Jul'] = precip_val\n",
    "    preds = model_B.get_prediction(pred_df)\n",
    "    pred_summary = preds.summary_frame(alpha=0.05)\n",
    "    \n",
    "    ax.plot(x_range_jul, pred_summary['mean'], color=precip_colors[color_key], linewidth=3, label='Mean Prediction')\n",
    "    ax.fill_between(x_range_jul, pred_summary['mean_ci_lower'], pred_summary['mean_ci_upper'], color=precip_colors[color_key], alpha=0.2, label='95% Confidence Interval')\n",
    "    \n",
    "    ax.set_title(f'Scenario: {label}', fontsize=14)\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    ax.set_xlim(df_maize['temperature_Jul'].min(), 26)\n",
    "    ax.set_ylim(5, 30)\n",
    "\n",
    "fig.text(0.5, 0.06, 'Average July Temperature (C)', ha='center', va='center', fontsize=14)\n",
    "fig.text(0.06, 0.5, 'Predicted Maize Yield (tonnes per hectare)', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "plt.tight_layout(rect=[0.08, 0.08, 1, 0.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee2490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Testing for the 'Inverted U' Shape in May Temperature ---\")\n",
    "\n",
    "# --- 1. Use the model-ready dataframe from the previous step ---\n",
    "# It contains the necessary spline columns for the controls.\n",
    "\n",
    "# --- 2. Fit a Specific Model to Isolate the May Temperature Effect ---\n",
    "# We use a quadratic term I(temperature_May**2) to allow for a hump shape.\n",
    "formula_may = f\"yield_maize ~ year + temperature_May + I(temperature_May**2) + {spline_str}\"\n",
    "model_may = smf.glm(\n",
    "    formula=formula_may,\n",
    "    data=df_model_ready,\n",
    "    family=sm.families.Gamma(link=sm.families.links.log())\n",
    ").fit()\n",
    "\n",
    "print(\"\\nModel Summary for May Temperature Effect:\")\n",
    "print(model_may.summary())\n",
    "\n",
    "\n",
    "# --- 3. Create a Prediction Grid for May Temperature ---\n",
    "x_range_may = np.linspace(df_maize['temperature_May'].min(), df_maize['temperature_May'].max(), 200)\n",
    "\n",
    "pred_df_may = pd.DataFrame({\n",
    "    'temperature_May': x_range_may,\n",
    "    'year': df_maize['year'].mean(),\n",
    "    'lat': df_maize['lat'].mean(),\n",
    "    'lon': df_maize['lon'].mean()\n",
    "})\n",
    "\n",
    "# --- 4. Generate Predictions and Confidence Intervals ---\n",
    "preds_may = model_may.get_prediction(pred_df_may)\n",
    "pred_summary_may = preds_may.summary_frame(alpha=0.05)\n",
    "\n",
    "# --- 5. Plot the Yield Response Curve for May ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the raw data (May temp vs. Yield)\n",
    "plt.scatter(df_maize['temperature_May'], df_maize['yield_maize'], alpha=0.1, color='gray', label='Raw Data Points')\n",
    "\n",
    "# Plot the mean prediction line\n",
    "plt.plot(x_range_may, pred_summary_may['mean'], color='purple', linewidth=3, label='Mean Predicted Yield (Quadratic Fit)')\n",
    "\n",
    "# Plot the confidence interval\n",
    "plt.fill_between(x_range_may, pred_summary_may['mean_ci_lower'], pred_summary_may['mean_ci_upper'], color='purple', alpha=0.2, label='95% Confidence Interval')\n",
    "\n",
    "plt.title('Maize Yield Response to May Temperature', fontsize=18)\n",
    "plt.xlabel('Average May Temperature (C)', fontsize=14)\n",
    "plt.ylabel('Predicted Maize Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e845e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Plotting the Average Yield Response Curve for July Temperature ---\")\n",
    "\n",
    "# --- 1. Ensure the Champion Model (Model B) is available ---\n",
    "# (Assuming Model B from the previous cells is in memory)\n",
    "\n",
    "# --- 2. Create a Prediction Grid for the \"Average\" Scenario ---\n",
    "x_range_jul = np.linspace(df_maize['temperature_Jul'].min(), df_maize['temperature_Jul'].max(), 200)\n",
    "\n",
    "pred_df_avg = pd.DataFrame({\n",
    "    'temperature_Jul': x_range_jul,\n",
    "    'temperature_May': df_maize['temperature_May'].mean(), # Hold constant at average\n",
    "    'precipitation_Jul': df_maize['precipitation_Jul'].mean(), # Hold constant at average\n",
    "    'year': df_maize['year'].mean(),\n",
    "    'lat': df_maize['lat'].mean(),\n",
    "    'lon': df_maize['lon'].mean()\n",
    "})\n",
    "\n",
    "# --- 3. Generate Predictions and Confidence Intervals ---\n",
    "preds_avg = model_B.get_prediction(pred_df_avg)\n",
    "pred_summary_avg = preds_avg.summary_frame(alpha=0.05)\n",
    "\n",
    "# --- 4. Plot the Yield Response Curve ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the raw data for context\n",
    "plt.scatter(df_maize['temperature_Jul'], df_maize['yield_maize'], alpha=0.1, color='gray', label='Raw Data Points')\n",
    "\n",
    "# Plot the mean prediction line\n",
    "plt.plot(x_range_jul, pred_summary_avg['mean'], color='blue', linewidth=3, label='Mean Predicted Yield (Average Scenario)')\n",
    "\n",
    "# Plot the confidence interval\n",
    "plt.fill_between(x_range_jul, pred_summary_avg['mean_ci_lower'], pred_summary_avg['mean_ci_upper'], color='blue', alpha=0.2, label='95% Confidence Interval')\n",
    "\n",
    "# --- Formatting ---\n",
    "plt.title('Maize Yield Response to July Temperature (Average Scenario)', fontsize=18)\n",
    "plt.xlabel('Average July Temperature (C)', fontsize=14)\n",
    "plt.ylabel('Predicted Maize Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylim(5, 30) # Use consistent, realistic axes\n",
    "plt.xlim(right=26)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276557b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Plotting the Average Yield Response Curve for July Temperature ---\")\n",
    "\n",
    "# --- 1. Ensure the Champion Model (Model B) is available ---\n",
    "# (Assuming Model B from the previous cells is in memory)\n",
    "\n",
    "# --- 2. Create a Prediction Grid for the \"Average\" Scenario ---\n",
    "x_range_jul = np.linspace(df_maize['temperature_Jul'].min(), df_maize['temperature_Jul'].max(), 200)\n",
    "\n",
    "pred_df_avg = pd.DataFrame({\n",
    "    'temperature_Jul': x_range_jul,\n",
    "    'temperature_May': df_maize['temperature_May'].mean(), # Hold constant at average\n",
    "    'precipitation_Jul': df_maize['precipitation_Jul'].mean(), # Hold constant at average\n",
    "    'year': df_maize['year'].mean(),\n",
    "    'lat': df_maize['lat'].mean(),\n",
    "    'lon': df_maize['lon'].mean()\n",
    "})\n",
    "\n",
    "# --- 3. Generate Predictions and Confidence Intervals ---\n",
    "preds_avg = model_B.get_prediction(pred_df_avg)\n",
    "pred_summary_avg = preds_avg.summary_frame(alpha=0.05)\n",
    "\n",
    "# --- 4. Plot the Yield Response Curve ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the raw data for context\n",
    "plt.scatter(df_maize['temperature_Jul'], df_maize['yield_maize'], alpha=0.1, color='gray', label='Raw Data Points')\n",
    "\n",
    "# Plot the mean prediction line\n",
    "plt.plot(x_range_jul, pred_summary_avg['mean'], color='blue', linewidth=3, label='Mean Predicted Yield (Average Scenario)')\n",
    "\n",
    "# Plot the confidence interval\n",
    "plt.fill_between(x_range_jul, pred_summary_avg['mean_ci_lower'], pred_summary_avg['mean_ci_upper'], color='blue', alpha=0.2, label='95% Confidence Interval')\n",
    "\n",
    "# --- Formatting ---\n",
    "plt.title('Maize Yield Response to July Temperature (Average Scenario)', fontsize=18)\n",
    "plt.xlabel('Average July Temperature (C)', fontsize=14)\n",
    "plt.ylabel('Predicted Maize Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylim(5, 30) # Use consistent, realistic axes\n",
    "plt.xlim(right=26)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Plotting the Yield Response Curve for July Precipitation ---\")\n",
    "\n",
    "# --- 1. Ensure the Champion Model (Model B) is available ---\n",
    "# (Assuming Model B from the previous cell is in memory)\n",
    "\n",
    "# --- 2. Create a Prediction Grid ---\n",
    "# This time, precipitation is our variable of interest on the x-axis.\n",
    "x_range_precip = np.linspace(df_maize['precipitation_Jul'].min(), df_maize['precipitation_Jul'].max(), 200)\n",
    "\n",
    "# We will hold the temperature stressors constant at their average values.\n",
    "pred_df_precip = pd.DataFrame({\n",
    "    'precipitation_Jul': x_range_precip,\n",
    "    'temperature_Jul': df_maize['temperature_Jul'].mean(), # Hold constant\n",
    "    'temperature_May': df_maize['temperature_May'].mean(), # Hold constant\n",
    "    'year': df_maize['year'].mean(),\n",
    "    'lat': df_maize['lat'].mean(),\n",
    "    'lon': df_maize['lon'].mean()\n",
    "})\n",
    "\n",
    "# --- 3. Generate Predictions and Confidence Intervals ---\n",
    "preds_precip = model_B.get_prediction(pred_df_precip)\n",
    "pred_summary_precip = preds_precip.summary_frame(alpha=0.05)\n",
    "\n",
    "# --- 4. Plot the Yield Response Curve ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the mean prediction line\n",
    "plt.plot(pred_df_precip['precipitation_Jul'], pred_summary_precip['mean'], color='dodgerblue', linewidth=3, label='Mean Predicted Yield')\n",
    "\n",
    "# Plot the confidence interval\n",
    "plt.fill_between(pred_df_precip['precipitation_Jul'], pred_summary_precip['mean_ci_lower'], pred_summary_precip['mean_ci_upper'], color='dodgerblue', alpha=0.2, label='95% Confidence Interval')\n",
    "\n",
    "# --- Formatting ---\n",
    "plt.title('Maize Yield Response to July Precipitation', fontsize=18)\n",
    "plt.xlabel('Average July Precipitation (mm)', fontsize=14)\n",
    "plt.ylabel('Predicted Maize Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8337cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Deriving Final Vulnerability Curves from the Champion Model ---\")\n",
    "\n",
    "# --- 1. Re-run the setup from the previous cell to ensure we have the model ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "df_maize = pd.read_csv(file_path)\n",
    "df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "\n",
    "design_formula = \"yield_maize ~ year + bs(lat, df=4) + bs(lon, df=4)\"\n",
    "y, X = dmatrices(design_formula, data=df_maize, return_type='dataframe')\n",
    "df_model_ready = pd.concat([df_maize, X.drop(columns=['Intercept', 'year'])], axis=1)\n",
    "\n",
    "spline_cols = [col for col in X.columns if 'bs(' in col]\n",
    "spline_str = ' + '.join(spline_cols)\n",
    "\n",
    "formula_B = f\"yield_maize ~ year + temperature_Jul * temperature_May + bs(lat, df=4) + bs(lon, df=4)\"\n",
    "model_B = smf.glm(formula=formula_B, data=df_model_ready, family=sm.families.Gamma(link=sm.families.links.log())).fit()\n",
    "\n",
    "# --- 2. Generate Predictions for the \"Average May, Average July Precip\" Scenario ---\n",
    "# This will be our reference curve.\n",
    "avg_may = df_maize['temperature_May'].min()\n",
    "avg_jul_precip = df_maize['precipitation_Jul'].mean()\n",
    "x_range_jul = np.linspace(df_maize['temperature_Jul'].min(), df_maize['temperature_Jul'].max(), 200)\n",
    "\n",
    "pred_df_ref = pd.DataFrame({\n",
    "    'temperature_Jul': x_range_jul,\n",
    "    'temperature_May': avg_may,\n",
    "    'precipitation_Jul': avg_jul_precip,\n",
    "    'year': df_maize['year'].mean(),\n",
    "    'lat': df_maize['lat'].mean(),\n",
    "    'lon': df_maize['lon'].mean()\n",
    "})\n",
    "y_predicted = model_B.predict(pred_df_ref)\n",
    "\n",
    "# --- 3. Transform Yield Response to a Vulnerability Curve ---\n",
    "# Step 1: Define Potential Yield. Let's find the max predicted yield on our curve.\n",
    "y_potential = y_predicted.max()\n",
    "print(f\"\\nCalculated Potential Yield (baseline for no stress): {y_potential:.2f} t/ha\")\n",
    "\n",
    "# Step 2: Calculate Absolute Loss\n",
    "absolute_loss = y_potential - y_predicted\n",
    "\n",
    "# Step 3: Calculate Percentage Loss\n",
    "percentage_loss = (absolute_loss / y_potential) * 100\n",
    "\n",
    "# --- 4. Plot the Final Vulnerability Curve ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(x_range_jul, percentage_loss, color='red', linewidth=3)\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.axhline(100, color='black', linestyle='--', label='100% Loss')\n",
    "\n",
    "# Formatting\n",
    "plt.title('Final Maize Vulnerability Curve for July Temperature', fontsize=18)\n",
    "plt.xlabel('Average July Temperature (C) [Intensity Variable]', fontsize=14)\n",
    "plt.ylabel('Predicted Crop Loss (%)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.ylim(-5, 105) # Give a little space around 0 and 100\n",
    "plt.xlim(df_maize['temperature_Jul'].min(), 26) # Focus on the data-rich region\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08700de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5cb2e",
   "metadata": {},
   "source": [
    "### what can we say abbout this analysis?\n",
    "\n",
    "Our data-driven vulnerability curve shows the percentage of maize yield lost due to increasing heat stress in July, relative to a cool, non-stressful baseline. The curve begins at 0% loss at low temperatures, not because these are optimal for growth, but because they represent a condition of zero heat stress. As the average July temperature rises, the damage from heat begins and accelerates, leading to a predicted yield loss of over 60% as temperatures approach 26C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0de709",
   "metadata": {},
   "source": [
    "Vulnerability to July Drought:\n",
    "X-Axis: precipitation_Jul (from low to high)\n",
    "Y-Axis: Predicted Crop Loss (%)\n",
    "How: You would ask the model the same kind of question: \"Holding temperature_Jul at a constant stressful level (e.g., 25C), show me how yield loss changes as precipitation increases.\" This would likely show a downward-sloping curve (more rain = less loss)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 38-maize-gamma-updated.ipynb/38-maize-gamma-updated.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f447aad1",
   "metadata": {},
   "source": [
    "# Maize Vulnerability Modeling: A Regularized GLM Approach\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this notebook is to develop a statistically robust and interpretable explanatory model for maize yield in Northern Italy. Following a thorough baseline analysis and advisor feedback, this new approach prioritizes rigorous model specification and selection over preliminary visualization.\n",
    "\n",
    "Our objectives are to:\n",
    "1.  Systematically identify the most important climate stressors from a large set of correlated monthly variables.\n",
    "2.  Use a regularized Gamma GLM to automatically perform variable selection and handle multicollinearity.\n",
    "3.  Carefully interpret the coefficients and interactions of the final \"champion\" model to build a scientifically sound narrative about maize vulnerability.\n",
    "4.  Refine the final model by testing for non-linear relationships using explicit, interpretable functions (e.g., quadratic terms).\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "To achieve these goals, we will employ a specific set of statistical techniques:\n",
    "\n",
    "### Why a Generalized Linear Model (GLM)?\n",
    "Standard linear models (OLS) assume that the model's errors are normally distributed and the outcome can be any real number. Crop yield, however, is a continuous variable that is always positive and often has a right-skewed distribution. A **Generalized Linear Model (GLM)** is a more flexible framework that allows us to choose a probability distribution that matches the nature of our outcome variable.\n",
    "\n",
    "### The Gamma Distribution\n",
    "For this analysis, we will use a **Gamma GLM**. The Gamma distribution is ideal for modeling continuous, strictly positive, and often skewed data like crop yield (measured in tonnes per hectare). We will use a `log` link function, which ensures that the model's predictions are always positive and helps in interpreting the coefficients as multiplicative (percentage) effects.\n",
    "\n",
    "### The Challenge of Multicollinearity\n",
    "Our dataset contains many monthly climate variables that are naturally correlated (e.g., `temperature_May` is correlated with `temperature_Jun`). When multiple predictors are highly correlated, a standard model struggles to disentangle their individual effects, leading to unstable and unreliable coefficient estimates. Our previous VIF analysis confirmed this was a significant issue.\n",
    "\n",
    "### The Solution: Regularization (Lasso - L1)\n",
    "Instead of manually dropping variables, which can be subjective, we will use **regularization**. This is a modern technique that adds a penalty to the model for having overly complex or large coefficients. We will specifically use the **L1 (Lasso) penalty**.\n",
    "\n",
    "*   **How it Works:** Lasso adds a penalty proportional to the absolute value of the coefficients. A key feature of this penalty is that it can force the coefficients of less important or redundant variables to become **exactly zero**.\n",
    "*   **Automatic Variable Selection:** This process effectively performs an automatic and objective form of variable selection, \"silencing\" the predictors that don't contribute enough unique information.\n",
    "*   **`fit_regularized(refit=True)`:** We will use the `.fit_regularized()` method in `statsmodels`. With `refit=True`, the process is twofold:\n",
    "    1.  **Selection:** The model is first fit with the Lasso penalty, which zeroes out a subset of coefficients.\n",
    "    2.  **Refitting:** The model then takes only the \"surviving\" variables (those with non-zero coefficients) and fits a final, standard, unpenalized GLM on just them. This provides clean, unbiased coefficients and valid p-values for the most important predictors.\n",
    "\n",
    "## Plan of Action\n",
    "\n",
    "This notebook will proceed in a structured, step-by-step manner:\n",
    "\n",
    "1.  **Setup & EDA:** We will load the crop-specific, growing-season-filtered maize dataset and import necessary libraries. Our primary EDA will be to compute and visualize a full pairwise correlation matrix of all monthly stressors to understand their relationships.\n",
    "\n",
    "2.  **Full Model Definition:** We will define a comprehensive \"full\" model that includes all relevant monthly temperature and precipitation variables, along with their key two- and three-way interactions. This model will also include the `year` trend and spatial splines (`bs(lat) + bs(lon)`) as controls.\n",
    "\n",
    "3.  **Regularized Model Fitting:** We will fit this complex model using `statsmodels`' `.fit_regularized()` method with an L1 (Lasso) penalty. This will automatically select the most impactful subset of variables and interactions.\n",
    "\n",
    "4.  **Analysis of the Champion Model:** We will carefully analyze the summary of the refitted \"champion\" model. We will interpret the coefficients, p-values, and interactions of the variables that \"survived\" the regularization process to build our primary narrative.\n",
    "\n",
    "5.  **Refinement with Non-Linear Terms:** Guided by agronomic logic and the results of the champion model, we will test if adding explicit non-linear functions (e.g., quadratic terms `I(variable**2)`) for the most important surviving stressors can further improve the model's fit and interpretation, using AIC for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787e1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis - Full Correlation Matrix\n",
    "\n",
    "print(\"--- EDA: Correlation Analysis of All Monthly Stressors ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# As confirmed, this file is already specific to maize and its growing season.\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Select Only the Monthly Stressor Variables ---\n",
    "    # We will select all columns that have a month name in them, which is a robust\n",
    "    # way to grab all the monthly predictors we want to investigate.\n",
    "    monthly_stressors = [col for col in df_maize.columns if '_' in col and 'yield' not in col]\n",
    "    df_corr = df_maize[monthly_stressors]\n",
    "    \n",
    "    print(f\"\\nSelected {len(df_corr.columns)} monthly stressor variables for correlation analysis.\")\n",
    "\n",
    "    # --- 3. Calculate and Print the Correlation Matrix ---\n",
    "    correlation_matrix = df_corr.corr()\n",
    "    \n",
    "    # Optional: If you want to see the full numerical matrix, uncomment the next line\n",
    "    # print(\"\\n--- Full Pairwise Correlation Matrix ---\")\n",
    "    # print(correlation_matrix)\n",
    "\n",
    "    # --- 4. Visualize the Matrix with a Heatmap ---\n",
    "    # A heatmap is the best way to see the broad patterns of collinearity.\n",
    "    print(\"\\nGenerating correlation heatmap...\")\n",
    "    \n",
    "    plt.figure(figsize=(18, 15))\n",
    "    heatmap = sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        cmap='coolwarm',  # Use a diverging colormap (red=positive, blue=negative)\n",
    "        center=0,         # Center the colormap at zero\n",
    "        vmin=-1,          # Set the color scale limits to the theoretical min/max\n",
    "        vmax=1,\n",
    "        linewidths=.5,\n",
    "        annot=False       # Annotations are turned off as the matrix is too large to be readable\n",
    "    )\n",
    "    \n",
    "    plt.title('Pairwise Correlation Matrix of All Monthly Stressors for Maize', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73f7293a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Full INTERACTION Model ---\n",
      "Data prepared.\n",
      "\n",
      "Using the 'full interaction' formula for selection:\n",
      "\n",
      "    yield_maize ~ year \n",
      "                  + temperature_May * temperature_Jun * temperature_Jul\n",
      "                  + precipitation_May + precipitation_Jun + precipitation_Jul\n",
      "                  + bs(lat, df=4) + bs(lon, df=4)\n",
      "\n",
      "\n",
      "--- Fitting Regularization Path (Elastic Net, L1_wt=0.5) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1464: UserWarning: Elastic net fitting did not converge\n",
      "  warnings.warn(\"Elastic net fitting did not converge\")\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1464: UserWarning: Elastic net fitting did not converge\n",
      "  warnings.warn(\"Elastic net fitting did not converge\")\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Regularization Path Results (Full Interaction Model) ---\n",
      "Showing which variables 'survived' at each penalty level (alpha).\n",
      "\n",
      "--- alpha_0.1 ---\n",
      "temperature_Jul   -0.086252\n",
      "\n",
      "--- alpha_0.05 ---\n",
      "temperature_Jul                   -0.174719\n",
      "temperature_May:temperature_Jun   -0.109306\n",
      "\n",
      "--- alpha_0.01 ---\n",
      "temperature_Jul                                   -0.145968\n",
      "temperature_May:temperature_Jun                   -0.122060\n",
      "temperature_May:temperature_Jun:temperature_Jul   -0.013699\n",
      "\n",
      "--- alpha_0.005 ---\n",
      "temperature_Jul                                   -0.184663\n",
      "temperature_May:temperature_Jul                   -0.088230\n",
      "temperature_May:temperature_Jun                   -0.045426\n",
      "temperature_Jun                                   -0.038470\n",
      "precipitation_Jul                                 -0.027964\n",
      "precipitation_May                                 -0.018074\n",
      "temperature_May:temperature_Jun:temperature_Jul   -0.017401\n",
      "precipitation_Jun                                 -0.013951\n",
      "\n",
      "--- alpha_0.001 ---\n",
      "temperature_Jul                                   -0.191465\n",
      "temperature_May:temperature_Jul                   -0.070945\n",
      "temperature_May:temperature_Jun                   -0.049864\n",
      "temperature_Jun                                   -0.042611\n",
      "precipitation_Jul                                 -0.022631\n",
      "temperature_May:temperature_Jun:temperature_Jul   -0.018387\n",
      "precipitation_May                                 -0.015916\n",
      "precipitation_Jun                                 -0.011421\n",
      "\n",
      "\n",
      "--- Detailed Summary for the Complex Model (alpha=0.01) ---\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:            yield_maize   No. Observations:                 1470\n",
      "Model:                            GLM   Df Residuals:                     1463\n",
      "Model Family:                   Gamma   Df Model:                            7\n",
      "Link Function:                    log   Scale:                        0.029941\n",
      "Method:                   elastic_net   Log-Likelihood:                -2848.3\n",
      "Date:                Tue, 11 Nov 2025   Deviance:                       43.943\n",
      "Time:                        19:26:47   Pearson chi2:                     43.8\n",
      "No. Iterations:                   160   Pseudo R-squ. (CS):             0.7351\n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================================================\n",
      "                                                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                           2.2921      0.010    221.405      0.000       2.272       2.312\n",
      "year                                                0.1097      0.005     24.043      0.000       0.101       0.119\n",
      "temperature_May                                          0          0        nan        nan           0           0\n",
      "temperature_Jun                                          0          0        nan        nan           0           0\n",
      "temperature_May:temperature_Jun                    -0.1221      0.006    -20.244      0.000      -0.134      -0.110\n",
      "temperature_Jul                                    -0.1460      0.007    -19.578      0.000      -0.161      -0.131\n",
      "temperature_May:temperature_Jul                          0          0        nan        nan           0           0\n",
      "temperature_Jun:temperature_Jul                          0          0        nan        nan           0           0\n",
      "temperature_May:temperature_Jun:temperature_Jul    -0.0137      0.003     -3.992      0.000      -0.020      -0.007\n",
      "precipitation_May                                        0          0        nan        nan           0           0\n",
      "precipitation_Jun                                        0          0        nan        nan           0           0\n",
      "precipitation_Jul                                        0          0        nan        nan           0           0\n",
      "bs(lat, df=4)[0]                                         0          0        nan        nan           0           0\n",
      "bs(lat, df=4)[1]                                         0          0        nan        nan           0           0\n",
      "bs(lat, df=4)[2]                                         0          0        nan        nan           0           0\n",
      "bs(lat, df=4)[3]                                         0          0        nan        nan           0           0\n",
      "bs(lon, df=4)[0]                                    0.3196      0.025     12.553      0.000       0.270       0.369\n",
      "bs(lon, df=4)[1]                                         0          0        nan        nan           0           0\n",
      "bs(lon, df=4)[2]                                         0          0        nan        nan           0           0\n",
      "bs(lon, df=4)[3]                                    0.1783      0.019      9.472      0.000       0.141       0.215\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Full INTERACTION Model\n",
    "\n",
    "print(\"--- Full INTERACTION Model ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "df_maize = pd.read_csv(file_path)\n",
    "df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "print(\"Data prepared.\")\n",
    "\n",
    "# ---  Define Variables and Standardize ---\n",
    "predictors_to_scale = [col for col in df_maize.columns if col not in ['yield_maize', 'lat', 'lon']]\n",
    "df_to_scale = df_maize[predictors_to_scale]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_to_scale), columns=predictors_to_scale, index=df_maize.index)\n",
    "\n",
    "df_model_ready = pd.concat([df_maize[['yield_maize', 'lat', 'lon']], df_scaled], axis=1)\n",
    "\n",
    "\n",
    "# --- 1. Define the FULL INTERACTION Model Formula ---\n",
    "# This is the complex formula from your advisor's notes.\n",
    "formula_full_interaction = \"\"\"\n",
    "    yield_maize ~ year \n",
    "                  + temperature_May * temperature_Jun * temperature_Jul\n",
    "                  + precipitation_May + precipitation_Jun + precipitation_Jul\n",
    "                  + bs(lat, df=4) + bs(lon, df=4)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nUsing the 'full interaction' formula for selection:\")\n",
    "print(formula_full_interaction)\n",
    "\n",
    "# --- 2. Fit the Regularization Path ---\n",
    "alphas = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "results_path_complex = {}\n",
    "\n",
    "print(\"\\n--- Fitting Regularization Path (Elastic Net, L1_wt=0.5) ---\")\n",
    "for alpha_val in alphas:\n",
    "    try:\n",
    "        glm_complex = smf.glm(\n",
    "            formula=formula_full_interaction,\n",
    "            data=df_model_ready, # Use the STANDARDIZED data\n",
    "            family=sm.families.Gamma(link=sm.families.links.log())\n",
    "        )\n",
    "        \n",
    "        model_reg_complex = glm_complex.fit_regularized(\n",
    "            method='elastic_net',\n",
    "            alpha=alpha_val,\n",
    "            L1_wt=0.5,\n",
    "            refit=True,\n",
    "            maxiter=200 # Increase iterations to give the model a better chance to converge\n",
    "        )\n",
    "        \n",
    "        params = pd.Series(model_reg_complex.params, index=model_reg_complex.model.exog_names)\n",
    "        results_path_complex[f\"alpha_{alpha_val}\"] = params[params.abs() > 1e-6]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fit model for alpha={alpha_val}. Error: {e}\")\n",
    "\n",
    "# --- 3. Display the Results of the Path ---\n",
    "print(\"\\n--- Regularization Path Results (Full Interaction Model) ---\")\n",
    "print(\"Showing which variables 'survived' at each penalty level (alpha).\")\n",
    "\n",
    "for alpha_key, params in results_path_complex.items():\n",
    "    print(f\"\\n--- {alpha_key} ---\")\n",
    "    stressor_params = params.filter(regex='temp|precip')\n",
    "    if not stressor_params.empty:\n",
    "        print(stressor_params.reindex(stressor_params.abs().sort_values(ascending=False).index).to_string())\n",
    "    else:\n",
    "        print(\"All stressor coefficients were shrunk to zero.\")\n",
    "\n",
    "\n",
    "# --- 4. Fit and Summarize the \"Champion\" Complex Model ---\n",
    "# We will still choose alpha=0.01 for a direct comparison with the simpler model.\n",
    "champion_alpha_complex = 0.01\n",
    "print(f\"\\n\\n--- Detailed Summary for the Complex Model (alpha={champion_alpha_complex}) ---\")\n",
    "\n",
    "try:\n",
    "    glm_champion_complex = smf.glm(\n",
    "        formula=formula_full_interaction, \n",
    "        data=df_model_ready, \n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "    \n",
    "    model_champion_complex = glm_champion_complex.fit_regularized(\n",
    "        method='elastic_net',\n",
    "        alpha=champion_alpha_complex,\n",
    "        L1_wt=0.5,\n",
    "        refit=True,\n",
    "        maxiter=200\n",
    "    )\n",
    "    \n",
    "    print(model_champion_complex.summary())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while fitting the champion complex model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a12a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3 (Final): Regularization Path and Champion Model Summary ---\n",
      "Data prepared.\n",
      "\n",
      "Using the 'main effects' formula for selection.\n",
      "\n",
      "--- Fitting Regularization Path (Elastic Net, L1_wt=0.5) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1464: UserWarning: Elastic net fitting did not converge\n",
      "  warnings.warn(\"Elastic net fitting did not converge\")\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1464: UserWarning: Elastic net fitting did not converge\n",
      "  warnings.warn(\"Elastic net fitting did not converge\")\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1464: UserWarning: Elastic net fitting did not converge\n",
      "  warnings.warn(\"Elastic net fitting did not converge\")\n",
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Regularization Path Results (High-Level View) ---\n",
      "Showing which stressors 'survived' at each penalty level (alpha).\n",
      "\n",
      "--- alpha_0.1 ---\n",
      "temperature_Jul   -0.086252\n",
      "\n",
      "--- alpha_0.05 ---\n",
      "temperature_Jul   -0.089489\n",
      "\n",
      "--- alpha_0.01 ---\n",
      "temperature_Jul   -0.156565\n",
      "soil_water_Sep     0.038958\n",
      "soil_water_Jul     0.004207\n",
      "\n",
      "--- alpha_0.005 ---\n",
      "temperature_Jul     -0.198160\n",
      "precipitation_Jul   -0.066454\n",
      "soil_water_Aug       0.055624\n",
      "precipitation_Aug   -0.038040\n",
      "soil_water_Sep       0.022459\n",
      "soil_water_Jul       0.020979\n",
      "precipitation_Sep    0.020011\n",
      "precipitation_May   -0.014178\n",
      "\n",
      "--- alpha_0.001 ---\n",
      "temperature_Jul     -0.172572\n",
      "temperature_Sep     -0.067603\n",
      "soil_water_Jul       0.061563\n",
      "soil_water_Aug       0.050745\n",
      "precipitation_Jul   -0.047415\n",
      "temperature_Jun     -0.039850\n",
      "precipitation_Sep    0.031610\n",
      "temperature_Aug     -0.020012\n",
      "precipitation_May   -0.012150\n",
      "precipitation_Aug   -0.008309\n",
      "soil_water_Sep      -0.007860\n",
      "precipitation_Jun    0.005515\n",
      "\n",
      "\n",
      "--- Detailed Summary for our Champion Model (alpha=0.01) ---\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:            yield_maize   No. Observations:                 1470\n",
      "Model:                            GLM   Df Residuals:                     1461\n",
      "Model Family:                   Gamma   Df Model:                            9\n",
      "Link Function:                    log   Scale:                        0.036000\n",
      "Method:                   elastic_net   Log-Likelihood:                -2993.7\n",
      "Date:                Tue, 11 Nov 2025   Deviance:                       53.500\n",
      "Time:                        19:22:15   Pearson chi2:                     52.6\n",
      "No. Iterations:                    50   Pseudo R-squ. (CS):             0.6032\n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept             1.7289      0.027     64.422      0.000       1.676       1.782\n",
      "year                  0.1069      0.005     21.483      0.000       0.097       0.117\n",
      "temperature_May            0          0        nan        nan           0           0\n",
      "temperature_Jun            0          0        nan        nan           0           0\n",
      "temperature_Jul      -0.1566      0.008    -18.543      0.000      -0.173      -0.140\n",
      "temperature_Aug            0          0        nan        nan           0           0\n",
      "temperature_Sep            0          0        nan        nan           0           0\n",
      "precipitation_May          0          0        nan        nan           0           0\n",
      "precipitation_Jun          0          0        nan        nan           0           0\n",
      "precipitation_Jul          0          0        nan        nan           0           0\n",
      "precipitation_Aug          0          0        nan        nan           0           0\n",
      "precipitation_Sep          0          0        nan        nan           0           0\n",
      "soil_water_May             0          0        nan        nan           0           0\n",
      "soil_water_Jun             0          0        nan        nan           0           0\n",
      "soil_water_Jul        0.0042      0.008      0.515      0.607      -0.012       0.020\n",
      "soil_water_Aug             0          0        nan        nan           0           0\n",
      "soil_water_Sep        0.0390      0.007      5.906      0.000       0.026       0.052\n",
      "bs(lat, df=4)[0]      0.1262      0.030      4.274      0.000       0.068       0.184\n",
      "bs(lat, df=4)[1]           0          0        nan        nan           0           0\n",
      "bs(lat, df=4)[2]           0          0        nan        nan           0           0\n",
      "bs(lat, df=4)[3]           0          0        nan        nan           0           0\n",
      "bs(lon, df=4)[0]      1.1780      0.046     25.524      0.000       1.088       1.269\n",
      "bs(lon, df=4)[1]           0          0        nan        nan           0           0\n",
      "bs(lon, df=4)[2]      0.7280      0.042     17.474      0.000       0.646       0.810\n",
      "bs(lon, df=4)[3]      0.5445      0.027     20.170      0.000       0.492       0.597\n",
      "=====================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1464: UserWarning: Elastic net fitting did not converge\n",
      "  warnings.warn(\"Elastic net fitting did not converge\")\n"
     ]
    }
   ],
   "source": [
    "# Regularized Path and Champion Model Summary\n",
    "\n",
    "print(\"--- Regularization Path and Champion Model Summary ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "df_maize = pd.read_csv(file_path)\n",
    "df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "print(\"Data prepared.\")\n",
    "\n",
    "# --- 2. Define Variables and Standardize ---\n",
    "predictors_to_scale = [col for col in df_maize.columns if col not in ['yield_maize', 'lat', 'lon']]\n",
    "df_to_scale = df_maize[predictors_to_scale]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_to_scale), columns=predictors_to_scale, index=df_maize.index)\n",
    "\n",
    "df_model_ready = pd.concat([df_maize[['yield_maize', 'lat', 'lon']], df_scaled], axis=1)\n",
    "\n",
    "\n",
    "# --- 3. Define the Model Formula ---\n",
    "formula_simple = \"\"\"\n",
    "    yield_maize ~ year \n",
    "                  + temperature_May + temperature_Jun + temperature_Jul + temperature_Aug + temperature_Sep\n",
    "                  + precipitation_May + precipitation_Jun + precipitation_Jul + precipitation_Aug + precipitation_Sep\n",
    "                  + soil_water_May + soil_water_Jun + soil_water_Jul + soil_water_Aug + soil_water_Sep\n",
    "                  + bs(lat, df=4) + bs(lon, df=4)\n",
    "\"\"\"\n",
    "print(\"\\nUsing the 'main effects' formula for selection.\")\n",
    "\n",
    "\n",
    "# --- 4. Fit the Regularization Path (for high-level overview) ---\n",
    "alphas = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "results_path = {}\n",
    "\n",
    "print(\"\\n--- Fitting Regularization Path (Elastic Net, L1_wt=0.5) ---\")\n",
    "for alpha_val in alphas:\n",
    "    try:\n",
    "        glm_model = smf.glm(formula=formula_simple, data=df_model_ready, family=sm.families.Gamma(link=sm.families.links.log()))\n",
    "        model_reg = glm_model.fit_regularized(method='elastic_net', alpha=alpha_val, L1_wt=0.5, refit=True)\n",
    "        params = pd.Series(model_reg.params, index=model_reg.model.exog_names)\n",
    "        results_path[f\"alpha_{alpha_val}\"] = params[params.abs() > 1e-6]\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fit model for alpha={alpha_val}. Error: {e}\")\n",
    "\n",
    "# --- 5. Display the Results of the Path ---\n",
    "print(\"\\n--- Regularization Path Results (High-Level View) ---\")\n",
    "print(\"Showing which stressors 'survived' at each penalty level (alpha).\")\n",
    "\n",
    "for alpha_key, params in results_path.items():\n",
    "    print(f\"\\n--- {alpha_key} ---\")\n",
    "    stressor_params = params.filter(regex='temp|precip|soil')\n",
    "    if not stressor_params.empty:\n",
    "        print(stressor_params.reindex(stressor_params.abs().sort_values(ascending=False).index).to_string())\n",
    "    else:\n",
    "        print(\"All stressor coefficients were shrunk to zero.\")\n",
    "\n",
    "\n",
    "# --- 6. Fit and Summarize the \"Champion\" Model ---\n",
    "# Based on the path, alpha=0.01 seems like a good balance. Let's fit it\n",
    "# and show the full summary for detailed interpretation.\n",
    "champion_alpha = 0.01\n",
    "print(f\"\\n\\n--- Detailed Summary for our Champion Model (alpha={champion_alpha}) ---\")\n",
    "\n",
    "try:\n",
    "    glm_champion = smf.glm(formula=formula_simple, data=df_model_ready, family=sm.families.Gamma(link=sm.families.links.log()))\n",
    "    \n",
    "    model_champion = glm_champion.fit_regularized(\n",
    "        method='elastic_net',\n",
    "        alpha=champion_alpha,\n",
    "        L1_wt=0.5,\n",
    "        refit=True\n",
    "    )\n",
    "    \n",
    "    print(model_champion.summary())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while fitting the champion model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9556d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building and Interpreting the Final Champion Model ---\n",
      "Data prepared.\n",
      "\n",
      "Fitting the Champion Model with the formula:\n",
      "\n",
      "    yield_maize ~ year \n",
      "                  + temperature_Jul + I(temperature_Jul**2)\n",
      "                  + precipitation_Jul + I(precipitation_Jul**2)\n",
      "                  + temperature_Jul:precipitation_Jul\n",
      "                  + bs(lat, df=4) + bs(lon, df=4)\n",
      "\n",
      "\n",
      "--- Summary of the Final Champion Model ---\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:            yield_maize   No. Observations:                 1470\n",
      "Model:                            GLM   Df Residuals:                     1455\n",
      "Model Family:                   Gamma   Df Model:                           14\n",
      "Link Function:                    log   Scale:                        0.025392\n",
      "Method:                          IRLS   Log-Likelihood:                -2726.3\n",
      "Date:                Tue, 11 Nov 2025   Deviance:                       37.253\n",
      "Time:                        20:23:10   Pearson chi2:                     36.9\n",
      "No. Iterations:                    13   Pseudo R-squ. (CS):             0.8255\n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================================\n",
      "                                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Intercept                           -21.6388      0.875    -24.736      0.000     -23.353     -19.924\n",
      "year                                  0.0115      0.000     26.800      0.000       0.011       0.012\n",
      "temperature_Jul                       0.1233      0.017      7.177      0.000       0.090       0.157\n",
      "I(temperature_Jul ** 2)              -0.0039      0.000    -10.162      0.000      -0.005      -0.003\n",
      "precipitation_Jul                     0.0049      0.001      6.504      0.000       0.003       0.006\n",
      "I(precipitation_Jul ** 2)         -4.578e-06   1.09e-06     -4.196      0.000   -6.72e-06   -2.44e-06\n",
      "temperature_Jul:precipitation_Jul    -0.0002   2.93e-05     -6.679      0.000      -0.000      -0.000\n",
      "bs(lat, df=4)[0]                     -0.2779      0.039     -7.041      0.000      -0.355      -0.201\n",
      "bs(lat, df=4)[1]                     -0.3653      0.047     -7.808      0.000      -0.457      -0.274\n",
      "bs(lat, df=4)[2]                     -0.2808      0.043     -6.543      0.000      -0.365      -0.197\n",
      "bs(lat, df=4)[3]                     -0.5781      0.049    -11.758      0.000      -0.675      -0.482\n",
      "bs(lon, df=4)[0]                      0.9266      0.053     17.450      0.000       0.823       1.031\n",
      "bs(lon, df=4)[1]                      0.1793      0.043      4.145      0.000       0.095       0.264\n",
      "bs(lon, df=4)[2]                      0.5270      0.044     11.929      0.000       0.440       0.614\n",
      "bs(lon, df=4)[3]                      0.3899      0.045      8.756      0.000       0.303       0.477\n",
      "=====================================================================================================\n",
      "\n",
      "Final Model AIC: 5,482.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esthe\\anaconda3\\envs\\climarisc\\lib\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# The Final Champion Explanatory Model\n",
    "\n",
    "print(\"--- Building and Interpreting the Final Champion Model ---\")\n",
    "\n",
    "# --- 1. Load Data (no scaling needed for this final model) ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "df_maize = pd.read_csv(file_path)\n",
    "df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "print(\"Data prepared.\")\n",
    "\n",
    "# --- 2. Define and Fit the Champion Model Formula ---\n",
    "# This formula is the result of our entire diagnostic process.\n",
    "# It is simple, robust, and focuses on the most important effects.\n",
    "champion_formula = \"\"\"\n",
    "    yield_maize ~ year \n",
    "                  + temperature_Jul + I(temperature_Jul**2)\n",
    "                  + precipitation_Jul + I(precipitation_Jul**2)\n",
    "                  + temperature_Jul:precipitation_Jul\n",
    "                  + bs(lat, df=4) + bs(lon, df=4)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nFitting the Champion Model with the formula:\")\n",
    "print(champion_formula)\n",
    "\n",
    "try:\n",
    "    champion_model = smf.glm(\n",
    "        formula=champion_formula,\n",
    "        data=df_maize,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    ).fit()\n",
    "\n",
    "    print(\"\\n--- Summary of the Final Champion Model ---\")\n",
    "    print(champion_model.summary())\n",
    "    \n",
    "    # We can also check the AIC as a final performance metric\n",
    "    print(f\"\\nFinal Model AIC: {champion_model.aic:,.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27adc6b3",
   "metadata": {},
   "source": [
    "I(precipitation_Jul**2) has a tiny coefficient (-4.578e-06). But remember, the input to this variable is precipitation squared. If average precipitation is 100mm, the input is 10,000. So, 10000 * -4.578e-06 = -0.04578, which is a meaningful number on the log-yield scale.\n",
    "\n",
    "temperature_Jul:precipitation_Jul has a small coefficient (-0.0002). The input is temp * precip (e.g., 25 * 100 = 2500). So, 2500 * -0.0002 = -0.5. Again, a very meaningful number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e908d188",
   "metadata": {},
   "source": [
    "### Analysis of the Final Champion Model\n",
    "\n",
    "After a rigorous, multi-step process of model selection and refinement, this model represents our final, champion explanatory model for maize yield. It is statistically robust, parsimonious, and provides several key insights.\n",
    "\n",
    "#### Model Performance and Justification\n",
    "The model's **Pseudo R-squared of 0.826** is very strong, indicating that our chosen predictors account for a substantial portion of the variation in maize yield. All predictors in the model are **highly statistically significant** (p < 0.001 for all key stressors), giving us high confidence in their effects.\n",
    "\n",
    "This model structure was chosen after a thorough diagnostic process. An initial attempt to model all monthly stressors with complex interactions proved to be statistically unstable due to severe multicollinearity, a finding confirmed by a `ConvergenceWarning` and high Variance Inflation Factors (VIFs). A subsequent regularization pass using an Elastic Net (`fit_regularized`) objectively identified `temperature_Jul` and `precipitation_Jul` as the most consistent and important predictors. Our final model is therefore built using this data-driven subset of variables, ensuring the coefficients are stable and interpretable.\n",
    "\n",
    "#### Interpretation of Key Climate Effects\n",
    "\n",
    "The model reveals a complex, multi-layered story about how climate affects maize yield:\n",
    "\n",
    "1.  **The Dominant Role of July Heat (Non-Linear):**\n",
    "    The model includes both a linear (`temperature_Jul`) and a quadratic (`I(temperature_Jul**2)`) term for July temperature.\n",
    "    *   The linear term's coefficient is **positive** (`0.1233`), while the quadratic term's is **negative** (`-0.0039`). This mathematical combination describes an **inverted 'U' shape**. However, because the range of historical July temperatures in Northern Italy falls on the right-hand side of this curve, the practical result is a story of accelerating damage. As temperatures rise into the stressful high-20s, the negative quadratic term begins to dominate, causing yield to decrease at an ever-faster rate.\n",
    "\n",
    "2.  **The Role of July Precipitation (Non-Linear):**\n",
    "    Similarly, the model finds a significant quadratic relationship for `precipitation_Jul`.\n",
    "    *   The linear term is **positive** (`0.0049`) and the quadratic term is **negative** (`-4.578e-06`). This describes a classic crop response curve: yield increases with more rainfall up to an optimal point, after which excessive rainfall leads to diminishing returns and potentially slight decreases in yield.\n",
    "\n",
    "3.  **The Critical Heat x Drought Interaction:**\n",
    "    The interaction term `temperature_Jul:precipitation_Jul` is **highly significant (p=0.000)** with a **negative coefficient** (`-0.0002`). This is a crucial finding. It means that the two stressors are not independent. The negative sign indicates that higher precipitation **buffers the damaging effect of heat**. In other words, the negative impact of a very hot July is significantly less severe in years that also have adequate rainfall, confirming a classic heat-drought interaction.\n",
    "\n",
    "#### Final Conclusion for Maize\n",
    "\n",
    "In summary, our final model demonstrates that maize yield in Northern Italy is primarily driven by a non-linear vulnerability to July heat. This vulnerability is not fixed; it is significantly buffered by the availability of July precipitation, and the crop's response to rainfall itself follows a classic optimal curve. The model successfully isolates these weather-driven effects while controlling for both long-term trends in production (the `year` term) and smooth geographic variations in baseline yield (the `bs(lat)` and `bs(lon)` terms).```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 39-rice-gamma-model.ipynb/39-rice-gamma-model.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4bab61c",
   "metadata": {},
   "source": [
    "# EDA for Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9df3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import scipy.stats as stats\n",
    "from patsy import dmatrices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd209b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis - Full Correlation Matrix\n",
    "\n",
    "print(\"--- EDA: Correlation Analysis of All Monthly Stressors ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# As confirmed, this file is already specific to maize and its growing season.\n",
    "file_path = '../data-cherry-pick/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_rice = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Select Only the Monthly Stressor Variables ---\n",
    "    # We will select all columns that have a month name in them, which is a robust\n",
    "    # way to grab all the monthly predictors we want to investigate.\n",
    "    monthly_stressors = [col for col in df_rice.columns if '_' in col and 'yield' not in col]\n",
    "    df_corr = df_rice[monthly_stressors]\n",
    "    \n",
    "    print(f\"\\nSelected {len(df_corr.columns)} monthly stressor variables for correlation analysis.\")\n",
    "\n",
    "    # --- 3. Calculate and Print the Correlation Matrix ---\n",
    "    correlation_matrix = df_corr.corr()\n",
    "    \n",
    "    # Optional: If you want to see the full numerical matrix, uncomment the next line\n",
    "    # print(\"\\n--- Full Pairwise Correlation Matrix ---\")\n",
    "    # print(correlation_matrix)\n",
    "\n",
    "    # --- 4. Visualize the Matrix with a Heatmap ---\n",
    "    # A heatmap is the best way to see the broad patterns of collinearity.\n",
    "    print(\"\\nGenerating correlation heatmap...\")\n",
    "    \n",
    "    plt.figure(figsize=(18, 15))\n",
    "    heatmap = sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        cmap='coolwarm',  # Use a diverging colormap (red=positive, blue=negative)\n",
    "        center=0,         # Center the colormap at zero\n",
    "        vmin=-1,          # Set the color scale limits to the theoretical min/max\n",
    "        vmax=1,\n",
    "        linewidths=.5,\n",
    "        annot=False       # Annotations are turned off as the matrix is too large to be readable\n",
    "    )\n",
    "    \n",
    "    plt.title('Pairwise Correlation Matrix of All Monthly Stressors for Rice', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Extended EDA for Rice Yield Analysis ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_rice = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- Task 1: Examine the Distribution of the Dependent Variable (yield_rice) ---\n",
    "    print(\"--- Task 1: Analyzing the distribution of the dependent variable 'yield_rice' ---\")\n",
    "    \n",
    "    # Create a figure with two subplots side-by-side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Distribution Analysis for yield_rice', fontsize=16)\n",
    "\n",
    "    # a) Histogram with a Kernel Density Estimate (KDE)\n",
    "    # This helps us visually assess the shape, center, and spread of the yield data.\n",
    "    # We are checking for positive skewness, which is characteristic of data modeled by a Gamma distribution.\n",
    "    sns.histplot(df_rice['yield_rice'], kde=True, ax=axes[0])\n",
    "    axes[0].set_title('Histogram of Rice Yield')\n",
    "    axes[0].set_xlabel('Yield (tonnes per hectare)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    # b) Q-Q (Quantile-Quantile) Plot against a theoretical normal distribution\n",
    "    # This plot helps us assess if the data's distribution follows a specific theoretical distribution.\n",
    "    # Deviations from the red line suggest skewness or heavy tails.\n",
    "    # While our target is a Gamma GLM, a Q-Q plot vs. Normal is a standard first step to detect non-normality.\n",
    "    stats.probplot(df_rice['yield_rice'], dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot of Rice Yield vs. Normal Distribution')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Distribution plots generated. Check for positive skew, which supports our choice of a Gamma GLM.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 2: Bivariate Scatter Plots of Yield vs. Key Stressors ---\n",
    "    print(\"--- Task 2: Visualizing relationships between yield and key climate stressors ---\")\n",
    "    \n",
    "    # Select a few key stressors based on agronomic theory for rice\n",
    "    key_stressors = ['temperature_Jul', 'precipitation_Jun', 'soil_water_Aug']\n",
    "    \n",
    "    # Create a figure to hold the scatter plots\n",
    "    fig, axes = plt.subplots(1, len(key_stressors), figsize=(21, 6))\n",
    "    fig.suptitle('Bivariate Relationships: Rice Yield vs. Key Stressors', fontsize=16)\n",
    "\n",
    "    for i, stressor in enumerate(key_stressors):\n",
    "        # We use a regression plot with a LOWESS (Locally Weighted Scatterplot Smoothing) curve.\n",
    "        # This is a non-parametric way to see the underlying trend without assuming a linear relationship.\n",
    "        # It's excellent for spotting potential non-linearities (like an inverted 'U' shape).\n",
    "        sns.regplot(\n",
    "            x=stressor,\n",
    "            y='yield_rice',\n",
    "            data=df_rice,\n",
    "            ax=axes[i],\n",
    "            lowess=True, # Use LOWESS smoother to detect non-linear patterns\n",
    "            scatter_kws={'alpha': 0.1, 'color': 'gray'}, # De-emphasize individual points\n",
    "            line_kws={'color': 'blue'} # Emphasize the trend line\n",
    "        )\n",
    "        axes[i].set_title(f'Yield vs. {stressor}')\n",
    "        axes[i].set_xlabel(f'{stressor}')\n",
    "        axes[i].set_ylabel('Yield (tonnes per hectare)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Scatter plots generated. Look for non-linear patterns that might inform our final model.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 3: Plot Key Variables Over Time ---\n",
    "    print(\"--- Task 3: Examining long-term trends in yield and a key climate variable ---\")\n",
    "    \n",
    "    # Calculate the mean of yield and a key stressor for each year\n",
    "    yearly_data = df_rice.groupby('year')[['yield_rice', 'temperature_Aug']].mean().reset_index()\n",
    "\n",
    "    # Create a plot with a primary and secondary y-axis to show both trends together.\n",
    "    # This confirms the necessity of including 'year' as a control variable to capture trends\n",
    "    # likely related to technology, while also checking for climate trends.\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plotting average yield on the primary (left) y-axis\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Average Yield (tonnes per hectare)', color=color)\n",
    "    ax1.plot(yearly_data['year'], yearly_data['yield_rice'], color=color, marker='o', label='Average Yield')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Create a second y-axis that shares the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plotting average temperature on the secondary (right) y-axis\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Average August Temperature (C)', color=color)\n",
    "    ax2.plot(yearly_data['year'], yearly_data['temperature_Aug'], color=color, linestyle='--', marker='x', label='Avg. August Temp')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('Long-Term Trend of Rice Yield and August Temperature (1982-2016)', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    # Adding a single legend for both lines\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"Time-series plot generated. Note the clear upward trend in yield, confirming the need for a 'year' control variable.\\n\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: A required column was not found in the dataset: {e}. Please check the CSV file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e1ac2",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "### 1. Most Important Things to Consider for Any Regularization\n",
    "\n",
    "Regardless of the specific technique (Lasso, Ridge, or Elastic Net), there are universal principles you must always follow for the process to be valid:\n",
    "\n",
    "1.  **Feature Scaling is Non-Negotiable:** This is the most important prerequisite. Regularization works by adding a penalty based on the size of the coefficients to the model's loss function. If your predictors are on different scales (e.g., temperature from 10-30, precipitation from 0-200), the algorithm will unfairly penalize the feature with the larger-scale coefficient, not because it's less important, but simply because its numerical value is larger. **Standardization** (scaling features to have a mean of 0 and a standard deviation of 1) is the standard procedure to ensure every variable is treated equally by the penalty.\n",
    "\n",
    "2.  **Understand the Bias-Variance Trade-off:** The entire purpose of regularization is to manage this trade-off. A standard, unregularized model (like OLS) has low bias but can have very high variance, meaning it overfits the training data and performs poorly on new data. Regularization intentionally introduces a small amount of bias (it shrinks the coefficients, making them technically \"wrong\" for the training data) to achieve a massive reduction in variance. The result is a simpler, more stable model that generalizes better to unseen data.\n",
    "\n",
    "3.  **Hyperparameter Tuning via Cross-Validation:** The strength of the regularization penalty (often denoted by `alpha` or `lambda`) is a hyperparameter that we must choose. It is never \"learned\" from the data directly. The only reliable way to select the optimal `alpha` is through **cross-validation (CV)**. The process involves testing a range of `alpha` values and selecting the one that results in the best model performance (e.g., lowest Mean Squared Error or Deviance) on average across all the CV folds. Flying blind and picking an arbitrary `alpha` will not produce a robust model.\n",
    "\n",
    "4.  **Coefficients are for Selection, Not Final Interpretation:** Because the coefficients in a regularized model are shrunk (biased towards zero), their direct interpretation as \"a one-unit increase in X is associated with a Y-unit change in the outcome\" is complicated. The primary goal of this step is **variable selection** and identifying the most robust predictors. This is why our workflow correctly includes a final step where we refit a standard GLM using *only* the variables selected by regularization to get clean, unbiased coefficients for interpretation.\n",
    "\n",
    "### 2. Considerations for Our Data & Necessary Manipulations\n",
    "\n",
    "Yes, our data absolutely needs to be manipulated before we can apply regularization. Based on the principles above:\n",
    "\n",
    "*   **Action Required:** We must **standardize all predictor variables**.\n",
    "*   **Which Variables?** This includes all the monthly climate stressors (`temperature_May`, `precipitation_Jun`, etc.) as well as our `year` control variable. The `year` variable (e.g., 1982, 1983, ...) is on a much larger scale than our climate data and must be scaled to be comparable. The basis functions generated by our spatial splines (`bs(lat, df=4)`) must also be included in the standardization.\n",
    "*   **What NOT to Manipulate:** We do **not** standardize the dependent variable (`yield_rice`). The model's objective is to predict the yield in its original, interpretable units (tonnes per hectare).\n",
    "\n",
    "### 3. Best Regularization Technique for This Case: Elastic Net\n",
    "\n",
    "For our specific project, the **Elastic Net is the superior choice**. Here is the argument comparing it to its components, Ridge and Lasso.\n",
    "\n",
    "*   **Ridge Regression (L2 Penalty):**\n",
    "    *   **How it works:** It shrinks coefficients towards zero but **never sets them to exactly zero**. It keeps all variables in the model, but reduces the impact of less important ones.\n",
    "    *   **Strength:** It handles multicollinearity very well. It will tend to give correlated predictors similar coefficients.\n",
    "    *   **Weakness for us:** Our goal is not just to manage collinearity, but also to perform *variable selection*. We want to identify the handful of months that are most critical. Since Ridge keeps all predictors, it doesn't help us simplify the model in this way.\n",
    "\n",
    "*   **Lasso Regression (L1 Penalty):**\n",
    "    *   **How it works:** It is capable of shrinking coefficients to **exactly zero**, effectively removing them from the model. This makes it a tool for automatic feature selection.\n",
    "    *   **Strength:** It produces sparse, simpler models by identifying and eliminating irrelevant predictors.\n",
    "    *   **Weakness for us:** Lasso has a major flaw when dealing with a group of highly correlated variables (like our `temperature_Jun`, `temperature_Jul`, `temperature_Aug` block). It will tend to **arbitrarily select only one** of them and zero out the others. This is unstable and undesirable. We would lose the information that the entire summer period's temperature is important, and the model might pick a different month each time it's run.\n",
    "\n",
    "*   **Elastic Net (A mix of L1 and L2):**\n",
    "    *   **How it works:** It combines both the Ridge and Lasso penalties. It has a parameter (`l1_ratio`) that lets us balance between the two.\n",
    "    *   **Strength:** It inherits the best of both worlds. It can perform variable selection like Lasso, but the Ridge component allows it to handle correlated predictors gracefully. It exhibits a **\"grouping effect\"**: if a set of predictors are highly correlated, the Elastic Net will tend to keep or discard them as a group.\n",
    "    *   **Why it's perfect for us:** Our EDA heatmap showed us that our data is defined by highly correlated blocks of variables. The Elastic Net is specifically designed for this scenario. It will allow us to both select the most important *groups* of stressors and shrink the coefficients within those groups, giving us a stable, interpretable, and robust selection of variables to carry forward to our final explanatory model.\n",
    "\n",
    "Therefore, our plan to use the Elastic Net is not just a good choice; it is the **theoretically correct and most robust choice** for the structure of our data and the goals of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a615cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_rice = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Define the Full Model Formula ---\n",
    "    # Programmatically get all monthly stressor column names\n",
    "    monthly_stressors = [col for col in df_rice.columns if '_' in col and 'yield' not in col]\n",
    "    \n",
    "    # Join them with '+' to create the predictor part of the formula\n",
    "    stressor_formula_part = ' + '.join(monthly_stressors)\n",
    "    \n",
    "    # Construct the complete R-style formula string.\n",
    "    # We include our controls (year, spatial splines) and all potential predictors.\n",
    "    # Note: patsy's bs() function creates the basis spline columns.\n",
    "    formula = f\"yield_rice ~ year + bs(lat, df=4) + bs(lon, df=4) + {stressor_formula_part}\"\n",
    "    \n",
    "    print(\"\\nGenerated model formula for patsy:\")\n",
    "    print(formula) # Uncomment to see the full, very long formula string\n",
    "\n",
    "    # --- 3. Create the Design Matrix (X) and Response Vector (y) ---\n",
    "    # patsy processes the formula and the dataframe to create the matrices needed for modeling.\n",
    "    # 'y' will be our dependent variable, 'X' will be the full set of predictors.\n",
    "    # The intercept is automatically included in 'X' by patsy.\n",
    "    print(\"\\nCreating design matrix (X) and response vector (y) using patsy...\")\n",
    "    y, X = dmatrices(formula, data=df_rice, return_type='dataframe')\n",
    "    \n",
    "    print(f\"Successfully created response vector y with shape: {y.shape}\")\n",
    "    print(f\"Successfully created design matrix X with shape: {X.shape}\")\n",
    "    print(f\"The {X.shape[1]} columns in X include the intercept, year, 8 spline bases (4 for lat, 4 for lon), and {len(monthly_stressors)} climate stressors.\")\n",
    "\n",
    "    # --- 4. Standardize the Predictor Matrix (X) ---\n",
    "    # This is the critical step we discussed. We scale ALL predictors to have a mean of 0 and a standard deviation of 1.\n",
    "    # This ensures the regularization penalty is applied fairly to all variables.\n",
    "    # We do NOT scale the response variable y.\n",
    "    print(\"\\nStandardizing the design matrix X...\")\n",
    "    \n",
    "    # We remove the Intercept column before scaling, as it should not be regularized or scaled.\n",
    "    # We will add it back later if needed, but scikit-learn's models handle it by default.\n",
    "    X_no_intercept = X.drop('Intercept', axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_values = scaler.fit_transform(X_no_intercept)\n",
    "    \n",
    "    # Convert the scaled array back to a pandas DataFrame with the original column names\n",
    "    X_scaled = pd.DataFrame(X_scaled_values, columns=X_no_intercept.columns, index=X.index)\n",
    "    \n",
    "    print(\"Standardization complete.\")\n",
    "    \n",
    "    # Verification: Check the mean and standard deviation of a few scaled columns\n",
    "    print(\"\\n--- Verification of Standardization ---\")\n",
    "    verification_cols = ['year', 'bs(lat, df=4)[0]', 'temperature_Jul']\n",
    "    for col in verification_cols:\n",
    "        mean_val = X_scaled[col].mean()\n",
    "        std_val = X_scaled[col].std()\n",
    "        print(f\"Column '{col}': Mean = {mean_val:.4f}, Std Dev = {std_val:.4f}\")\n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be4a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume 'y' and 'X_scaled' are already in memory from the previous step.\n",
    "# If not, you would need to re-run the data preparation script.\n",
    "\n",
    "try:\n",
    "    # --- 1. Define the GLM Model ---\n",
    "    # We specify our model family (Gamma) and the link function (log) as per our project plan.\n",
    "    # We pass the prepared y and the fully scaled X matrix.\n",
    "    # Note: statsmodels requires the intercept to be in the X matrix, which patsy provided.\n",
    "    \n",
    "    # We need to add the intercept back to the scaled data for statsmodels GLM\n",
    "    X_scaled_with_intercept = X.copy() # Start with the original X to preserve intercept and structure\n",
    "    X_scaled_with_intercept[X_no_intercept.columns] = X_scaled # Replace non-intercept columns with scaled versions\n",
    "\n",
    "    gl_gamma = sm.GLM(y, X_scaled_with_intercept, family=sm.families.Gamma(link=sm.families.links.log()))\n",
    "    print(\"Successfully initialized Gamma GLM with a log link.\")\n",
    "\n",
    "    # --- 2. Set up the Regularization Path ---\n",
    "    # We need to test a series of alpha values (penalty strengths).\n",
    "    # A logarithmic scale is best for this, from a weak penalty to a strong one.\n",
    "    n_alphas = 100\n",
    "    alphas = np.logspace(-3, 0.5, n_alphas) # From 0.001 to ~3.16\n",
    "\n",
    "    # The L1_wt parameter controls the Elastic Net mix (0=Ridge, 1=Lasso). \n",
    "    # 0.5 is a balanced choice.\n",
    "    elastic_net_l1_wt = 0.5 \n",
    "    \n",
    "    print(f\"Will fit the model for {n_alphas} alpha values with L1_wt (l1_ratio) = {elastic_net_l1_wt}\")\n",
    "\n",
    "    # --- 3. Fit the Model for Each Alpha and Store Coefficients ---\n",
    "    # We will loop through our alphas and save the coefficients from each model fit.\n",
    "    coefficients = []\n",
    "    \n",
    "    for alpha_val in alphas:\n",
    "        # The fit_regularized method performs the Elastic Net estimation.\n",
    "        # We set refit=False because we want to see the shrunken coefficients for this analysis.\n",
    "        results = gl_gamma.fit_regularized(\n",
    "            method='elastic_net', \n",
    "            alpha=alpha_val, \n",
    "            L1_wt=elastic_net_l1_wt,\n",
    "            refit=False \n",
    "        )\n",
    "        coefficients.append(results.params)\n",
    "    \n",
    "    # Convert the list of coefficient series into a DataFrame for easy plotting\n",
    "    coef_df = pd.DataFrame(coefficients, index=alphas)\n",
    "    coef_df.index.name = \"alpha\"\n",
    "    \n",
    "    # Exclude the Intercept for plotting, as it's not regularized and has a different scale.\n",
    "    coef_df_no_intercept = coef_df.drop('Intercept', axis=1)\n",
    "    \n",
    "    print(\"\\nCompleted fitting models along the regularization path.\")\n",
    "\n",
    "    # --- 4. Visualize the Regularization Path ---\n",
    "    print(\"Generating the regularization path plot...\")\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    ax.plot(coef_df_no_intercept)\n",
    "    ax.set_xscale('log') # The alpha path is best viewed on a log scale\n",
    "    \n",
    "    # Add a vertical line at zero\n",
    "    ax.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    \n",
    "    ax.set_title('Regularization Path for Rice Model Coefficients (Elastic Net)', fontsize=18)\n",
    "    ax.set_xlabel('Alpha (Penalty Strength)', fontsize=14)\n",
    "    ax.set_ylabel('Standardized Coefficient Value', fontsize=14)\n",
    "    \n",
    "    # To avoid a cluttered legend, we don't add one here. The goal is to see the general pattern.\n",
    "    # Alternatively, for fewer variables, a legend could be useful:\n",
    "    # ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"ERROR: Make sure that 'y' and 'X_scaled' DataFrames from the previous step are available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ea5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Corrected code to identify the most robust variables ---\n",
    "# We will inspect the coefficients at a moderately high alpha value\n",
    "# This tells us which variables \"survived\" the penalty the longest.\n",
    "alpha_to_inspect = 0.03 \n",
    "\n",
    "try:\n",
    "    # Find the alpha in our index that is closest to our target\n",
    "    # CORRECTED LINE: The operation works directly on the index without .flat\n",
    "    closest_alpha = coef_df.index[np.abs(coef_df.index - alpha_to_inspect).argmin()]\n",
    "\n",
    "    print(f\"--- Coefficients at alpha  {closest_alpha:.4f} ---\")\n",
    "\n",
    "    # Get the coefficients at this alpha and sort them by absolute value\n",
    "    robust_coeffs = coef_df.loc[closest_alpha].copy()\n",
    "    robust_coeffs_sorted = robust_coeffs.abs().sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nVariables sorted by the absolute magnitude of their shrunken coefficient:\")\n",
    "    # We display more variables to get a fuller picture\n",
    "    print(robust_coeffs_sorted.head(15))\n",
    "\n",
    "    # Let's also see their actual values (positive or negative) for the top variables\n",
    "    print(\"\\n--- Actual coefficient values for the most robust variables ---\")\n",
    "    print(coef_df.loc[closest_alpha, robust_coeffs_sorted.index].head(10))\n",
    "\n",
    "except NameError:\n",
    "     print(\"ERROR: Make sure that 'coef_df' DataFrame from the previous step is available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb97e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Base Champion Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_rice = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the Champion Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    champion_formula = \"yield_rice ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + soil_water_Aug + solar_radiation\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    base_model = smf.glm(\n",
    "        formula=champion_formula,\n",
    "        data=df_rice,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    base_model_results = base_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(base_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Future Comparison ---\n",
    "    # The AIC is a key metric for comparing different model formulations. Lower is better.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Base Champion Model: {base_model_results.aic:.2f}\")\n",
    "    print(\"This will be our benchmark for comparison.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d9518",
   "metadata": {},
   "source": [
    "### interpretation comments:\n",
    "\n",
    "1. The solar_radiation coef is tiny. The coefficient of -5.25e-10 means that for a one-unit increase in solar_radiation, the log of yield decreases by that amount. The units of solar radiation are very large. A \"one-unit\" increase is a physically meaningless, tiny change. Because a single unit change is so small, the coefficient associated with it must also be minuscule. What matters is its sign (negative) and its p-value (0.000). The model is telling us with very high confidence that there is a real, negative relationship. \n",
    "\n",
    "2. The soil_water_Aug coef is large. The units for soil_water_Aug are a proportion or a percentage (e.g., ranging from 0.1 to 0.5, representing 10% to 50% volumetric water content). A \"one-unit increase\" in this variable would be a massive change (e.g., from 0.2 to 1.2), which is often physically impossible.\n",
    "\n",
    "3. Our model explains almost 70% of the variation in rice yield. \n",
    "\n",
    "4. Control Variables\n",
    "    *   `year`: The coefficient is positive and highly significant (p=0.000). For each passing year, yield increases by about 0.48% (`e^0.0048  1.0048`), confirming a strong technological trend.\n",
    "    *   `bs(lat, df=4)` & `bs(lon, df=4)`: As a group, these are overwhelmingly significant. This confirms our earlier finding: **geography is a dominant driver of rice yield.**\n",
    "\n",
    "5. Climate Stressors\n",
    "    *   `potential_evaporation_May`: Positive and highly significant (p=0.000). Higher potential evaporation in May (proxy for sun and warmth) is associated with higher yields, likely by promoting strong early plant growth.\n",
    "    *   `soil_water_Aug`: Positive and highly significant (p=0.000). More available water in the soil during the critical grain-filling month of August is strongly associated with higher yields. This makes perfect agronomic sense.\n",
    "    *   `solar_radiation`: Negative and highly significant (p=0.000). After controlling for everything else, higher season-average solar radiation (a proxy for overall heat/sun stress) is associated with lower yields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ea2a1",
   "metadata": {},
   "source": [
    "## plan for testing the following things to see if they improve our model:\n",
    "\n",
    "1. Add a quadratic (squared) term for the most influential climate variables identified in the base model. This would be I(potential_evaporation_May**2)\n",
    "\n",
    "2. Add an interaction term between the two most plausible climate variables. Something like soil_water_Aug * solar_radiation\n",
    "\n",
    "3. Fit an alternative model where the aggregate solar_radiation is replaced by its most likely monthly counterpart (e.g., solar_radiation_Jul or solar_radiation_Aug).\n",
    "\n",
    "4. Identify the \"first variable to be eliminated\" from our regularization path and add it back into our champion model to see if it contributes meaningfully. I am not sure tho if this is worth it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190553d",
   "metadata": {},
   "source": [
    "## Add a quadratic (squared) term for the most influential climate variables identified in the base model. This would be I(potential_evaporation_May**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbd6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Testing for Non-Linearity (Quadratic Term) ---\")\n",
    "print(\"--- Model with squared term for potential_evaporation_May ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_rice = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the Model with a Quadratic Term ---\n",
    "    # We add I(potential_evaporation_May**2) to test for a non-linear, U-shaped effect.\n",
    "    # The I() function ensures that the squaring operation is performed mathematically.\n",
    "    quadratic_formula = \"yield_rice ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + I(potential_evaporation_May**2) + soil_water_Aug + solar_radiation\"\n",
    "\n",
    "    # Initialize the GLM model using the new formula.\n",
    "    quadratic_model = smf.glm(\n",
    "        formula=quadratic_formula,\n",
    "        data=df_rice,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model.\n",
    "    quadratic_model_results = quadratic_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    print(\"--- Summary of the Model with Quadratic Term ---\")\n",
    "    print(quadratic_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to the Base Champion Model's AIC. A lower value indicates a better model.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Quadratic Model: {quadratic_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the Base Champion Model's AIC.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4ece4",
   "metadata": {},
   "source": [
    "### interpretations\n",
    "\n",
    "Looking at the potential_evaporation_May coef, it is now negative, meanwhile the I(potential_evaporation_May ** 2) is positive. This combination describes a U-shaped curve that opens upwards. \n",
    "\n",
    "At low levels of potential evaporation in May, increasing it seems to have a small negative effect on yield. This could be because very low potential evaporation is associated with cold, damp, overcast conditions which are not ideal for newly established rice plants. However, once potential evaporation crosses a certain threshold, the positive squared term begins to dominate, and its effect becomes strongly positive. This aligns with the idea that sunny, warm conditions in May promote vigorous growth that leads to higher yields. \n",
    "\n",
    "We also tested to see if adding a double quadratic term (current model plus I(soil_water_Aug ** 2)), which gave us a model that had an AIC of 4817.59. Adding I(soil_water_Aug ** 2) was also not significant with a pval of >0.6. So we rejected this new model and are continuing with the previous one (the one above). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed596e7",
   "metadata": {},
   "source": [
    "## Add an interaction term between the two most plausible climate variables. Something like soil_water_Aug * solar_radiation\n",
    "\n",
    "We want to look at interractions that are plausible from an agronomic or physical standpoint.\n",
    "\n",
    "1.  **Hypothesis 1 (Water & Sun - MOST PLAUSIBLE):** The effect of ample `soil_water_Aug` on yield is *stronger* during seasons with very high `solar_radiation` (i.e., high heat and sun stress). In other words, water is more valuable when it's hot and sunny. This is a classic and highly plausible interaction.\n",
    "    *   **Formula Term:** `soil_water_Aug:solar_radiation`\n",
    "\n",
    "2.  **Hypothesis 2 (Early Growth & Water):** The benefit of `soil_water_Aug` depends on the `potential_evaporation_May`. This is also plausible. A plant that established well in May (good conditions) might be able to utilize August water more efficiently.\n",
    "    *   **Formula Term:** `potential_evaporation_May:soil_water_Aug`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b1746",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Testing for Interaction Effect (Water x Sun) ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_rice = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the Model with the Interaction Term ---\n",
    "    # We build on our best model by adding a single, agronomically-plausible interaction.\n",
    "    interaction_formula = \"yield_rice ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + I(potential_evaporation_May**2) + soil_water_Aug + solar_radiation + soil_water_Aug:solar_radiation\"\n",
    "\n",
    "    # Initialize the GLM model using the new formula.\n",
    "    interaction_model = smf.glm(\n",
    "        formula=interaction_formula,\n",
    "        data=df_rice,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model.\n",
    "    interaction_model_results = interaction_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    print(\"--- Summary of the Model with Interaction Term ---\")\n",
    "    print(interaction_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion model's AIC (4815.85).\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Interaction Model: {interaction_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the Quadratic Model's AIC (4815.85).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Testing for Interaction Effect ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_rice = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the Model with the Interaction Term ---\n",
    "    # We build on our best model by adding a single, agronomically-plausible interaction.\n",
    "    interaction_formula = \"yield_rice ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + I(potential_evaporation_May**2) + soil_water_Aug + solar_radiation + potential_evaporation_May:soil_water_Aug\"\n",
    "\n",
    "    # Initialize the GLM model using the new formula.\n",
    "    interaction_model = smf.glm(\n",
    "        formula=interaction_formula,\n",
    "        data=df_rice,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model.\n",
    "    interaction_model_results = interaction_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    print(\"--- Summary of the Model with Interaction Term ---\")\n",
    "    print(interaction_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion model's AIC (4815.85).\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Interaction Model: {interaction_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the Quadratic Model's AIC (4815.85).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b3cb15",
   "metadata": {},
   "source": [
    "### interpretations and questions\n",
    "\n",
    "Okay. The first new additions arent significant. So we reject the model (Aic score is really not that different at all). Our model also became unstable. So our best model is still the: \n",
    "\n",
    "yield_rice ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + I(potential_evaporation_May**2) + soil_water_Aug + solar_radiation\n",
    "\n",
    "Would it be worth to look into if the specific solar_radiation_Jul/Jun/May... had an effect here instead of a growing season average? Or do we simply move on? Our idea was to take the current best model which uses solar_radiation and replace it with each of the solar_radiation_month to test the monthly alternatives. And if there is an alternative that is just as good or better, we could retry the interraction term? \n",
    "\n",
    "\n",
    "The second new model is shpwing a good improvement in AIC, can explain more of the variance in rice yield, and even though potentia_evaporation_May is not significant anymore, since the new interraction term is significant, due to Principle of Marginality, we still keep the non significant term in. When there is an interaction, the meaning of the main effect's coefficient changes. The coefficient for potential_evaporation_May (0.0003) now represents the effect of May potential evaporation only when soil_water_Aug is exactly zero. Since a value of zero for soil water is not physically meaningful in our data, it's not surprising that this specific coefficient is not significant.\n",
    "\n",
    "- soil_water_Aug coefficient is positive (2.3056).\n",
    "\n",
    "- potential_evaporation_May:soil_water_Aug coefficient is negative (-0.0075).\n",
    "\n",
    "On average, more August soil water is strongly beneficial to yield (the positive main effect). However, the negative interaction term acts as a \"dimmer switch.\" The positive effect of August water is weakened as May potential evaporation increases. If the rice crop had a very sunny, warm, and stressful start in May (high potential evaporation), it is less able to take full advantage of ample water in August. Conversely, a crop that had a milder, less stressful start in May is more efficient at converting late-season water into final yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32097b4",
   "metadata": {},
   "source": [
    "## Testing solar radiation monethly alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "print(\"--- Final Model Check: Testing Monthly Alternative for Solar Radiation ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_rice = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the Model with the Monthly Alternative ---\n",
    "    # This formula is identical to our champion, but replaces the aggregate 'solar_radiation'\n",
    "    # with the specific 'solar_radiation_Aug'.\n",
    "    monthly_alt_formula = \"yield_rice ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + I(potential_evaporation_May**2) + soil_water_Aug + solar_radiation_Jun + potential_evaporation_May:soil_water_Aug\"\n",
    "\n",
    "    # Initialize the GLM model using the new formula.\n",
    "    monthly_alt_model = smf.glm(\n",
    "        formula=monthly_alt_formula,\n",
    "        data=df_rice,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model.\n",
    "    monthly_alt_model_results = monthly_alt_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    print(\"--- Summary of the Model with Monthly Solar Radiation ---\")\n",
    "    print(monthly_alt_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Final Comparison ---\n",
    "    # This is the final decision point. Is this AIC lower than our champion's?\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Monthly Alternative Model: {monthly_alt_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to our Champion Model's AIC (4806.65).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758f252",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "We tested all the potential growing season months, and found that solar_radiation_Jun made our model improve the msot, and fit our data better. All stressors satyed significant (besides the potential_evaporation_mayb, but see the interpretation on the interraction term to see why thats fine (Principle of Marginality)), and the model is able toe xplain almost 72% of the variation in rice yield.\n",
    "\n",
    "The early-season conditions are best described by a complex, U-shaped, and interactive effect of May's potential evaporation, while the primary summer stress comes specifically from June's solar radiation.\n",
    "\n",
    "New champ model:\n",
    "yield_rice ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + I(potential_evaporation_May**2) + soil_water_Aug + solar_radiation_Jun + potential_evaporation_May:soil_water_Aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1aca2f",
   "metadata": {},
   "source": [
    "# Final Model Summary and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "print(\"--- Final Model Interpretation ---\")\n",
    "print(\"--- Summary of the Final Champion Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_rice = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the Final Champion Model ---\n",
    "    # This formula includes the quadratic term and the significant interaction we discovered.\n",
    "    final_champion_formula = \"yield_rice ~ year + bs(lat, df=4) + bs(lon, df=4) + \" \\\n",
    "    \"potential_evaporation_May + I(potential_evaporation_May**2) + \" \\\n",
    "    \"soil_water_Aug + solar_radiation_Jun + \" \\\n",
    "    \"potential_evaporation_May:soil_water_Aug\"\n",
    "\n",
    "    # Initialize and fit the final GLM model.\n",
    "    final_model = smf.glm(\n",
    "        formula=final_champion_formula,\n",
    "        data=df_rice,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "    final_model_results = final_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    print(\"--- Summary of the Final Champion Model ---\")\n",
    "    print(final_model_results.summary())\n",
    "\n",
    "    # --- 4. Print Final AIC ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Final Champion Model AIC: {final_model_results.aic:.2f}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d047e",
   "metadata": {},
   "source": [
    "### Detailed Interpretation of the Final Champion Model\n",
    "\n",
    "This model is our final champion after testingg several different potential versions, as well as imploying regularization. \n",
    "\n",
    "#### Overall Model Performance\n",
    "\n",
    "*   **Predictive Power:** The **Pseudo R-squared (CS) is 0.7194**, indicating that our model explains approximately **72% of the variation in rice yield**. This is a strong fit for a model based on climate and spatial data alone.\n",
    "*   **Model Selection:** The final **AIC is 4800.07**. This is the lowest AIC achieved among all tested models, confirming that it represents the best balance of predictive power and parsimony.\n",
    "\n",
    "#### Interpretation of Control Variables\n",
    "\n",
    "These variables correctly account for non-climate factors, allowing us to isolate the true effect of the stressors.\n",
    "\n",
    "*   **`year` (Technology Trend):** The coefficient (`0.0046`) is positive and highly significant (p=0.000). This confirms a consistent underlying trend of yield improvement over time, likely due to advancements in technology, seeds, and farming practices. For each year, yields increase by approximately 0.46%. However it is worth noting that we might be losing some climate signal byu including this, as we also ecpect that the climate has followed an upwards trent. \n",
    "*   **`bs(lat, df=4)` and `bs(lon, df=4)` (Spatial Effects):** The spline terms for latitude and longitude are jointly highly significant. This tells us that **geography is a dominant factor**. The specific location, with its unique soil properties, topography, and water management, is a primary driver of rice yield. Controlling for this spatial variation is essential for accurately estimating the climate effects.\n",
    "\n",
    "#### Interpretation of Key Climate Effects\n",
    "\n",
    "This is the core of our project. The model has identified three critical periods and stressors that drive year-to-year variability in rice yield.\n",
    "\n",
    "1.  **June Solar Radiation (Early Summer Stress):**\n",
    "    *   The `solar_radiation_Jun` coefficient is **negative and highly significant** (p=0.000).\n",
    "    *   **Story:** This is our most straightforward stressor. High solar radiation in Junea proxy for intense sun and heatis detrimental to rice yields. This occurs during the crucial late-vegetative and early reproductive stages, where excessive stress can harm canopy development and panicle formation, thus limiting the ultimate yield potential.\n",
    "\n",
    "2.  **August Soil Water (Late-Season Water Availability):**\n",
    "    *   The `soil_water_Aug` main effect is **positive and highly significant** (p=0.000).\n",
    "    *   **Story:** Ample soil water during the grain-filling period in August is strongly beneficial for yield. This aligns perfectly with agronomic principles: water is a key input for photosynthesis and for transporting nutrients to the developing grains. Water stress at this stage is known to be highly damaging.\n",
    "\n",
    "3.  **The May Potential Evaporation Interaction (Complex Early-Season Effect):**\n",
    "    *   This is our most sophisticated finding, captured by three significant terms working together: `I(potential_evaporation_May ** 2)`, `soil_water_Aug`, and their interaction `potential_evaporation_May:soil_water_Aug`.\n",
    "    *   **Story:** This complex term describes how early-season conditions set the stage for the rest of the season. The significant interaction tells us: **The benefit of having plentiful water in August is dependent on the conditions back in May.**\n",
    "        *   The negative interaction coefficient (`-0.0076`) means that if the crop endured a stressful start in May (high potential evaporation), its ability to utilize August water is *reduced*.\n",
    "        *   Conversely, a crop that had a milder start in May is better equipped to convert late-season water into high yields. This is a powerful, data-driven insight into the compounding nature of climate effects across a growing season."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0b8ad",
   "metadata": {},
   "source": [
    "## visalizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ab88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"--- Step 4b: Generating Final, Corrected Yield Response Curves for Rice (with y-axis fix) ---\")\n",
    "\n",
    "# --- 1. Load Data and Fit the Final Champion Model ---\n",
    "file_path = '../data-cherry-pick/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_rice = pd.read_csv(file_path)\n",
    "    df_rice = df_rice[df_rice['yield_rice'] > 0].copy() # Ensure positive yield\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # This is our confirmed Final Champion Model for Rice\n",
    "    final_champion_formula = \"yield_rice ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + I(potential_evaporation_May**2) + soil_water_Aug + solar_radiation_Jun + potential_evaporation_May:soil_water_Aug\"\n",
    "\n",
    "    final_model = smf.glm(\n",
    "        formula=final_champion_formula,\n",
    "        data=df_rice,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    ).fit()\n",
    "    print(f\"Final champion model for rice has been successfully fitted. AIC: {final_model.aic:.2f}\")\n",
    "\n",
    "    # --- 2. Define the \"Typical Case\" Correctly ---\n",
    "    median_values = {\n",
    "        'year': df_rice['year'].median(),\n",
    "        'potential_evaporation_May': df_rice['potential_evaporation_May'].median(),\n",
    "        'soil_water_Aug': df_rice['soil_water_Aug'].median(),\n",
    "        'solar_radiation_Jun': df_rice['solar_radiation_Jun'].median(),\n",
    "        'lat': df_rice['lat'].median(),\n",
    "        'lon': df_rice['lon'].median()\n",
    "    }\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # --- Plot 1: Yield Response to June Solar Radiation ---\n",
    "    print(\"\\nGenerating Plot 1: Yield Response to June Solar Radiation...\")\n",
    "    \n",
    "    solar_range = np.linspace(df_rice['solar_radiation_Jun'].min(), df_rice['solar_radiation_Jun'].max(), 100)\n",
    "    X_pred_solar = pd.DataFrame(median_values, index=range(100))\n",
    "    X_pred_solar['solar_radiation_Jun'] = solar_range\n",
    "\n",
    "    pred_solar = final_model.get_prediction(X_pred_solar).summary_frame(alpha=0.05)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.plot(X_pred_solar['solar_radiation_Jun'], pred_solar['mean'], color='blue', linewidth=3, label='Predicted Yield')\n",
    "    ax.fill_between(X_pred_solar['solar_radiation_Jun'], pred_solar['mean_ci_lower'], pred_solar['mean_ci_upper'], color='blue', alpha=0.2, label='95% Confidence Interval')\n",
    "    ax.scatter(df_rice['solar_radiation_Jun'], df_rice['yield_rice'], alpha=0.05, color='gray', label='Raw Data')\n",
    "    \n",
    "    ax.set_title('Yield Response to June Solar Radiation (Corrected)', fontsize=16)\n",
    "    ax.set_xlabel('Solar Radiation in June (units)', fontsize=12)\n",
    "    ax.set_ylabel('Predicted Yield (tonnes per hectare)', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(bottom=0)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot 2: Non-Linear Yield Response to May Potential Evaporation ---\n",
    "    print(\"Generating Plot 2: Non-Linear Yield Response to May Potential Evaporation...\")\n",
    "\n",
    "    pemay_range = np.linspace(df_rice['potential_evaporation_May'].min(), df_rice['potential_evaporation_May'].max(), 100)\n",
    "    X_pred_pemay = pd.DataFrame(median_values, index=range(100))\n",
    "    X_pred_pemay['potential_evaporation_May'] = pemay_range\n",
    "\n",
    "    pred_pemay = final_model.get_prediction(X_pred_pemay).summary_frame(alpha=0.05)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.plot(X_pred_pemay['potential_evaporation_May'], pred_pemay['mean'], color='green', linewidth=3, label='Predicted Yield')\n",
    "    ax.fill_between(X_pred_pemay['potential_evaporation_May'], pred_pemay['mean_ci_lower'], pred_pemay['mean_ci_upper'], color='green', alpha=0.2, label='95% Confidence Interval')\n",
    "    ax.scatter(df_rice['potential_evaporation_May'], df_rice['yield_rice'], alpha=0.05, color='gray', label='Raw Data')\n",
    "    \n",
    "    ax.set_title('Non-Linear Yield Response to May Potential Evaporation (Corrected)', fontsize=16)\n",
    "    ax.set_xlabel('Potential Evaporation in May (units)', fontsize=12)\n",
    "    ax.set_ylabel('Predicted Yield (tonnes per hectare)', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(bottom=0)\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Plot 3: The Interaction of August Soil Water and May PE ---\n",
    "    print(\"Generating Plot 3: The Interaction Effect...\")\n",
    "    \n",
    "    pe_may_quantiles = df_rice['potential_evaporation_May'].quantile([0.25, 0.5, 0.75])\n",
    "    scenarios = {\n",
    "        'Low May PE (25th Pct)': {'value': pe_may_quantiles[0.25], 'color': 'orange'},\n",
    "        'Median May PE (50th Pct)': {'value': pe_may_quantiles[0.50], 'color': 'purple'},\n",
    "        'High May PE (75th Pct)': {'value': pe_may_quantiles[0.75], 'color': 'red'}\n",
    "    }\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.scatter(df_rice['soil_water_Aug'], df_rice['yield_rice'], alpha=0.05, color='gray', label='Raw Data')\n",
    "\n",
    "    for scenario_name, props in scenarios.items():\n",
    "        soil_water_range = np.linspace(df_rice['soil_water_Aug'].min(), df_rice['soil_water_Aug'].max(), 100)\n",
    "        X_pred_interact = pd.DataFrame(median_values, index=range(100))\n",
    "        \n",
    "        X_pred_interact['potential_evaporation_May'] = props['value']\n",
    "        X_pred_interact['soil_water_Aug'] = soil_water_range\n",
    "        \n",
    "        pred_interact = final_model.get_prediction(X_pred_interact).summary_frame(alpha=0.05)\n",
    "        \n",
    "        ax.plot(X_pred_interact['soil_water_Aug'], pred_interact['mean'], color=props['color'], linewidth=3, label=scenario_name)\n",
    "        ax.fill_between(X_pred_interact['soil_water_Aug'], pred_interact['mean_ci_lower'], pred_interact['mean_ci_upper'], color=props['color'], alpha=0.1)\n",
    "\n",
    "    ax.set_title('Interaction Effect: August Water Benefit Depends on May Conditions (Corrected)', fontsize=16)\n",
    "    ax.set_xlabel('Soil Water in August (units)', fontsize=12)\n",
    "    ax.set_ylabel('Predicted Yield (tonnes per hectare)', fontsize=12)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    order = [1, 2, 3, 0]\n",
    "    ax.legend([handles[idx] for idx in order], [labels[idx] for idx in order], title='Scenario')\n",
    "    \n",
    "    # --- THIS IS THE MODIFIED LINE ---\n",
    "    # Zoom in on the y-axis to make the interaction effect clearer.\n",
    "    ax.set_ylim(4, 7)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2129b144",
   "metadata": {},
   "source": [
    "### **Analysis of Corrected Yield Response Curves (Rice)**\n",
    "\n",
    "These plots visualize the key relationships from our final statistical model. They show the predicted rice yield (in tonnes per hectare) in response to specific climate stressors, holding all other factors at their typical values. The corrected prediction methodology ensures the yield levels are realistic and accurate.\n",
    "\n",
    "#### **Plot 1: Yield Response to June Solar Radiation (Corrected)**\n",
    "\n",
    "*   **Primary Finding:** Higher solar radiation in June is consistently associated with a decrease in rice yield.\n",
    "*   **Interpretation:** The plot shows a clear and almost linear downward trend. This is the direct visual confirmation of the significant negative coefficient for `solar_radiation_Jun` in our model. This finding suggests that intense sun and associated heat stress during the early-to-mid growing season is a key limiting factor for rice production in this region.\n",
    "*   **Confidence:** The model is highly confident in this negative relationship, as shown by the tight 95% confidence interval across the entire observed range of solar radiation.\n",
    "\n",
    "#### **Plot 2: Non-Linear Yield Response to May Potential Evaporation (Corrected)**\n",
    "\n",
    "*   **Primary Finding:** The effect of May potential evaporation is non-linear, exhibiting a distinct **U-shape**. \n",
    "*   **Interpretation:** The curve reveals an optimal range for May PE. The model predicts the lowest yields at a moderate PE level of approximately 150 units. Yields are predicted to be higher at both **low** PE values (likely representing cool, humid, low-stress conditions) and at **high** PE values (representing very sunny, high-energy conditions that may drive photosynthesis if water is not limited). This complex relationship would have been completely missed by a simple linear model.\n",
    "*   **Confidence:** The model is very confident in this U-shaped curve, with the tight confidence interval confirming the statistical significance of this non-linear effect.\n",
    "\n",
    "#### **Plot 3: Interaction of August Soil Water and May Conditions (Corrected)**\n",
    "\n",
    "*   **Primary Finding:** The positive impact of having more soil water in August is dependent on the weather conditions the crop experienced in May.\n",
    "*   **Interpretation:** With the corrected and \"zoomed-in\" y-axis, the interaction effect is now much clearer visually:\n",
    "    *   The **overall upward trend** of all three lines confirms that more soil water in August is strongly beneficial for yield during the critical grain-filling stage.\n",
    "    *   The **key insight** is that the slopes of the lines are different. The orange line (representing a mild start in May with low PE) is visibly the steepest. The red line (a stressful start with high PE) is the flattest.\n",
    "    *   This is the powerful visual confirmation of our statistical finding: a stressful start to the season **limits the crop's ability to take full advantage** of plentiful late-season water. A mild start allows the crop to use that water much more efficiently to produce higher yields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4637ed",
   "metadata": {},
   "source": [
    "## Vulnerability curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656bec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "print(\"--- Generating the Final, Corrected Vulnerability Curve for Rice ---\")\n",
    "\n",
    "# --- 1. Load Data and Fit the Final Champion Model ---\n",
    "file_path = '../data-cherry-pick/rice_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_rice = pd.read_csv(file_path)\n",
    "    df_rice = df_rice[df_rice['yield_rice'] > 0].copy() # Ensure positive yield\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # This is our confirmed Final Champion Model for Rice\n",
    "    final_champion_formula = \"yield_rice ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + I(potential_evaporation_May**2) + soil_water_Aug + solar_radiation_Jun + potential_evaporation_May:soil_water_Aug\"\n",
    "\n",
    "    final_model = smf.glm(\n",
    "        formula=final_champion_formula,\n",
    "        data=df_rice,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    ).fit()\n",
    "    print(f\"Final champion model for rice has been successfully fitted. AIC: {final_model.aic:.2f}\")\n",
    "\n",
    "    # --- 2. Define the \"Typical Case\" Correctly (THE FIX) ---\n",
    "    # Create a dictionary of median values that crucially includes lat and lon.\n",
    "    # This is the robust method that allows get_prediction to correctly handle the splines.\n",
    "    median_values = {\n",
    "        'year': df_rice['year'].median(),\n",
    "        'potential_evaporation_May': df_rice['potential_evaporation_May'].median(),\n",
    "        'soil_water_Aug': df_rice['soil_water_Aug'].median(),\n",
    "        'solar_radiation_Jun': df_rice['solar_radiation_Jun'].median(),\n",
    "        'lat': df_rice['lat'].median(),  # Added lat\n",
    "        'lon': df_rice['lon'].median()   # Added lon\n",
    "    }\n",
    "\n",
    "    # --- 3. Define and Predict the \"Baseline\" Yield ---\n",
    "    # The baseline is the yield for a typical case, with the stressor (solar_rad_jun) at its median.\n",
    "    X_baseline = pd.DataFrame(median_values, index=[0])\n",
    "    \n",
    "    baseline_pred = final_model.get_prediction(X_baseline).summary_frame(alpha=0.05)\n",
    "    yield_baseline = baseline_pred['mean'].iloc[0]\n",
    "    print(f\"\\nPredicted baseline yield for a typical case: {yield_baseline:.2f} tonnes/hectare\")\n",
    "    \n",
    "    # --- 4. Predict Yield Across the Range of the Stressor ---\n",
    "    solar_range = np.linspace(df_rice['solar_radiation_Jun'].min(), df_rice['solar_radiation_Jun'].max(), 100)\n",
    "    \n",
    "    # Create the prediction grid where only the stressor varies\n",
    "    X_stress = pd.DataFrame(median_values, index=range(100))\n",
    "    X_stress['solar_radiation_Jun'] = solar_range\n",
    "\n",
    "    stress_pred = final_model.get_prediction(X_stress).summary_frame(alpha=0.05)\n",
    "    yield_predicted = stress_pred['mean']\n",
    "    yield_ci_lower = stress_pred['mean_ci_lower']\n",
    "    yield_ci_upper = stress_pred['mean_ci_upper']\n",
    "\n",
    "    # --- 5. Calculate Percentage Yield Change ---\n",
    "    yield_change_pct = ((yield_predicted - yield_baseline) / yield_baseline) * 100\n",
    "    yield_change_lower_pct = ((yield_ci_lower - yield_baseline) / yield_baseline) * 100\n",
    "    yield_change_upper_pct = ((yield_ci_upper - yield_baseline) / yield_baseline) * 100\n",
    "\n",
    "    # --- 6. Plot the Vulnerability Curve ---\n",
    "    print(\"Generating the vulnerability curve plot...\")\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    ax.plot(solar_range, yield_change_pct, color='red', label='Predicted Yield Change')\n",
    "    ax.fill_between(solar_range, yield_change_lower_pct, yield_change_upper_pct, color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "\n",
    "    ax.set_title('Vulnerability of Rice Yield to June Solar Radiation (Corrected)', fontsize=18)\n",
    "    ax.set_xlabel('Solar Radiation in June (units)', fontsize=12)\n",
    "    # Use a more accurate y-axis label\n",
    "    ax.set_ylabel('Yield Change vs. Typical Year (%)', fontsize=12)\n",
    "    ax.legend()\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5d11d",
   "metadata": {},
   "source": [
    "### **Analysis of the Corrected Vulner-ability Curve (Rice)**\n",
    "\n",
    "It translates the statistical relationship into a clear measure of risk and opportunity by showing the percentage change in rice yield in response to June solar radiation, compared to a typical year.\n",
    "\n",
    "#### **Primary Findings**\n",
    "\n",
    "This plot reveals a consistent and significant vulnerability to early-summer conditions:\n",
    "\n",
    "1.  **Favorable Conditions (Yield GAIN):**\n",
    "    *   When June solar radiation is **lower than average** (moving to the left of the 0% baseline), the model predicts a significant **yield gain**.\n",
    "    *   In years with the most favorable (coolest and cloudiest) June conditions, the predicted yield can be up to **+20% higher** than in a typical year. This quantifies the considerable upside potential of a mild start to the summer.\n",
    "\n",
    "2.  **Stressful Conditions (Yield LOSS):**\n",
    "    *   When June solar radiation is **higher than average** (moving to the right of the 0% baseline), the model predicts a **yield loss**.\n",
    "    *   In years with very high solar radiation (hot, sunny Junes), the predicted yield loss is approximately **-10%** compared to a typical year. This is a direct, quantitative measure of the downside risk from heat and sun stress.\n",
    "\n",
    "3.  **Model Confidence:**\n",
    "    *   The shaded 95% confidence interval is tight around the predicted line, indicating that the model is confident in this negative relationship.\n",
    "    *   The interval widens at the extremes (especially on the low-radiation side), which correctly reflects that the model has less certainty when predicting the outcome of very rare weather events.\n",
    "\n",
    "#### **Conclusion**\n",
    "\n",
    "The corrected vulnerability curve provides a clear and realistic summary of our project's key finding for rice. It demonstrates that rice yield in Northern Italy is highly sensitive to June solar radiation. The relationship is clear and consistent: while mild Junes can lead to a significant yield boost of up to 20%, hot and sunny Junes pose a clear risk, capable of reducing yields by around 10%. This realistic, quantitative insight is critical for understanding and mitigating the impacts of climate variability on regional agriculture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadedb33",
   "metadata": {},
   "source": [
    "### **Final Rice Model: Interpretation and Conclusions**\n",
    "\n",
    "This section summarizes the final champion model developed to explain the relationship between monthly climate stressors and rice yield in Northern Italy. The model is the result of a multi-step workflow designed to be statistically robust, parsimonious, and interpretable.\n",
    "\n",
    "#### **The Final Champion Model**\n",
    "\n",
    "After a data-driven process of variable selection and iterative refinement, the final, best-performing model was determined to be a Gamma GLM with the following structure:\n",
    "\n",
    "**Final Model Formula:**\n",
    "```\n",
    "yield_rice ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + I(potential_evaporation_May**2) + soil_water_Aug + solar_radiation_Jun + potential_evaporation_May:soil_water_Aug\n",
    "```\n",
    "\n",
    "**Key Performance Metrics:**\n",
    "*   **Akaike Information Criterion (AIC):** `4800.07` (The lowest of all tested models)\n",
    "*   **Pseudo R-squared (CS):** `0.7194` (Explains approx. **72%** of the variation in yield)\n",
    "\n",
    "#### **The Modeling Journey: How We Arrived Here**\n",
    "\n",
    "The final model was not assumed but was systematically built and validated:\n",
    "\n",
    "1.  **Variable Selection:** An **Elastic Net regularization** was used on a full model to identify robust predictors from a large set of correlated climate variables. This data-driven step selected `year`, spatial splines, `potential_evaporation_May`, `soil_water_Aug`, and aggregate `solar_radiation`.\n",
    "\n",
    "2.  **Testing for Non-Linearity:** We tested for a quadratic effect of `potential_evaporation_May`, which resulted in a **significant drop in AIC**, confirming a non-linear relationship was a better fit for the data.\n",
    "\n",
    "3.  **Testing for Interactions & Alternatives:** We tested two agronomically plausible interactions. The `potential_evaporation_May:soil_water_Aug` interaction proved **highly significant and massively improved the AIC**, becoming a core part of the model. A subsequent sensitivity analysis, prompted by this new finding, revealed that `solar_radiation_Jun` was a statistically superior predictor to the aggregate `solar_radiation`, leading to our final, most powerful model.\n",
    "\n",
    "This structured process ensures our final model is not overfit and that every included term is statistically justified and meaningful.\n",
    "\n",
    "#### **Detailed Interpretation of the Final Model**\n",
    "\n",
    "*   **Control Variables:**\n",
    "    *   `year`: The positive, significant coefficient confirms a strong **technological trend**, with rice yields consistently increasing over time.\n",
    "    *   `bs(lat, df=4)` & `bs(lon, df=4)`: The high significance of the spatial splines confirms that **geography is a dominant driver** of yield, accounting for underlying variations in soil, topography, and local infrastructure.\n",
    "\n",
    "*   **Key Climate Drivers:**\n",
    "    *   `solar_radiation_Jun`: This term has a strong, significant negative coefficient. It represents the most direct climate stressor, indicating that high solar radiation (and associated heat) during the early-to-mid growing season is consistently detrimental to rice yields.\n",
    "    *   `soil_water_Aug`: The main effect is strongly positive, highlighting the critical importance of ample water availability during the grain-filling period in August.\n",
    "    *   **The `May PE` x `August Water` Interaction**: This is the most sophisticated finding. The significant, negative interaction coefficient (`potential_evaporation_May:soil_water_Aug`) reveals a compounding climate effect. It means that the benefit of having plentiful water in August is **dependent on early-season conditions**. A stressful start in May (high potential evaporation) significantly reduces the crop's ability to efficiently use late-season water to build yield.\n",
    "\n",
    "#### **Insights from Visualization**\n",
    "\n",
    "*   **The Solar Radiation Response Curve:** The yield response plot for `solar_radiation_Jun` visually confirms a clear, consistent negative relationship. The plot shows a steady decline in yield as June solar radiation increases, quantifying the impact of this key stressor.\n",
    "\n",
    "*   **The May Potential Evaporation Curve:** The plot for `potential_evaporation_May` reveals a complex, non-linear **U-shape**. This shows an optimal range, where both very low and very high potential evaporation in May are more favorable for yield than moderate levels.\n",
    "\n",
    "*   **The Interaction Plot:** This plot beautifully visualizes the core insight. The yield response to `soil_water_Aug` is shown for different May scenarios. The curve for a \"Low May PE\" (a mild start) is visibly steeper than the curve for a \"High May PE\" (a stressful start), perfectly illustrating how a good start to the season allows the crop to take full advantage of late-season water.\n",
    "\n",
    "*   **The Final Vulnerability Curve:** This plot synthesizes the `solar_radiation_Jun` finding into a quantitative measure of risk. It shows that:\n",
    "    *   **Opportunity:** A cool, cloudy June can lead to a **yield gain of up to +20%** compared to a typical year.\n",
    "    *   **Vulnerability:** A hot, sunny June can lead to a **yield loss of around -10%**.\n",
    "\n",
    "#### **Overall Conclusion**\n",
    "\n",
    "The model provides a powerful and statistically robust explanation of rice yield vulnerability in Northern Italy. The findings highlight a complex interplay between early and late-season conditions. While consistent water in August is critical for filling the grain, the model's core insight is that **a stressful start in May can limit the crop's ultimate potential**, regardless of late-season conditions. Furthermore, the model clearly identifies **heat and sun stress in June** as a primary, direct driver of yield loss. These quantitative insights are crucial for understanding the compounding nature of climate risk for rice production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 40-maize-gamma-model.ipynb/40-maize-gamma-model.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8de28b9",
   "metadata": {},
   "source": [
    "# EDA for Maize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54350fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import scipy.stats as stats\n",
    "from patsy import dmatrices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c86e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis - Full Correlation Matrix\n",
    "\n",
    "print(\"--- EDA: Correlation Analysis of All Monthly Stressors ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# As confirmed, this file is already specific to maize and its growing season.\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Select Only the Monthly Stressor Variables ---\n",
    "    # We will select all columns that have a month name in them, which is a robust\n",
    "    # way to grab all the monthly predictors we want to investigate.\n",
    "    monthly_stressors = [col for col in df_maize.columns if '_' in col and 'yield' not in col]\n",
    "    df_corr = df_maize[monthly_stressors]\n",
    "    \n",
    "    print(f\"\\nSelected {len(df_corr.columns)} monthly stressor variables for correlation analysis.\")\n",
    "\n",
    "    # --- 3. Calculate and Print the Correlation Matrix ---\n",
    "    correlation_matrix = df_corr.corr()\n",
    "    \n",
    "    # Optional: If you want to see the full numerical matrix, uncomment the next line\n",
    "    # print(\"\\n--- Full Pairwise Correlation Matrix ---\")\n",
    "    # print(correlation_matrix)\n",
    "\n",
    "    # --- 4. Visualize the Matrix with a Heatmap ---\n",
    "    # A heatmap is the best way to see the broad patterns of collinearity.\n",
    "    print(\"\\nGenerating correlation heatmap...\")\n",
    "    \n",
    "    plt.figure(figsize=(18, 15))\n",
    "    heatmap = sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        cmap='coolwarm',  # Use a diverging colormap (red=positive, blue=negative)\n",
    "        center=0,         # Center the colormap at zero\n",
    "        vmin=-1,          # Set the color scale limits to the theoretical min/max\n",
    "        vmax=1,\n",
    "        linewidths=.5,\n",
    "        annot=False       # Annotations are turned off as the matrix is too large to be readable\n",
    "    )\n",
    "    \n",
    "    plt.title('Pairwise Correlation Matrix of All Monthly Stressors for Rice', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Extended EDA for Maize Yield Analysis ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- Task 1: Examine the Distribution of the Dependent Variable (yield_maize) ---\n",
    "    print(\"--- Task 1: Analyzing the distribution of the dependent variable 'yield_maize' ---\")\n",
    "    \n",
    "    # Create a figure with two subplots side-by-side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Distribution Analysis for yield_maize', fontsize=16)\n",
    "\n",
    "    # a) Histogram with a Kernel Density Estimate (KDE)\n",
    "    # This helps us visually assess the shape, center, and spread of the yield data.\n",
    "    # We are checking for positive skewness, which is characteristic of data modeled by a Gamma distribution.\n",
    "    sns.histplot(df_maize['yield_maize'], kde=True, ax=axes[0])\n",
    "    axes[0].set_title('Histogram of Maize Yield')\n",
    "    axes[0].set_xlabel('Yield (tonnes per hectare)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    # b) Q-Q (Quantile-Quantile) Plot against a theoretical normal distribution\n",
    "    # This plot helps us assess if the data's distribution follows a specific theoretical distribution.\n",
    "    # Deviations from the red line suggest skewness or heavy tails.\n",
    "    # While our target is a Gamma GLM, a Q-Q plot vs. Normal is a standard first step to detect non-normality.\n",
    "    stats.probplot(df_maize['yield_maize'], dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot of Maize Yield vs. Normal Distribution')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Distribution plots generated. Check for positive skew, which supports our choice of a Gamma GLM.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 2: Bivariate Scatter Plots of Yield vs. Key Stressors ---\n",
    "    print(\"--- Task 2: Visualizing relationships between yield and key climate stressors ---\")\n",
    "    \n",
    "    # Select a few key stressors based on agronomic theory for Maize\n",
    "    key_stressors = ['temperature_Jul', 'precipitation_Jun', 'soil_water_Aug']\n",
    "    \n",
    "    # Create a figure to hold the scatter plots\n",
    "    fig, axes = plt.subplots(1, len(key_stressors), figsize=(21, 6))\n",
    "    fig.suptitle('Bivariate Relationships: Maize Yield vs. Key Stressors', fontsize=16)\n",
    "\n",
    "    for i, stressor in enumerate(key_stressors):\n",
    "        # We use a regression plot with a LOWESS (Locally Weighted Scatterplot Smoothing) curve.\n",
    "        # This is a non-parametric way to see the underlying trend without assuming a linear relationship.\n",
    "        # It's excellent for spotting potential non-linearities (like an inverted 'U' shape).\n",
    "        sns.regplot(\n",
    "            x=stressor,\n",
    "            y='yield_maize',\n",
    "            data=df_maize,\n",
    "            ax=axes[i],\n",
    "            lowess=True, # Use LOWESS smoother to detect non-linear patterns\n",
    "            scatter_kws={'alpha': 0.1, 'color': 'gray'}, # De-emphasize individual points\n",
    "            line_kws={'color': 'blue'} # Emphasize the trend line\n",
    "        )\n",
    "        axes[i].set_title(f'Yield vs. {stressor}')\n",
    "        axes[i].set_xlabel(f'{stressor}')\n",
    "        axes[i].set_ylabel('Yield (tonnes per hectare)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Scatter plots generated. Look for non-linear patterns that might inform our final model.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 3: Plot Key Variables Over Time ---\n",
    "    print(\"--- Task 3: Examining long-term trends in yield and a key climate variable ---\")\n",
    "    \n",
    "    # Calculate the mean of yield and a key stressor for each year\n",
    "    yearly_data = df_maize.groupby('year')[['yield_maize', 'temperature_Jul']].mean().reset_index()\n",
    "\n",
    "    # Create a plot with a primary and secondary y-axis to show both trends together.\n",
    "    # This confirms the necessity of including 'year' as a control variable to capture trends\n",
    "    # likely related to technology, while also checking for climate trends.\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plotting average yield on the primary (left) y-axis\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Average Yield (tonnes per hectare)', color=color)\n",
    "    ax1.plot(yearly_data['year'], yearly_data['yield_maize'], color=color, marker='o', label='Average Yield')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Create a second y-axis that shares the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plotting average temperature on the secondary (right) y-axis\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Average July Temperature (C)', color=color)\n",
    "    ax2.plot(yearly_data['year'], yearly_data['temperature_Jul'], color=color, linestyle='--', marker='x', label='Avg. July Temp')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('Long-Term Trend of Maize Yield and July Temperature (1982-2016)', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    # Adding a single legend for both lines\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"Time-series plot generated. Note the clear upward trend in yield, confirming the need for a 'year' control variable.\\n\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: A required column was not found in the dataset: {e}. Please check the CSV file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2008f08",
   "metadata": {},
   "source": [
    "# Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Define the Full Model Formula ---\n",
    "    # Programmatically get all monthly stressor column names\n",
    "    monthly_stressors = [col for col in df_maize.columns if '_' in col and 'yield' not in col]\n",
    "    \n",
    "    # Join them with '+' to create the predictor part of the formula\n",
    "    stressor_formula_part = ' + '.join(monthly_stressors)\n",
    "    \n",
    "    # Construct the complete R-style formula string.\n",
    "    # We include our controls (year, spatial splines) and all potential predictors.\n",
    "    # Note: patsy's bs() function creates the basis spline columns.\n",
    "    formula = f\"yield_maize ~ year + bs(lat, df=4) + bs(lon, df=4) + {stressor_formula_part}\"\n",
    "    \n",
    "    print(\"\\nGenerated model formula for patsy:\")\n",
    "    print(formula) # Uncomment to see the full, very long formula string\n",
    "\n",
    "    # --- 3. Create the Design Matrix (X) and Response Vector (y) ---\n",
    "    # patsy processes the formula and the dataframe to create the matrices needed for modeling.\n",
    "    # 'y' will be our dependent variable, 'X' will be the full set of predictors.\n",
    "    # The intercept is automatically included in 'X' by patsy.\n",
    "    print(\"\\nCreating design matrix (X) and response vector (y) using patsy...\")\n",
    "    y, X = dmatrices(formula, data=df_maize, return_type='dataframe')\n",
    "    \n",
    "    print(f\"Successfully created response vector y with shape: {y.shape}\")\n",
    "    print(f\"Successfully created design matrix X with shape: {X.shape}\")\n",
    "    print(f\"The {X.shape[1]} columns in X include the intercept, year, 8 spline bases (4 for lat, 4 for lon), and {len(monthly_stressors)} climate stressors.\")\n",
    "\n",
    "    # --- 4. Standardize the Predictor Matrix (X) ---\n",
    "    # We scale ALL predictors to have a mean of 0 and a standard deviation of 1.\n",
    "    # This ensures the regularization penalty is applied fairly to all variables.\n",
    "    # We do NOT scale the response variable y.\n",
    "    print(\"\\nStandardizing the design matrix X...\")\n",
    "    \n",
    "    # We remove the Intercept column before scaling, as it should not be regularized or scaled.\n",
    "    # We will add it back later if needed, but scikit-learn's models handle it by default.\n",
    "    X_no_intercept = X.drop('Intercept', axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_values = scaler.fit_transform(X_no_intercept)\n",
    "    \n",
    "    # Convert the scaled array back to a pandas DataFrame with the original column names\n",
    "    X_scaled = pd.DataFrame(X_scaled_values, columns=X_no_intercept.columns, index=X.index)\n",
    "    \n",
    "    print(\"Standardization complete.\")\n",
    "    \n",
    "    # Verification: Check the mean and standard deviation of a few scaled columns\n",
    "    print(\"\\n--- Verification of Standardization ---\")\n",
    "    verification_cols = ['year', 'bs(lat, df=4)[0]', 'temperature_Jul']\n",
    "    for col in verification_cols:\n",
    "        mean_val = X_scaled[col].mean()\n",
    "        std_val = X_scaled[col].std()\n",
    "        print(f\"Column '{col}': Mean = {mean_val:.4f}, Std Dev = {std_val:.4f}\")\n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume 'y' and 'X_scaled' are already in memory from the previous step.\n",
    "# If not, you would need to re-run the data preparation script.\n",
    "\n",
    "try:\n",
    "    # --- 1. Define the GLM Model ---\n",
    "    # We specify our model family (Gamma) and the link function (log) as per our project plan.\n",
    "    # We pass the prepared y and the fully scaled X matrix.\n",
    "    # Note: statsmodels requires the intercept to be in the X matrix, which patsy provided.\n",
    "    \n",
    "    # We need to add the intercept back to the scaled data for statsmodels GLM\n",
    "    X_scaled_with_intercept = X.copy() # Start with the original X to preserve intercept and structure\n",
    "    X_scaled_with_intercept[X_no_intercept.columns] = X_scaled # Replace non-intercept columns with scaled versions\n",
    "\n",
    "    gl_gamma = sm.GLM(y, X_scaled_with_intercept, family=sm.families.Gamma(link=sm.families.links.log()))\n",
    "    print(\"Successfully initialized Gamma GLM with a log link.\")\n",
    "\n",
    "    # --- 2. Set up the Regularization Path ---\n",
    "    # We need to test a series of alpha values (penalty strengths).\n",
    "    # A logarithmic scale is best for this, from a weak penalty to a strong one.\n",
    "    n_alphas = 100\n",
    "    alphas = np.logspace(-3, 0.5, n_alphas) # From 0.001 to ~3.16\n",
    "\n",
    "    # The L1_wt parameter controls the Elastic Net mix (0=Ridge, 1=Lasso). \n",
    "    # 0.5 is a balanced choice.\n",
    "    elastic_net_l1_wt = 0.5 \n",
    "    \n",
    "    print(f\"Will fit the model for {n_alphas} alpha values with L1_wt (l1_ratio) = {elastic_net_l1_wt}\")\n",
    "\n",
    "    # --- 3. Fit the Model for Each Alpha and Store Coefficients ---\n",
    "    # We will loop through our alphas and save the coefficients from each model fit.\n",
    "    coefficients = []\n",
    "    \n",
    "    for alpha_val in alphas:\n",
    "        # The fit_regularized method performs the Elastic Net estimation.\n",
    "        # We set refit=False because we want to see the shrunken coefficients for this analysis.\n",
    "        results = gl_gamma.fit_regularized(\n",
    "            method='elastic_net', \n",
    "            alpha=alpha_val, \n",
    "            L1_wt=elastic_net_l1_wt,\n",
    "            refit=False \n",
    "        )\n",
    "        coefficients.append(results.params)\n",
    "    \n",
    "    # Convert the list of coefficient series into a DataFrame for easy plotting\n",
    "    coef_df = pd.DataFrame(coefficients, index=alphas)\n",
    "    coef_df.index.name = \"alpha\"\n",
    "    \n",
    "    # Exclude the Intercept for plotting, as it's not regularized and has a different scale.\n",
    "    coef_df_no_intercept = coef_df.drop('Intercept', axis=1)\n",
    "    \n",
    "    print(\"\\nCompleted fitting models along the regularization path.\")\n",
    "\n",
    "    # --- 4. Visualize the Regularization Path ---\n",
    "    print(\"Generating the regularization path plot...\")\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    ax.plot(coef_df_no_intercept)\n",
    "    ax.set_xscale('log') # The alpha path is best viewed on a log scale\n",
    "    \n",
    "    # Add a vertical line at zero\n",
    "    ax.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    \n",
    "    ax.set_title('Regularization Path for Rice Model Coefficients (Elastic Net)', fontsize=18)\n",
    "    ax.set_xlabel('Alpha (Penalty Strength)', fontsize=14)\n",
    "    ax.set_ylabel('Standardized Coefficient Value', fontsize=14)\n",
    "    \n",
    "    # To avoid a cluttered legend, we don't add one here. The goal is to see the general pattern.\n",
    "    # Alternatively, for fewer variables, a legend could be useful:\n",
    "    # ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"ERROR: Make sure that 'y' and 'X_scaled' DataFrames from the previous step are available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Corrected code to identify the most robust variables ---\n",
    "# We will inspect the coefficients at a moderately high alpha value\n",
    "# This tells us which variables \"survived\" the penalty the longest.\n",
    "alpha_to_inspect = 0.03 \n",
    "\n",
    "try:\n",
    "    # Find the alpha in our index that is closest to our target\n",
    "    # CORRECTED LINE: The operation works directly on the index without .flat\n",
    "    closest_alpha = coef_df.index[np.abs(coef_df.index - alpha_to_inspect).argmin()]\n",
    "\n",
    "    print(f\"--- Coefficients at alpha  {closest_alpha:.4f} ---\")\n",
    "\n",
    "    # Get the coefficients at this alpha and sort them by absolute value\n",
    "    robust_coeffs = coef_df.loc[closest_alpha].copy()\n",
    "    robust_coeffs_sorted = robust_coeffs.abs().sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nVariables sorted by the absolute magnitude of their shrunken coefficient:\")\n",
    "    # We display more variables to get a fuller picture\n",
    "    print(robust_coeffs_sorted.head(15))\n",
    "\n",
    "    # Let's also see their actual values (positive or negative) for the top variables\n",
    "    print(\"\\n--- Actual coefficient values for the most robust variables ---\")\n",
    "    print(coef_df.loc[closest_alpha, robust_coeffs_sorted.index].head(10))\n",
    "\n",
    "except NameError:\n",
    "     print(\"ERROR: Make sure that 'coef_df' DataFrame from the previous step is available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed63896",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Base Champion Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the Champion Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    champion_formula = \"yield_maize ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + soil_water + temperature_Jul + precipitation_Sep + precipitation_Jun\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    base_model = smf.glm(\n",
    "        formula=champion_formula,\n",
    "        data=df_maize,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    base_model_results = base_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(base_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Future Comparison ---\n",
    "    # The AIC is a key metric for comparing different model formulations. Lower is better.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Base Champion Model: {base_model_results.aic:.2f}\")\n",
    "    print(\"This will be our benchmark for comparison.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c7391",
   "metadata": {},
   "source": [
    "## analysis\n",
    "\n",
    "This is a really good starting point. High explanitory power and almost all variables are significant. We will test a simplified version of this model which excludes the non-significant variable next, and see if the model improves (The Principle of Parsimony). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded373d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting a Simplified Base Champion Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the Simplified Champion Model ---\n",
    "    # We have removed 'precipitation_Jun' based on its high p-value (0.587) in the previous model.\n",
    "    # This is a test of the Principle of Parsimony.\n",
    "    simplified_formula = \"yield_maize ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + soil_water + temperature_Jul + precipitation_Sep\"\n",
    "\n",
    "    # Initialize the GLM model using the new, simplified formula.\n",
    "    base_model_simplified = smf.glm(\n",
    "        formula=simplified_formula,\n",
    "        data=df_maize,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model.\n",
    "    base_model_simplified_results = base_model_simplified.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    print(\"--- Summary of the Simplified Base Champion Model ---\")\n",
    "    print(base_model_simplified_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to the previous model's AIC (5327.32).\n",
    "    # A lower or similar AIC will justify removing the variable.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Simplified Base Model: {base_model_simplified_results.aic:.2f}\")\n",
    "    print(\"Compare this to the previous Base Champion Model's AIC (5327.32).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d607b53",
   "metadata": {},
   "source": [
    "## analysis\n",
    "\n",
    "AIC improved, all stressors are significant, and we explain a high amount oif variation in yield. This is our new champion.\n",
    "\n",
    "Next, we test for non linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a43a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Testing for Non-Linearity (Quadratic Term for temp_Jul) ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the Model with the Quadratic Term ---\n",
    "    # We build on our simplified champion model by adding a squared term for temperature_Jul.\n",
    "    # This directly tests the inverted U-shape hypothesis from our EDA.\n",
    "    # The I() function tells patsy to treat the operation inside as a mathematical formula.\n",
    "    quadratic_formula = \"yield_maize ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + soil_water + temperature_Jul + I(temperature_Jul**2) + precipitation_Sep\"\n",
    "\n",
    "    # Initialize the GLM model using the new quadratic formula.\n",
    "    quadratic_model = smf.glm(\n",
    "        formula=quadratic_formula,\n",
    "        data=df_maize,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model.\n",
    "    quadratic_model_results = quadratic_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    print(\"--- Summary of the Model with Quadratic Term ---\")\n",
    "    print(quadratic_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our new champion's AIC (5325.61).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Quadratic Model: {quadratic_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the Simplified Base Model's AIC (5325.61).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c6ad4",
   "metadata": {},
   "source": [
    "## interpretation\n",
    "\n",
    "Our new non linear model has a way better AIC, and eplains a tiny bit more of the variation in maize yield. \n",
    "\n",
    "Adding the non linear version of July temperature was highly successful and significant, but it rendered the linear july temp variable non signfificant. This is most likly due to the qudratic term capturing the inverted u shape of the data so well, that the simple linear term becomes redundant. But we keep both due to the principle of hierarchy. \n",
    "\n",
    "Next we will test of Sep precipitation should also be non linear. For simplicity reasons, since this did not yield an improved model, I will not incldue the code. But the new term was not signfiicnat, so we keep our champion model as:\n",
    "\n",
    "\"yield_maize ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + soil_water + temperature_Jul + I(temperature_Jul**2) + precipitation_Sep\"\n",
    "\n",
    "\n",
    "The next steps are to test for interractions. We want to start with heat and water stress:\n",
    "The Hypothesis: The negative impact of high temperature_Jul is made much worse when there is also a lack of water. Conversely, having ample water can help the plant cool itself (through transpiration) and partially mitigate the effects of heat stress. This was also not an improvemtn, so we reject the model. Again, for efficiiency, im not including the code unless its an improvment. \n",
    "\n",
    "lASTLY WE WANT TO TEST IF USING INDIVIDUAL MONTH AVERAGES FOR SOIL WATER IS AN IMPROVEMNT over using the full growing season average. Again, non showed an improvment over the current champion model, so we do not include the code for it. This ends our model development and now we can move into interpetations and visualisations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91fcbfb",
   "metadata": {},
   "source": [
    "## visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31437a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Generating Yield Response Curves for Maize ---\")\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "df_maize = pd.read_csv(file_path)\n",
    "df_maize = df_maize[df_maize['yield_maize'] > 0].copy()\n",
    "print(\"Data prepared successfully.\")\n",
    "\n",
    "# --- 2. Fit Our Final Champion Model ---\n",
    "# We can use the simple formula here; statsmodels/patsy will handle the splines.\n",
    "final_champion_formula = \"yield_maize ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + soil_water + temperature_Jul + I(temperature_Jul**2) + precipitation_Sep\"\n",
    "\n",
    "print(\"Fitting Final Champion model...\")\n",
    "final_model = smf.glm(\n",
    "    formula=final_champion_formula,\n",
    "    data=df_maize,\n",
    "    family=sm.families.Gamma(link=sm.families.links.log())\n",
    ").fit()\n",
    "print(f\"Model fitted successfully. AIC: {final_model.aic:.2f}\")\n",
    "\n",
    "\n",
    "# --- 3. Prepare a Base Prediction DataFrame with Mean/Median Values (THE FIX) ---\n",
    "# This is the stable base for our plots. We use the median for robustness.\n",
    "# CRUCIALLY, we MUST include lat and lon themselves for the spline prediction to work.\n",
    "mean_values = {\n",
    "    'year': df_maize['year'].median(),\n",
    "    'potential_evaporation_May': df_maize['potential_evaporation_May'].median(),\n",
    "    'soil_water': df_maize['soil_water'].median(),\n",
    "    'temperature_Jul': df_maize['temperature_Jul'].median(),\n",
    "    'precipitation_Sep': df_maize['precipitation_Sep'].median(),\n",
    "    'lat': df_maize['lat'].median(), # Added lat\n",
    "    'lon': df_maize['lon'].median()  # Added lon\n",
    "}\n",
    "\n",
    "# --- VISUALIZATION 1: The Temperature Response Curve ---\n",
    "print(\"\\nGenerating plot 1: Non-Linear Yield Response to July Temperature...\")\n",
    "\n",
    "temp_jul_range = np.linspace(df_maize['temperature_Jul'].min(), df_maize['temperature_Jul'].max(), 100)\n",
    "\n",
    "pred_df_temp = pd.DataFrame(mean_values, index=range(100))\n",
    "pred_df_temp['temperature_Jul'] = temp_jul_range\n",
    "\n",
    "preds_temp = final_model.get_prediction(pred_df_temp)\n",
    "pred_summary_temp = preds_temp.summary_frame(alpha=0.05)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(pred_df_temp['temperature_Jul'], pred_summary_temp['mean'], color='red', linewidth=3, label='Predicted Yield')\n",
    "plt.fill_between(pred_df_temp['temperature_Jul'], pred_summary_temp['mean_ci_lower'], pred_summary_temp['mean_ci_upper'], color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "plt.scatter(df_maize['temperature_Jul'], df_maize['yield_maize'], alpha=0.05, color='gray', label='Raw Data')\n",
    "\n",
    "plt.title('Non-Linear Yield Response of Maize to July Temperature', fontsize=18)\n",
    "plt.xlabel('Average July Temperature (C)', fontsize=14)\n",
    "plt.ylabel('Predicted Maize Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylim(bottom=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- VISUALIZATION 2: The Soil Water Response Curve ---\n",
    "print(\"\\nGenerating plot 2: Yield Response to Aggregate Soil Water...\")\n",
    "\n",
    "soil_water_range = np.linspace(df_maize['soil_water'].min(), df_maize['soil_water'].max(), 100)\n",
    "\n",
    "pred_df_sw = pd.DataFrame(mean_values, index=range(100))\n",
    "pred_df_sw['soil_water'] = soil_water_range\n",
    "\n",
    "preds_sw = final_model.get_prediction(pred_df_sw)\n",
    "pred_summary_sw = preds_sw.summary_frame(alpha=0.05)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(pred_df_sw['soil_water'], pred_summary_sw['mean'], color='blue', linewidth=3, label='Predicted Yield')\n",
    "plt.fill_between(pred_df_sw['soil_water'], pred_summary_sw['mean_ci_lower'], pred_summary_sw['mean_ci_upper'], color='blue', alpha=0.2, label='95% Confidence Interval')\n",
    "plt.scatter(df_maize['soil_water'], df_maize['yield_maize'], alpha=0.05, color='gray', label='Raw Data')\n",
    "\n",
    "plt.title('Yield Response of Maize to Aggregate Soil Water', fontsize=18)\n",
    "plt.xlabel('Growing Season Average Soil Water (units)', fontsize=14)\n",
    "plt.ylabel('Predicted Maize Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylim(bottom=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---  Generating Vulnerability Curve for Maize ---\")\n",
    "\n",
    "# --- 1. Load Data and Fit the Final Champion Model ---\n",
    "file_path = '../data-cherry-pick/maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_maize = pd.read_csv(file_path)\n",
    "    df_maize = df_maize[df_maize['yield_maize'] > 0].copy() # Ensure positive yield\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # This is our confirmed Final Champion Model for Maize\n",
    "    final_champion_formula = \"yield_maize ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + soil_water + temperature_Jul + I(temperature_Jul**2) + precipitation_Sep\"\n",
    "\n",
    "    final_model = smf.glm(\n",
    "        formula=final_champion_formula,\n",
    "        data=df_maize,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    ).fit()\n",
    "    print(f\"Final champion model for maize has been successfully fitted. AIC: {final_model.aic:.2f}\")\n",
    "\n",
    "    # --- 2. Find a Real, Representative Observation for Predictions ---\n",
    "    # Find the single observation closest to the median year and median latitude.\n",
    "    median_year = df_maize['year'].median()\n",
    "    median_lat = df_maize['lat'].median()\n",
    "    closest_idx = ((df_maize['year'] - median_year)**2 + (df_maize['lat'] - median_lat)**2).idxmin()\n",
    "    X_template = df_maize.loc[[closest_idx]].reset_index(drop=True)\n",
    "    print(f\"Using a real observation from index {closest_idx} as the template for predictions.\")\n",
    "\n",
    "    # --- 3. Define and Predict the \"Baseline\" Yield ---\n",
    "    # The baseline is the yield at our real template, but with temp_Jul set to its median value.\n",
    "    X_baseline = X_template.copy()\n",
    "    X_baseline['temperature_Jul'] = df_maize['temperature_Jul'].median()\n",
    "    \n",
    "    baseline_pred = final_model.get_prediction(X_baseline).summary_frame(alpha=0.05)\n",
    "    yield_baseline = baseline_pred['mean'].iloc[0]\n",
    "    print(f\"\\nPredicted baseline yield for a typical case: {yield_baseline:.2f} tonnes/hectare\")\n",
    "    \n",
    "    # --- 4. Predict Yield Across the Range of the Stressor ---\n",
    "    temp_jul_range = np.linspace(df_maize['temperature_Jul'].min(), df_maize['temperature_Jul'].max(), 100)\n",
    "    \n",
    "    # Create the prediction grid by replicating our template and overwriting the stressor\n",
    "    X_stress = pd.concat([X_template] * 100, ignore_index=True)\n",
    "    X_stress['temperature_Jul'] = temp_jul_range\n",
    "\n",
    "    stress_pred = final_model.get_prediction(X_stress).summary_frame(alpha=0.05)\n",
    "    yield_predicted = stress_pred['mean']\n",
    "    yield_ci_lower = stress_pred['mean_ci_lower']\n",
    "    yield_ci_upper = stress_pred['mean_ci_upper']\n",
    "\n",
    "    # --- 5. Calculate Percentage Yield Change ---\n",
    "    # The vulnerability is the percentage change from the baseline yield.\n",
    "    yield_change_pct = ((yield_predicted - yield_baseline) / yield_baseline) * 100\n",
    "    yield_change_lower_pct = ((yield_ci_lower - yield_baseline) / yield_baseline) * 100\n",
    "    yield_change_upper_pct = ((yield_ci_upper - yield_baseline) / yield_baseline) * 100\n",
    "\n",
    "    # --- 6. Plot the Vulnerability Curve ---\n",
    "    print(\"Generating the vulnerability curve plot...\")\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    ax.plot(temp_jul_range, yield_change_pct, color='red', label='Predicted Yield Change')\n",
    "    ax.fill_between(temp_jul_range, yield_change_lower_pct, yield_change_upper_pct, color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "\n",
    "    ax.set_title('Vulnerability of Maize Yield to July Temperature', fontsize=18)\n",
    "    ax.set_xlabel('Average July Temperature (C)', fontsize=12)\n",
    "    ax.set_ylabel('Yield Change vs. Typical Year (%)', fontsize=12)\n",
    "    ax.legend()\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f86c6",
   "metadata": {},
   "source": [
    "### **Final Maize Model: Interpretation and Conclusions**\n",
    "\n",
    "This section summarizes the final champion model developed to explain the relationship between monthly climate stressors and maize yield in Northern Italy. The model is the result of a  multi-step workflow designed to be statistically robust, parsimonious, and interpretable.\n",
    "\n",
    "#### **The Final Champion Model**\n",
    "\n",
    "After a data-driven process of variable selection and iterative refinement, the final, best-performing model was determined to be a Gamma GLM with the following structure:\n",
    "\n",
    "**Final Model Formula:**\n",
    "```\n",
    "yield_maize ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + soil_water + temperature_Jul + I(temperature_Jul**2) + precipitation_Sep\n",
    "```\n",
    "\n",
    "**Key Performance Metrics:**\n",
    "*   **Akaike Information Criterion (AIC):** `5309.83` (The lowest of all tested models)\n",
    "*   **Pseudo R-squared (CS):** `0.8755` (Explains approx. **87.6%** of the variation in yield)\n",
    "\n",
    "#### **The Modeling Journey: How We Arrived Here**\n",
    "\n",
    "The final model was not assumed but was systematically built and validated:\n",
    "\n",
    "1.  **Variable Selection:** An **Elastic Net regularization** was used on a full model to overcome severe multicollinearity. This process objectively identified a small set of robust predictors (`year`, spatial splines, `potential_evaporation_May`, `soil_water`, `temperature_Jul`, `precipitation_Sep`, and `precipitation_Jun`).\n",
    "\n",
    "2.  **Model Refinement (Parsimony):** An initial base model was fitted, and the non-significant `precipitation_Jun` term was removed, resulting in a simpler model with an improved AIC score.\n",
    "\n",
    "3.  **Testing for Non-Linearity:** Guided by strong evidence from our EDA, we tested for a non-linear effect of `temperature_Jul`. Adding a quadratic term (`I(temperature_Jul**2)`) resulted in a **massive drop in AIC**, confirming a critical non-linear relationship. Further tests for non-linearity in other variables (e.g., `precipitation_Sep`) did not improve the model and were rejected.\n",
    "\n",
    "4.  **Testing for Interactions & Alternatives:** We tested the most agronomically plausible interaction (`temperature_Jul:soil_water`), which was not significant and worsened the model's AIC. A sensitivity analysis also confirmed that the aggregate `soil_water` variable was a much stronger predictor than any individual monthly soil water variable.\n",
    "\n",
    "This structured process ensures our final model is not overfit and that every included term is statistically justified and meaningful.\n",
    "\n",
    "#### **Detailed Interpretation of the Final Model**\n",
    "\n",
    "*   **Control Variables:**\n",
    "    *   `year`: The positive, significant coefficient confirms a strong **technological trend**, with yields consistently increasing over time.\n",
    "    *   `bs(lat, df=4)` & `bs(lon, df=4)`: The high significance of the spatial splines confirms that **geography is a dominant driver** of yield, likely due to underlying soil and landscape characteristics.\n",
    "\n",
    "*   **Key Climate Drivers:**\n",
    "    *   `soil_water` & `precipitation_Sep`: These terms both have significant positive coefficients, highlighting the critical importance of **water availability** for maize. The model confirms that a wetter growing season and sufficient late-season rain for grain-filling lead to higher yields.\n",
    "    *   `potential_evaporation_May`: This has a positive coefficient, suggesting that higher evaporative demand in May (a proxy for sunny and warm conditions) is beneficial for early-stage crop establishment.\n",
    "    *   `temperature_Jul` & `I(temperature_Jul**2)`: This is the most important climate finding. The combination of a positive linear term and a significant negative quadratic term confirms a non-linear, **inverted U-shaped response**. This indicates that while warming is initially beneficial, there is an optimal temperature for maize in July. Beyond this peak, higher temperatures become increasingly damaging, a clear statistical signature of **heat stress** during the critical pollination period.\n",
    "\n",
    "#### **Insights from Visualization**\n",
    "\n",
    "*   **The Temperature Response Curve:** The yield response plot for `temperature_Jul` visually confirms the heat stress effect. For the range of data observed in Northern Italy, the curve is almost entirely on the downward-sloping side of the inverted U. This reveals that the primary threat in this region is from temperatures being **too hot, not too cold**, during July.\n",
    "\n",
    "*   **The Soil Water Response Curve:** This plot shows a clear and consistent positive relationship. It visually confirms that higher average soil water throughout the season is strongly associated with higher predicted yields, underscoring the crop's fundamental need for water.\n",
    "\n",
    "*   **The Final Vulnerability Curve:** This plot synthesizes the temperature finding into a quantitative measure of risk. It shows that:\n",
    "    *   **Opportunity:** A cool July (e.g., 15C) can lead to a **yield gain of over 25%** compared to a typical year.\n",
    "    *   **Vulnerability:** A hot July (e.g., 25C) can lead to a **yield loss of over -20%**. In an extreme heatwave year (>27C), losses can exceed **-30%**.\n",
    "\n",
    "#### **Overall Conclusion**\n",
    "\n",
    "The model provides a powerful and statistically robust explanation of maize yield vulnerability in Northern Italy. The dominant story is the dual importance of **heat and water**. While consistent water availability provides a strong foundation for high yields, the ultimate determining factor for year-to-year success or failure is **heat stress during the critical pollination month of July**. The model successfully quantifies this non-linear vulnerability, providing a clear and actionable insight into the primary climate risk facing maize production in the region."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 41-soybean-gamma-model.ipynb/41-soybean-gamma-model.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d2fd42",
   "metadata": {},
   "source": [
    "# EDA for Soybean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bcb4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import scipy.stats as stats\n",
    "from patsy import dmatrices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fbf69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis - Full Correlation Matrix\n",
    "\n",
    "print(\"--- EDA: Correlation Analysis of All Monthly Stressors ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# As confirmed, this file is already specific to soybean and its growing season.\n",
    "file_path = '../data-cherry-pick/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_soybean = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Select Only the Monthly Stressor Variables ---\n",
    "    # We will select all columns that have a month name in them, which is a robust\n",
    "    # way to grab all the monthly predictors we want to investigate.\n",
    "    monthly_stressors = [col for col in df_soybean.columns if '_' in col and 'yield' not in col]\n",
    "    df_corr = df_soybean[monthly_stressors]\n",
    "    \n",
    "    print(f\"\\nSelected {len(df_corr.columns)} monthly stressor variables for correlation analysis.\")\n",
    "\n",
    "    # --- 3. Calculate and Print the Correlation Matrix ---\n",
    "    correlation_matrix = df_corr.corr()\n",
    "    \n",
    "    # Optional: If you want to see the full numerical matrix, uncomment the next line\n",
    "    # print(\"\\n--- Full Pairwise Correlation Matrix ---\")\n",
    "    # print(correlation_matrix)\n",
    "\n",
    "    # --- 4. Visualize the Matrix with a Heatmap ---\n",
    "    # A heatmap is the best way to see the broad patterns of collinearity.\n",
    "    print(\"\\nGenerating correlation heatmap...\")\n",
    "    \n",
    "    plt.figure(figsize=(18, 15))\n",
    "    heatmap = sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        cmap='coolwarm',  # Use a diverging colormap (red=positive, blue=negative)\n",
    "        center=0,         # Center the colormap at zero\n",
    "        vmin=-1,          # Set the color scale limits to the theoretical min/max\n",
    "        vmax=1,\n",
    "        linewidths=.5,\n",
    "        annot=False       # Annotations are turned off as the matrix is too large to be readable\n",
    "    )\n",
    "    \n",
    "    plt.title('Pairwise Correlation Matrix of All Monthly Stressors for soybean', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Extended EDA for Soybean Yield Analysis ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_soybean = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- Task 1: Examine the Distribution of the Dependent Variable (yield_soybean) ---\n",
    "    print(\"--- Task 1: Analyzing the distribution of the dependent variable 'yield_soybean' ---\")\n",
    "    \n",
    "    # Create a figure with two subplots side-by-side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Distribution Analysis for yield_soybeane', fontsize=16)\n",
    "\n",
    "    # a) Histogram with a Kernel Density Estimate (KDE)\n",
    "    # This helps us visually assess the shape, center, and spread of the yield data.\n",
    "    # We are checking for positive skewness, which is characteristic of data modeled by a Gamma distribution.\n",
    "    sns.histplot(df_soybean['yield_soybean'], kde=True, ax=axes[0])\n",
    "    axes[0].set_title('Histogram of Soybean Yield')\n",
    "    axes[0].set_xlabel('Yield (tonnes per hectare)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    # b) Q-Q (Quantile-Quantile) Plot against a theoretical normal distribution\n",
    "    # This plot helps us assess if the data's distribution follows a specific theoretical distribution.\n",
    "    # Deviations from the red line suggest skewness or heavy tails.\n",
    "    # While our target is a Gamma GLM, a Q-Q plot vs. Normal is a standard first step to detect non-normality.\n",
    "    stats.probplot(df_soybean['yield_soybean'], dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot of Soybean Yield vs. Normal Distribution')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Distribution plots generated. Check for positive skew, which supports our choice of a Gamma GLM.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 2: Bivariate Scatter Plots of Yield vs. Key Stressors ---\n",
    "    print(\"--- Task 2: Visualizing relationships between yield and key climate stressors ---\")\n",
    "    \n",
    "    # Select a few key stressors based on agronomic theory for soybean\n",
    "    key_stressors = ['temperature_Jul', 'precipitation_Jun', 'soil_water_Aug']\n",
    "    \n",
    "    # Create a figure to hold the scatter plots\n",
    "    fig, axes = plt.subplots(1, len(key_stressors), figsize=(21, 6))\n",
    "    fig.suptitle('Bivariate Relationships: Soybean Yield vs. Key Stressors', fontsize=16)\n",
    "\n",
    "    for i, stressor in enumerate(key_stressors):\n",
    "        # We use a regression plot with a LOWESS (Locally Weighted Scatterplot Smoothing) curve.\n",
    "        # This is a non-parametric way to see the underlying trend without assuming a linear relationship.\n",
    "        # It's excellent for spotting potential non-linearities (like an inverted 'U' shape).\n",
    "        sns.regplot(\n",
    "            x=stressor,\n",
    "            y='yield_soybean',\n",
    "            data=df_soybean,\n",
    "            ax=axes[i],\n",
    "            lowess=True, # Use LOWESS smoother to detect non-linear patterns\n",
    "            scatter_kws={'alpha': 0.1, 'color': 'gray'}, # De-emphasize individual points\n",
    "            line_kws={'color': 'blue'} # Emphasize the trend line\n",
    "        )\n",
    "        axes[i].set_title(f'Yield vs. {stressor}')\n",
    "        axes[i].set_xlabel(f'{stressor}')\n",
    "        axes[i].set_ylabel('Yield (tonnes per hectare)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Scatter plots generated. Look for non-linear patterns that might inform our final model.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 3: Plot Key Variables Over Time ---\n",
    "    print(\"--- Task 3: Examining long-term trends in yield and a key climate variable ---\")\n",
    "    \n",
    "    # Calculate the mean of yield and a key stressor for each year\n",
    "    yearly_data = df_soybean.groupby('year')[['yield_soybean', 'temperature_Jul']].mean().reset_index()\n",
    "\n",
    "    # Create a plot with a primary and secondary y-axis to show both trends together.\n",
    "    # This confirms the necessity of including 'year' as a control variable to capture trends\n",
    "    # likely related to technology, while also checking for climate trends.\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plotting average yield on the primary (left) y-axis\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Average Yield (tonnes per hectare)', color=color)\n",
    "    ax1.plot(yearly_data['year'], yearly_data['yield_soybean'], color=color, marker='o', label='Average Yield')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Create a second y-axis that shares the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plotting average temperature on the secondary (right) y-axis\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Average July Temperature (C)', color=color)\n",
    "    ax2.plot(yearly_data['year'], yearly_data['temperature_Jul'], color=color, linestyle='--', marker='x', label='Avg. July Temp')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('Long-Term Trend of Soybean Yield and July Temperature (1982-2016)', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    # Adding a single legend for both lines\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"Time-series plot generated. Note the clear upward trend in yield, confirming the need for a 'year' control variable.\\n\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: A required column was not found in the dataset: {e}. Please check the CSV file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e564c9b",
   "metadata": {},
   "source": [
    "# regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_soybean = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Define the Full Model Formula ---\n",
    "    # Programmatically get all monthly stressor column names\n",
    "    monthly_stressors = [col for col in df_soybean.columns if '_' in col and 'yield' not in col]\n",
    "    \n",
    "    # Join them with '+' to create the predictor part of the formula\n",
    "    stressor_formula_part = ' + '.join(monthly_stressors)\n",
    "    \n",
    "    # Construct the complete R-style formula string.\n",
    "    # We include our controls (year, spatial splines) and all potential predictors.\n",
    "    # Note: patsy's bs() function creates the basis spline columns.\n",
    "    formula = f\"yield_soybean ~ year + bs(lat, df=4) + bs(lon, df=4) + {stressor_formula_part}\"\n",
    "    \n",
    "    print(\"\\nGenerated model formula for patsy:\")\n",
    "    print(formula) # Uncomment to see the full, very long formula string\n",
    "\n",
    "    # --- 3. Create the Design Matrix (X) and Response Vector (y) ---\n",
    "    # patsy processes the formula and the dataframe to create the matrices needed for modeling.\n",
    "    # 'y' will be our dependent variable, 'X' will be the full set of predictors.\n",
    "    # The intercept is automatically included in 'X' by patsy.\n",
    "    print(\"\\nCreating design matrix (X) and response vector (y) using patsy...\")\n",
    "    y, X = dmatrices(formula, data=df_soybean, return_type='dataframe')\n",
    "    \n",
    "    print(f\"Successfully created response vector y with shape: {y.shape}\")\n",
    "    print(f\"Successfully created design matrix X with shape: {X.shape}\")\n",
    "    print(f\"The {X.shape[1]} columns in X include the intercept, year, 8 spline bases (4 for lat, 4 for lon), and {len(monthly_stressors)} climate stressors.\")\n",
    "\n",
    "    # --- 4. Standardize the Predictor Matrix (X) ---\n",
    "    # We scale ALL predictors to have a mean of 0 and a standard deviation of 1.\n",
    "    # This ensures the regularization penalty is applied fairly to all variables.\n",
    "    # We do NOT scale the response variable y.\n",
    "    print(\"\\nStandardizing the design matrix X...\")\n",
    "    \n",
    "    # We remove the Intercept column before scaling, as it should not be regularized or scaled.\n",
    "    # We will add it back later if needed, but scikit-learn's models handle it by default.\n",
    "    X_no_intercept = X.drop('Intercept', axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_values = scaler.fit_transform(X_no_intercept)\n",
    "    \n",
    "    # Convert the scaled array back to a pandas DataFrame with the original column names\n",
    "    X_scaled = pd.DataFrame(X_scaled_values, columns=X_no_intercept.columns, index=X.index)\n",
    "    \n",
    "    print(\"Standardization complete.\")\n",
    "    \n",
    "    # Verification: Check the mean and standard deviation of a few scaled columns\n",
    "    print(\"\\n--- Verification of Standardization ---\")\n",
    "    verification_cols = ['year', 'bs(lat, df=4)[0]', 'temperature_Jul']\n",
    "    for col in verification_cols:\n",
    "        mean_val = X_scaled[col].mean()\n",
    "        std_val = X_scaled[col].std()\n",
    "        print(f\"Column '{col}': Mean = {mean_val:.4f}, Std Dev = {std_val:.4f}\")\n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9905bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume 'y' and 'X_scaled' are already in memory from the previous step.\n",
    "# If not, you would need to re-run the data preparation script.\n",
    "\n",
    "try:\n",
    "    # --- 1. Define the GLM Model ---\n",
    "    # We specify our model family (Gamma) and the link function (log) as per our project plan.\n",
    "    # We pass the prepared y and the fully scaled X matrix.\n",
    "    # Note: statsmodels requires the intercept to be in the X matrix, which patsy provided.\n",
    "    \n",
    "    # We need to add the intercept back to the scaled data for statsmodels GLM\n",
    "    X_scaled_with_intercept = X.copy() # Start with the original X to preserve intercept and structure\n",
    "    X_scaled_with_intercept[X_no_intercept.columns] = X_scaled # Replace non-intercept columns with scaled versions\n",
    "\n",
    "    gl_gamma = sm.GLM(y, X_scaled_with_intercept, family=sm.families.Gamma(link=sm.families.links.log()))\n",
    "    print(\"Successfully initialized Gamma GLM with a log link.\")\n",
    "\n",
    "    # --- 2. Set up the Regularization Path ---\n",
    "    # We need to test a series of alpha values (penalty strengths).\n",
    "    # A logarithmic scale is best for this, from a weak penalty to a strong one.\n",
    "    n_alphas = 100\n",
    "    alphas = np.logspace(-3, 0.5, n_alphas) # From 0.001 to ~3.16\n",
    "\n",
    "    # The L1_wt parameter controls the Elastic Net mix (0=Ridge, 1=Lasso). \n",
    "    # 0.5 is a balanced choice.\n",
    "    elastic_net_l1_wt = 0.5 \n",
    "    \n",
    "    print(f\"Will fit the model for {n_alphas} alpha values with L1_wt (l1_ratio) = {elastic_net_l1_wt}\")\n",
    "\n",
    "    # --- 3. Fit the Model for Each Alpha and Store Coefficients ---\n",
    "    # We will loop through our alphas and save the coefficients from each model fit.\n",
    "    coefficients = []\n",
    "    \n",
    "    for alpha_val in alphas:\n",
    "        # The fit_regularized method performs the Elastic Net estimation.\n",
    "        # We set refit=False because we want to see the shrunken coefficients for this analysis.\n",
    "        results = gl_gamma.fit_regularized(\n",
    "            method='elastic_net', \n",
    "            alpha=alpha_val, \n",
    "            L1_wt=elastic_net_l1_wt,\n",
    "            refit=False \n",
    "        )\n",
    "        coefficients.append(results.params)\n",
    "    \n",
    "    # Convert the list of coefficient series into a DataFrame for easy plotting\n",
    "    coef_df = pd.DataFrame(coefficients, index=alphas)\n",
    "    coef_df.index.name = \"alpha\"\n",
    "    \n",
    "    # Exclude the Intercept for plotting, as it's not regularized and has a different scale.\n",
    "    coef_df_no_intercept = coef_df.drop('Intercept', axis=1)\n",
    "    \n",
    "    print(\"\\nCompleted fitting models along the regularization path.\")\n",
    "\n",
    "    # --- 4. Visualize the Regularization Path ---\n",
    "    print(\"Generating the regularization path plot...\")\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    ax.plot(coef_df_no_intercept)\n",
    "    ax.set_xscale('log') # The alpha path is best viewed on a log scale\n",
    "    \n",
    "    # Add a vertical line at zero\n",
    "    ax.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    \n",
    "    ax.set_title('Regularization Path for Rice Model Coefficients (Elastic Net)', fontsize=18)\n",
    "    ax.set_xlabel('Alpha (Penalty Strength)', fontsize=14)\n",
    "    ax.set_ylabel('Standardized Coefficient Value', fontsize=14)\n",
    "    \n",
    "    # To avoid a cluttered legend, we don't add one here. The goal is to see the general pattern.\n",
    "    # Alternatively, for fewer variables, a legend could be useful:\n",
    "    # ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"ERROR: Make sure that 'y' and 'X_scaled' DataFrames from the previous step are available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Corrected code to identify the most robust variables ---\n",
    "# We will inspect the coefficients at a moderately high alpha value\n",
    "# This tells us which variables \"survived\" the penalty the longest.\n",
    "alpha_to_inspect = 0.03 \n",
    "\n",
    "try:\n",
    "    # Find the alpha in our index that is closest to our target\n",
    "    # CORRECTED LINE: The operation works directly on the index without .flat\n",
    "    closest_alpha = coef_df.index[np.abs(coef_df.index - alpha_to_inspect).argmin()]\n",
    "\n",
    "    print(f\"--- Coefficients at alpha  {closest_alpha:.4f} ---\")\n",
    "\n",
    "    # Get the coefficients at this alpha and sort them by absolute value\n",
    "    robust_coeffs = coef_df.loc[closest_alpha].copy()\n",
    "    robust_coeffs_sorted = robust_coeffs.abs().sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nVariables sorted by the absolute magnitude of their shrunken coefficient:\")\n",
    "    # We display more variables to get a fuller picture\n",
    "    print(robust_coeffs_sorted.head(15))\n",
    "\n",
    "    # Let's also see their actual values (positive or negative) for the top variables\n",
    "    print(\"\\n--- Actual coefficient values for the most robust variables ---\")\n",
    "    print(coef_df.loc[closest_alpha, robust_coeffs_sorted.index].head(10))\n",
    "\n",
    "except NameError:\n",
    "     print(\"ERROR: Make sure that 'coef_df' DataFrame from the previous step is available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12c813",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b113c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Base Champion Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_soybean = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the Champion Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    champion_formula = \"yield_soybean ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + solar_radiation_Jul + soil_water + precipitation_May\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    base_model = smf.glm(\n",
    "        formula=champion_formula,\n",
    "        data=df_soybean,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    base_model_results = base_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(base_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Future Comparison ---\n",
    "    # The AIC is a key metric for comparing different model formulations. Lower is better.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Base Champion Model: {base_model_results.aic:.2f}\")\n",
    "    print(\"This will be our benchmark for comparison.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131c6b1",
   "metadata": {},
   "source": [
    "## non linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74415eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Testing for Non-Linearity (Quadratic Term for solar_rad_Jul) ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_soybean = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the Model with the Quadratic Term ---\n",
    "    # We build on our simplified champion by adding a squared term for solar_radiation_Jul.\n",
    "    # This directly tests the inverted U-shape hypothesis from our EDA.\n",
    "    quadratic_formula = \"yield_soybean ~ year + bs(lat, df=4) + bs(lon, df=4) + potential_evaporation_May + solar_radiation_Jul + soil_water + precipitation_May + I(solar_radiation_Jul**2)\"\n",
    "\n",
    "    # Initialize the GLM model using the new quadratic formula.\n",
    "    quadratic_model = smf.glm(\n",
    "        formula=quadratic_formula,\n",
    "        data=df_soybean,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model.\n",
    "    quadratic_model_results = quadratic_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    print(\"--- Summary of the Model with Quadratic Term ---\")\n",
    "    print(quadratic_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion's AIC (3516.86).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Quadratic Model: {quadratic_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the Simplified Base Model's AIC (3516.86).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f35d1",
   "metadata": {},
   "source": [
    "## interpreatation\n",
    "\n",
    "This model fails compleatly. Something breaks in the internal fitting process, and I think this is probs cause of multicolineariity introduced by the quadratic term, and the instense numerical variables of the solar radiation columns. So the idea is that if we standardize the solar radiation columns, this should fix the problem. \n",
    "\n",
    "We are doing z score standardization. It does two things: it centers the variable at zero AND it scales it to have a standard deviation of 1. This is like what we did when we standardized all our variables for regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ff5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Testing for Non-Linearity (Quadratic Term for solar_rad_Jul) ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_soybean = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    df_soybean[\"solar_rad_Jul_z\"] = (\n",
    "    df_soybean[\"solar_radiation_Jul\"] - df_soybean[\"solar_radiation_Jul\"].mean()\n",
    "    ) / df_soybean[\"solar_radiation_Jul\"].std()\n",
    "\n",
    "\n",
    "    # --- 2. Define and Fit the Model with the Quadratic Term ---\n",
    "    # We build on our simplified champion by adding a squared term for solar_radiation_Jul.\n",
    "    # This directly tests the inverted U-shape hypothesis from our EDA.\n",
    "    quadratic_formula = \"\"\"\n",
    "    yield_soybean ~ year + bs(lat, df=4) + bs(lon, df=4) +\n",
    "                    potential_evaporation_May + solar_rad_Jul_z +\n",
    "                    soil_water + precipitation_May +\n",
    "                    I(solar_rad_Jul_z**2)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Initialize the GLM model using the new quadratic formula.\n",
    "    quadratic_model = smf.glm(\n",
    "        formula=quadratic_formula,\n",
    "        data=df_soybean,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model.\n",
    "    quadratic_model_results = quadratic_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    print(\"--- Summary of the Model with Quadratic Term ---\")\n",
    "    print(quadratic_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion's AIC (3516.86).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Quadratic Model: {quadratic_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the Simplified Base Model's AIC (3516.86).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8efb62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Testing for Non-Linearity (Quadratic Term for solar_rad_Jul) ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_soybean = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    df_soybean[\"solar_rad_Jul_z\"] = (\n",
    "    df_soybean[\"solar_radiation_Jul\"] - df_soybean[\"solar_radiation_Jul\"].mean()\n",
    "    ) / df_soybean[\"solar_radiation_Jul\"].std()\n",
    "\n",
    "\n",
    "    # --- 2. Define and Fit the Model with the Quadratic Term ---\n",
    "    # We build on our simplified champion by adding a squared term for solar_radiation_Jul.\n",
    "    # This directly tests the inverted U-shape hypothesis from our EDA.\n",
    "    second_quadratic_formula = \"\"\"\n",
    "    yield_soybean ~ year + bs(lat, df=4) + bs(lon, df=4) +\n",
    "                    potential_evaporation_May + solar_rad_Jul_z +\n",
    "                    soil_water + precipitation_May +\n",
    "                    I(solar_rad_Jul_z**2) + I(precipitation_May**2)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Initialize the GLM model using the new quadratic formula.\n",
    "    second_quadratic_model = smf.glm(\n",
    "        formula=second_quadratic_formula,\n",
    "        data=df_soybean,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model.\n",
    "    second_quadratic_model_results = second_quadratic_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    print(\"--- Summary of the Model with Quadratic Term ---\")\n",
    "    print(second_quadratic_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion's AIC (3496.83).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Quadratic Model: {second_quadratic_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the Simplified Base Model's AIC (3496.83).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f83e9",
   "metadata": {},
   "source": [
    "## interractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f38b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Testing for interractions ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_soybean = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    df_soybean[\"solar_rad_Jul_z\"] = (\n",
    "    df_soybean[\"solar_radiation_Jul\"] - df_soybean[\"solar_radiation_Jul\"].mean()\n",
    "    ) / df_soybean[\"solar_radiation_Jul\"].std()\n",
    "\n",
    "\n",
    "    # --- 2. Define and Fit the Model with the interraction ---\n",
    "    interraction_formula = \"\"\"\n",
    "    yield_soybean ~ year + bs(lat, df=4) + bs(lon, df=4) +\n",
    "                    potential_evaporation_May + solar_rad_Jul_z +\n",
    "                    soil_water + precipitation_May +\n",
    "                    I(solar_rad_Jul_z**2) + I(precipitation_May**2)\n",
    "                    + solar_rad_Jul_z:soil_water + potential_evaporation_May:solar_rad_Jul_z\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Initialize the GLM model using the new quadratic formula.\n",
    "    interraction_model = smf.glm(\n",
    "        formula=interraction_formula,\n",
    "        data=df_soybean,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model.\n",
    "    interraction_model_results = interraction_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    print(\"--- Summary of the Model with interraction Term ---\")\n",
    "    print(interraction_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion's AIC (3487.03).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Quadratic Model: {interraction_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the Simplified Base Model's AIC (3487.03).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728e162",
   "metadata": {},
   "source": [
    "## visaulisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7481e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Generating Final Yield Response Curves for Soybean ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "df_soybean = pd.read_csv(file_path)\n",
    "df_soybean = df_soybean[df_soybean['yield_soybean'] > 0].copy()\n",
    "\n",
    "# Standardize the necessary variables for the model formula\n",
    "df_soybean[\"solar_rad_Jul_z\"] = (df_soybean[\"solar_radiation_Jul\"] - df_soybean[\"solar_radiation_Jul\"].mean()) / df_soybean[\"solar_radiation_Jul\"].std()\n",
    "print(\"Data prepared successfully.\")\n",
    "\n",
    "# --- 2. Fit Our Final Champion Model ---\n",
    "final_champion_formula = \"\"\"\n",
    "    yield_soybean ~ year + bs(lat, df=4) + bs(lon, df=4) +\n",
    "                    potential_evaporation_May + solar_rad_Jul_z +\n",
    "                    soil_water + precipitation_May +\n",
    "                    I(solar_rad_Jul_z**2) + I(precipitation_May**2) +\n",
    "                    solar_rad_Jul_z:soil_water + potential_evaporation_May:solar_rad_Jul_z\n",
    "\"\"\"\n",
    "print(\"Fitting Final Champion model for Soybean...\")\n",
    "final_model = smf.glm(\n",
    "    formula=final_champion_formula,\n",
    "    data=df_soybean,\n",
    "    family=sm.families.Gamma(link=sm.families.links.log())\n",
    ").fit()\n",
    "print(f\"Model fitted successfully. AIC: {final_model.aic:.2f}\")\n",
    "\n",
    "\n",
    "# --- 3. Prepare a Base Prediction Dictionary with Median Values ---\n",
    "median_values = {\n",
    "    'year': df_soybean['year'].median(),\n",
    "    'lat': df_soybean['lat'].median(),\n",
    "    'lon': df_soybean['lon'].median(),\n",
    "    'potential_evaporation_May': df_soybean['potential_evaporation_May'].median(),\n",
    "    'solar_rad_Jul_z': df_soybean['solar_rad_Jul_z'].median(), # Use the standardized variable\n",
    "    'soil_water': df_soybean['soil_water'].median(),\n",
    "    'precipitation_May': df_soybean['precipitation_May'].median()\n",
    "}\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "# --- PLOT A: Yield Response to Soil Water ---\n",
    "print(\"\\nGenerating Plot A: Yield Response to Soil Water...\")\n",
    "soil_water_range = np.linspace(df_soybean['soil_water'].min(), df_soybean['soil_water'].max(), 100)\n",
    "pred_df_sw = pd.DataFrame(median_values, index=range(100))\n",
    "pred_df_sw['soil_water'] = soil_water_range\n",
    "\n",
    "preds_sw = final_model.get_prediction(pred_df_sw).summary_frame(alpha=0.05)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(pred_df_sw['soil_water'], preds_sw['mean'], color='blue', linewidth=3, label='Predicted Yield')\n",
    "plt.fill_between(pred_df_sw['soil_water'], preds_sw['mean_ci_lower'], preds_sw['mean_ci_upper'], color='blue', alpha=0.2, label='95% Confidence Interval')\n",
    "plt.title('Yield Response of Soybean to Aggregate Soil Water', fontsize=18)\n",
    "plt.xlabel('Growing Season Average Soil Water (units)', fontsize=14)\n",
    "plt.ylabel('Predicted Soybean Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.legend(); plt.ylim(bottom=0); plt.show()\n",
    "\n",
    "\n",
    "# --- PLOT B: Non-Linear Yield Response to July Solar Radiation ---\n",
    "print(\"\\nGenerating Plot B: Non-Linear Yield Response to July Solar Radiation...\")\n",
    "solar_jul_z_range = np.linspace(df_soybean['solar_rad_Jul_z'].min(), df_soybean['solar_rad_Jul_z'].max(), 100)\n",
    "pred_df_solar = pd.DataFrame(median_values, index=range(100))\n",
    "pred_df_solar['solar_rad_Jul_z'] = solar_jul_z_range\n",
    "\n",
    "preds_solar = final_model.get_prediction(pred_df_solar).summary_frame(alpha=0.05)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(pred_df_solar['solar_rad_Jul_z'], preds_solar['mean'], color='red', linewidth=3, label='Predicted Yield')\n",
    "plt.fill_between(pred_df_solar['solar_rad_Jul_z'], preds_solar['mean_ci_lower'], preds_solar['mean_ci_upper'], color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "plt.title('Non-Linear Yield Response of Soybean to July Solar Radiation', fontsize=18)\n",
    "plt.xlabel('Standardized July Solar Radiation (Z-score)', fontsize=14)\n",
    "plt.ylabel('Predicted Soybean Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.legend(); plt.ylim(bottom=0); plt.show()\n",
    "\n",
    "\n",
    "# --- PLOT C: Non-Linear Yield Response to May Precipitation ---\n",
    "print(\"\\nGenerating Plot C: Non-Linear Yield Response to May Precipitation...\")\n",
    "precip_may_range = np.linspace(df_soybean['precipitation_May'].min(), df_soybean['precipitation_May'].max(), 100)\n",
    "pred_df_precip = pd.DataFrame(median_values, index=range(100))\n",
    "pred_df_precip['precipitation_May'] = precip_may_range\n",
    "\n",
    "preds_precip = final_model.get_prediction(pred_df_precip).summary_frame(alpha=0.05)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(pred_df_precip['precipitation_May'], preds_precip['mean'], color='green', linewidth=3, label='Predicted Yield')\n",
    "plt.fill_between(pred_df_precip['precipitation_May'], preds_precip['mean_ci_lower'], preds_precip['mean_ci_upper'], color='green', alpha=0.2, label='95% Confidence Interval')\n",
    "plt.title('Non-Linear Yield Response of Soybean to May Precipitation', fontsize=18)\n",
    "plt.xlabel('May Precipitation (mm)', fontsize=14)\n",
    "plt.ylabel('Predicted Soybean Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.legend(); plt.ylim(bottom=0); plt.show()\n",
    "\n",
    "\n",
    "# --- PLOT D: The \"Money Plot\" - Heat x Water Interaction ---\n",
    "print('\\nGenerating Plot D: The \"Heat x Water\" Interaction Effect...')\n",
    "sw_quantiles = df_soybean['soil_water'].quantile([0.25, 0.5, 0.75])\n",
    "scenarios = {\n",
    "    'Drought (25th Pct SW)': {'value': sw_quantiles[0.25], 'color': 'orange'},\n",
    "    'Average (50th Pct SW)': {'value': sw_quantiles[0.50], 'color': 'purple'},\n",
    "    'Wet (75th Pct SW)': {'value': sw_quantiles[0.75], 'color': 'cyan'}\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "for scenario_name, props in scenarios.items():\n",
    "    pred_df_interact = pd.DataFrame(median_values, index=range(100))\n",
    "    pred_df_interact['solar_rad_Jul_z'] = solar_jul_z_range # Vary solar radiation on x-axis\n",
    "    pred_df_interact['soil_water'] = props['value'] # Set the soil water for this scenario\n",
    "    \n",
    "    preds_interact = final_model.get_prediction(pred_df_interact).summary_frame(alpha=0.05)\n",
    "    \n",
    "    plt.plot(pred_df_interact['solar_rad_Jul_z'], preds_interact['mean'], color=props['color'], linewidth=3, label=scenario_name)\n",
    "    plt.fill_between(pred_df_interact['solar_rad_Jul_z'], preds_interact['mean_ci_lower'], preds_interact['mean_ci_upper'], color=props['color'], alpha=0.15)\n",
    "\n",
    "plt.title('Vulnerability to July Heat is Amplified by Drought', fontsize=18)\n",
    "plt.xlabel('Standardized July Solar Radiation (Z-score)', fontsize=14)\n",
    "plt.ylabel('Predicted Soybean Yield (tonnes per hectare)', fontsize=14)\n",
    "plt.legend(title='Soil Water Scenario'); plt.ylim(bottom=0); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e86f6",
   "metadata": {},
   "source": [
    "### **Analysis of Soybean Yield Response Curves**\n",
    "\n",
    "These plots visualize the key relationships from our final, complex statistical model. They show how predicted soybean yield responds to specific climate factors while holding all others at their typical values.\n",
    "\n",
    "#### **Plot A: Yield Response to Aggregate Soil Water**\n",
    "\n",
    "*   **Primary Finding:** Higher average soil water throughout the growing season is consistently associated with higher soybean yields.\n",
    "*   **Interpretation:** The plot shows a clear, strong positive relationship. This confirms that overall water availability is a fundamental driver of yield, which makes perfect agronomic sense as water is a key input for plant growth and seed development.\n",
    "\n",
    "#### **Plot B: Non-Linear Yield Response to July Solar Radiation**\n",
    "\n",
    "*   **Primary Finding:** The model reveals a distinct **inverted U-shape**, indicating an optimal level of solar radiation in July.\n",
    "*   **Interpretation:** Yields peak when standardized solar radiation is slightly below average (around -0.5 Z-score). Beyond this optimum, higher solar radiation (representing \"heat stress\") causes a significant and accelerating decline in yield. This captures the critical vulnerability of soybean during its flowering and pod-setting phase.\n",
    "\n",
    "#### **Plot C: Non-Linear Yield Response to May Precipitation**\n",
    "\n",
    "*   **Primary Finding:** The relationship with May precipitation is also an **inverted U-shape**, showing that both too little and too much rain are suboptimal.\n",
    "*   **Interpretation:** The curve shows that yields are maximized at a moderate level of May rainfall (around 200mm). This suggests that while some rain is essential for crop establishment, excessive rainfall can be detrimental, possibly due to waterlogging or associated cool, cloudy conditions.\n",
    "\n",
    "#### **Plot D: The Interaction of Heat and Water Stress**\n",
    "\n",
    "*   **Primary Finding:** The plant's vulnerability to July heat stress is **strongly mediated by water availability.**\n",
    "*   **Interpretation:** This plot visualizes our most sophisticated finding. The three curves show the yield response to July heat under different soil water scenarios. The \"Wet\" curve (cyan) is consistently higher than the \"Drought\" curve (orange). This visually proves that having ample soil water **increases the overall yield potential** and helps the crop **better tolerate heat stress**, leading to higher yields at any given level of July solar radiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Generating Final Vulnerability Curves for Soybean ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/soybean_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_soybean = pd.read_csv(file_path)\n",
    "    df_soybean = df_soybean[df_soybean['yield_soybean'] > 0].copy()\n",
    "\n",
    "    # Standardize the necessary variables\n",
    "    df_soybean[\"solar_rad_Jul_z\"] = (df_soybean[\"solar_radiation_Jul\"] - df_soybean[\"solar_radiation_Jul\"].mean()) / df_soybean[\"solar_radiation_Jul\"].std()\n",
    "    print(\"Data prepared successfully.\")\n",
    "\n",
    "    # --- 2. Fit Our Final Champion Model ---\n",
    "    final_champion_formula = \"\"\"\n",
    "        yield_soybean ~ year + bs(lat, df=4) + bs(lon, df=4) +\n",
    "                        potential_evaporation_May + solar_rad_Jul_z +\n",
    "                        soil_water + precipitation_May +\n",
    "                        I(solar_rad_Jul_z**2) + I(precipitation_May**2) +\n",
    "                        solar_rad_Jul_z:soil_water + potential_evaporation_May:solar_rad_Jul_z\n",
    "    \"\"\"\n",
    "    final_model = smf.glm(\n",
    "        formula=final_champion_formula,\n",
    "        data=df_soybean,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    ).fit()\n",
    "    print(f\"Final champion model for Soybean fitted successfully. AIC: {final_model.aic:.2f}\\n\")\n",
    "\n",
    "    # --- 3. Define Scenarios and the Baseline ---\n",
    "    # We define our water stress scenarios\n",
    "    sw_quantiles = df_soybean['soil_water'].quantile([0.25, 0.5, 0.75])\n",
    "    scenarios = {\n",
    "        'Drought (25th Pct SW)': {'value': sw_quantiles[0.25], 'color': 'orange'},\n",
    "        'Average (50th Pct SW)': {'value': sw_quantiles[0.50], 'color': 'purple'},\n",
    "        'Wet (75th Pct SW)': {'value': sw_quantiles[0.75], 'color': 'cyan'}\n",
    "    }\n",
    "    \n",
    "    # Our \"typical year\" is an average year for water and an average year for heat\n",
    "    median_values = {\n",
    "        'year': df_soybean['year'].median(),\n",
    "        'lat': df_soybean['lat'].median(),\n",
    "        'lon': df_soybean['lon'].median(),\n",
    "        'potential_evaporation_May': df_soybean['potential_evaporation_May'].median(),\n",
    "        'solar_rad_Jul_z': df_soybean['solar_rad_Jul_z'].median(), # Median (average) heat\n",
    "        'soil_water': df_soybean['soil_water'].median(),      # Median (average) water\n",
    "        'precipitation_May': df_soybean['precipitation_May'].median()\n",
    "    }\n",
    "    X_baseline = pd.DataFrame(median_values, index=[0])\n",
    "    yield_baseline = final_model.get_prediction(X_baseline).summary_frame()['mean'].iloc[0]\n",
    "    print(f\"Predicted baseline yield for a typical case (median heat, median water): {yield_baseline:.2f} t/ha\")\n",
    "\n",
    "    # --- 4. Generate and Plot Vulnerability for Each Scenario ---\n",
    "    print(\"Generating the vulnerability curve plot...\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(14, 9))\n",
    "    \n",
    "    solar_jul_z_range = np.linspace(df_soybean['solar_rad_Jul_z'].min(), df_soybean['solar_rad_Jul_z'].max(), 100)\n",
    "\n",
    "    for scenario_name, props in scenarios.items():\n",
    "        # Create the prediction grid for this scenario\n",
    "        pred_df_scenario = pd.DataFrame(median_values, index=range(100))\n",
    "        pred_df_scenario['solar_rad_Jul_z'] = solar_jul_z_range # Vary heat on x-axis\n",
    "        pred_df_scenario['soil_water'] = props['value']     # Set the water level for this scenario\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = final_model.get_prediction(pred_df_scenario).summary_frame(alpha=0.05)\n",
    "        yield_predicted = preds['mean']\n",
    "        \n",
    "        # Calculate percentage change from the single baseline\n",
    "        yield_change_pct = ((yield_predicted - yield_baseline) / yield_baseline) * 100\n",
    "        \n",
    "        # Plot the vulnerability curve for this scenario\n",
    "        ax.plot(solar_jul_z_range, yield_change_pct, color=props['color'], linewidth=3, label=scenario_name)\n",
    "\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_title('Vulnerability of Soybean Yield to July Heat, by Water Availability', fontsize=18)\n",
    "    ax.set_xlabel('Standardized July Solar Radiation (Z-score)', fontsize=14)\n",
    "    ax.set_ylabel('Yield Change vs. Typical Year (%)', fontsize=14)\n",
    "    ax.legend(title='Soil Water Scenario')\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4f88c",
   "metadata": {},
   "source": [
    "### **Analysis of the Final Soybean Vulnerability Curve**\n",
    "\n",
    "This multi-line vulnerability curve is the ultimate output of our soybean analysis. It visualizes the model's most important and sophisticated finding: the powerful interaction between July heat stress and overall water availability.\n",
    "\n",
    "#### **How to Read This Plot**\n",
    "\n",
    "*   The **X-axis** represents July conditions, from cool/cloudy (negative Z-scores) to hot/sunny (positive Z-scores).\n",
    "*   The **Y-axis** shows the predicted percentage change in yield compared to a baseline \"typical year\" (which has both average heat and average water).\n",
    "*   The **three colored lines** represent the vulnerability to July heat under three different seasonal water availability scenarios: Drought, Average, and Wet.\n",
    "\n",
    "#### **Primary Findings**\n",
    "\n",
    "1.  **Water Availability Sets the Yield Potential:**\n",
    "    *   The most striking feature is the clear separation of the curves. The \"Drought\" scenario (orange) is always in the negative, showing that a dry season consistently leads to **below-average yields**, with losses ranging from -10% to over -40%.\n",
    "    *   Conversely, the \"Wet\" scenario (cyan) is the only one that allows for **above-average yields**, with gains of over +10% possible under optimal temperatures.\n",
    "\n",
    "2.  **Heat Stress Determines the Final Outcome:**\n",
    "    *   All three curves show the same **inverted U-shape**, confirming that there is an optimal temperature for soybean in July. Extreme heat (moving to the right) is always damaging.\n",
    "    *   The key interaction is visible in the slopes: the decline in yield due to heat stress is **most severe in a drought year**. For example, moving from the optimal temperature to a \"heatwave\" (Z-score of +2) causes a much larger percentage loss in the \"Drought\" scenario than it does in the \"Wet\" scenario.\n",
    "\n",
    "#### **Overall Conclusion**\n",
    "\n",
    "The model's most powerful finding is that **water availability is the master variable that sets the yield potential for soybean, while July heat stress determines how much of that potential is realized or lost.** A drought year has a low yield ceiling and is highly vulnerable to heatwaves. A wet year not only has a much higher yield potential but also demonstrates greater resilience, suffering a smaller percentage loss when faced with the same level of heat stress. This quantifies the critical, compounding nature of climate risk for soybean production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a66a17",
   "metadata": {},
   "source": [
    "### **Final Soybean Model: Interpretation and Conclusions**\n",
    "\n",
    "This section summarizes the final champion model developed to explain the relationship between monthly climate stressors and soybean yield in Northern Italy. The model is the result of a multi-step workflow designed to be statistically robust, parsimonious, and highly insightful.\n",
    "\n",
    "#### **The Final Champion Model**\n",
    "\n",
    "After a data-driven process of variable selection and extensive iterative refinement, the final, best-performing model was determined to be a Gamma GLM with a complex interactive structure:\n",
    "\n",
    "**Final Model Formula:**\n",
    "```\n",
    "yield_soybean ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                potential_evaporation_May + solar_rad_Jul_z + \n",
    "                soil_water + precipitation_May + \n",
    "                I(solar_rad_Jul_z**2) + I(precipitation_May**2) + \n",
    "                solar_rad_Jul_z:soil_water + potential_evaporation_May:solar_rad_Jul_z\n",
    "```\n",
    "\n",
    "**Key Performance Metrics:**\n",
    "*   **Akaike Information Criterion (AIC):** `3447.76` (The lowest of all tested models)\n",
    "*   **Pseudo R-squared (CS):** `0.8513` (Explains approx. **85.1%** of the variation in yield)\n",
    "\n",
    "#### **The Modeling Journey: How We Arrived Here**\n",
    "\n",
    "The final model was the product of a systematic, evidence-based process:\n",
    "\n",
    "1.  **Variable Selection:** An **Elastic Net regularization** revealed that soybean yield is influenced by a more complex mix of factors than the previously modeled crops, identifying a broader set of robust climate predictors to carry forward.\n",
    "\n",
    "2.  **Model Refinement (Parsimony):** An initial base model was fitted, and non-significant predictors (`solar_radiation_Aug`, `solar_radiation_Sep`) were removed to create a simpler, more robust model with an improved AIC.\n",
    "\n",
    "3.  **Testing for Non-Linearity:** Guided by EDA and agronomic theory, we tested for non-linear effects one at a time. Adding a quadratic term for `solar_radiation_Jul` (after standardization) and `precipitation_May` both resulted in **massive drops in AIC**, confirming two distinct and critical non-linear relationships.\n",
    "\n",
    "4.  **Testing for Interactions:** We tested a series of plausible, theory-driven interactions. The model was significantly improved by including both the **`solar_rad_Jul_z:soil_water`** (Heat x Water Stress) and the **`potential_evaporation_May:solar_rad_Jul_z`** (Early vs. Mid-season Stress) interactions, each providing a large, statistically significant improvement in model fit.\n",
    "\n",
    "This structured process ensures our final model is not overfit and that its complexity is justified by strong statistical evidence.\n",
    "\n",
    "#### **Detailed Interpretation of the Final Model**\n",
    "\n",
    "*   **Control Variables:**\n",
    "    *   `year`: The positive, significant coefficient confirms a **technological trend**, with yields consistently increasing over time.\n",
    "    *   `bs(lat, df=4)` & `bs(lon, df=4)`: The high significance of the spatial splines confirms that **geography is a dominant driver** of yield.\n",
    "\n",
    "*   **Key Climate Drivers:**\n",
    "    *   **Non-Linear Effects:** The model identified two \"optimum\" conditions. Both `solar_radiation_Jul` and `precipitation_May` have **inverted U-shaped effects**, indicating that yields are maximized at a moderate level of July sun/heat and May rainfall. Too little or too much of either is detrimental.\n",
    "    *   **Interaction 1 (Heat x Water):** The significant `solar_rad_Jul_z:soil_water` interaction confirms that the negative impact of July heat stress is **amplified by drought**. Ample water availability helps the crop tolerate heat.\n",
    "    *   **Interaction 2 (Early x Mid-Season):** The significant `potential_evaporation_May:solar_rad_Jul_z` interaction reveals a compounding effect across the season, where early-season conditions in May modify the crop's response to heat stress in July.\n",
    "\n",
    "#### **Insights from Visualization**\n",
    "\n",
    "*   **The Non-Linear Response Curves:** The yield response plots for `solar_radiation_Jul` and `precipitation_May` visually confirm the **inverted U-shapes**. They allow us to identify the optimal points for these stressors and clearly see how yield declines when conditions deviate from this optimum.\n",
    "\n",
    "*   **The Final Vulnerability Curve (Interaction Plot):** This plot is the ultimate synthesis of the model's findings. By showing the vulnerability to July heat under different water scenarios (Drought, Average, Wet), it tells a powerful, multi-faceted story:\n",
    "    *   **Water Sets the Potential:** The plot clearly shows that achieving an **above-average yield** (+10%) is only possible in a \"Wet\" year. A \"Drought\" year has a much lower yield ceiling and is always predicted to have a below-average yield (from -10% to -40%).\n",
    "    *   **Heat Determines the Outcome:** In all scenarios, extreme heat (moving to the right on the x-axis) causes a sharp decline in yield. However, this decline is **most severe in a drought year**, visually confirming that water-stressed plants are far more vulnerable to heatwaves.\n",
    "\n",
    "#### **Overall Conclusion**\n",
    "\n",
    "The model provides a powerful and nuanced explanation of soybean yield vulnerability. The dominant story is a complex interplay of **heat and water**. The model's core insight is that **water availability is the master variable that sets the yield potential for the season, while July heat stress determines how much of that potential is ultimately realized.** A dry season is a state of chronic stress with a low yield ceiling and extreme vulnerability to heat. Conversely, a wet season not only has a higher yield potential but also provides a crucial buffer, making the crop more resilient to mid-summer heatwaves."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 42-wheat-spring-gamma-model.ipynb/42-wheat-spring-gamma-model.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b76663",
   "metadata": {},
   "source": [
    "# EDA for wheat_spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f666b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import scipy.stats as stats\n",
    "from patsy import dmatrices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeaa1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis - Full Correlation Matrix\n",
    "\n",
    "print(\"--- EDA: Correlation Analysis of All Monthly Stressors ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# As confirmed, this file is already specific to wheat_spring and its growing season.\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_spring = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Select Only the Monthly Stressor Variables ---\n",
    "    # We will select all columns that have a month name in them, which is a robust\n",
    "    # way to grab all the monthly predictors we want to investigate.\n",
    "    monthly_stressors = [col for col in df_wheat_spring.columns if '_' in col and 'yield' not in col]\n",
    "    df_corr = df_wheat_spring[monthly_stressors]\n",
    "    \n",
    "    print(f\"\\nSelected {len(df_corr.columns)} monthly stressor variables for correlation analysis.\")\n",
    "\n",
    "    # --- 3. Calculate and Print the Correlation Matrix ---\n",
    "    correlation_matrix = df_corr.corr()\n",
    "    \n",
    "    # Optional: If you want to see the full numerical matrix, uncomment the next line\n",
    "    # print(\"\\n--- Full Pairwise Correlation Matrix ---\")\n",
    "    # print(correlation_matrix)\n",
    "\n",
    "    # --- 4. Visualize the Matrix with a Heatmap ---\n",
    "    # A heatmap is the best way to see the broad patterns of collinearity.\n",
    "    print(\"\\nGenerating correlation heatmap...\")\n",
    "    \n",
    "    plt.figure(figsize=(18, 15))\n",
    "    heatmap = sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        cmap='coolwarm',  # Use a diverging colormap (red=positive, blue=negative)\n",
    "        center=0,         # Center the colormap at zero\n",
    "        vmin=-1,          # Set the color scale limits to the theoretical min/max\n",
    "        vmax=1,\n",
    "        linewidths=.5,\n",
    "        annot=False       # Annotations are turned off as the matrix is too large to be readable\n",
    "    )\n",
    "    \n",
    "    plt.title('Pairwise Correlation Matrix of All Monthly Stressors for wheat_spring', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800897af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Extended EDA for wheat_spring Yield Analysis ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_spring = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- Task 1: Examine the Distribution of the Dependent Variable (yield_wheat_spring) ---\n",
    "    print(\"--- Task 1: Analyzing the distribution of the dependent variable 'yield_wheat_spring' ---\")\n",
    "    \n",
    "    # Create a figure with two subplots side-by-side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Distribution Analysis for yield_wheat_springe', fontsize=16)\n",
    "\n",
    "    # a) Histogram with a Kernel Density Estimate (KDE)\n",
    "    # This helps us visually assess the shape, center, and spread of the yield data.\n",
    "    # We are checking for positive skewness, which is characteristic of data modeled by a Gamma distribution.\n",
    "    sns.histplot(df_wheat_spring['yield_wheat_spring'], kde=True, ax=axes[0])\n",
    "    axes[0].set_title('Histogram of wheat_spring Yield')\n",
    "    axes[0].set_xlabel('Yield (tonnes per hectare)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    # b) Q-Q (Quantile-Quantile) Plot against a theoretical normal distribution\n",
    "    # This plot helps us assess if the data's distribution follows a specific theoretical distribution.\n",
    "    # Deviations from the red line suggest skewness or heavy tails.\n",
    "    # While our target is a Gamma GLM, a Q-Q plot vs. Normal is a standard first step to detect non-normality.\n",
    "    stats.probplot(df_wheat_spring['yield_wheat_spring'], dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot of wheat_spring Yield vs. Normal Distribution')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Distribution plots generated. Check for positive skew, which supports our choice of a Gamma GLM.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 2: Bivariate Scatter Plots of Yield vs. Key Stressors ---\n",
    "    print(\"--- Task 2: Visualizing relationships between yield and key climate stressors ---\")\n",
    "    \n",
    "    # Select a few key stressors based on agronomic theory for wheat_spring\n",
    "    key_stressors = ['temperature_Jun', 'temperature_Jul', 'precipitation_Apr', \"temperature_May\" , \"temperature_Mar\" , \"potential_evaporation_Apr\"]\n",
    "    \n",
    "    # Create a figure to hold the scatter plots\n",
    "    fig, axes = plt.subplots(1, len(key_stressors), figsize=(21, 6))\n",
    "    fig.suptitle('Bivariate Relationships: wheat_spring Yield vs. Key Stressors', fontsize=16)\n",
    "\n",
    "    for i, stressor in enumerate(key_stressors):\n",
    "        # We use a regression plot with a LOWESS (Locally Weighted Scatterplot Smoothing) curve.\n",
    "        # This is a non-parametric way to see the underlying trend without assuming a linear relationship.\n",
    "        # It's excellent for spotting potential non-linearities (like an inverted 'U' shape).\n",
    "        sns.regplot(\n",
    "            x=stressor,\n",
    "            y='yield_wheat_spring',\n",
    "            data=df_wheat_spring,\n",
    "            ax=axes[i],\n",
    "            lowess=True, # Use LOWESS smoother to detect non-linear patterns\n",
    "            scatter_kws={'alpha': 0.1, 'color': 'gray'}, # De-emphasize individual points\n",
    "            line_kws={'color': 'blue'} # Emphasize the trend line\n",
    "        )\n",
    "        axes[i].set_title(f'Yield vs. {stressor}')\n",
    "        axes[i].set_xlabel(f'{stressor}')\n",
    "        axes[i].set_ylabel('Yield (tonnes per hectare)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Scatter plots generated. Look for non-linear patterns that might inform our final model.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 3: Plot Key Variables Over Time ---\n",
    "    print(\"--- Task 3: Examining long-term trends in yield and a key climate variable ---\")\n",
    "    \n",
    "    # Calculate the mean of yield and a key stressor for each year\n",
    "    yearly_data = df_wheat_spring.groupby('year')[['yield_wheat_spring', 'temperature_Jun']].mean().reset_index()\n",
    "\n",
    "    # Create a plot with a primary and secondary y-axis to show both trends together.\n",
    "    # This confirms the necessity of including 'year' as a control variable to capture trends\n",
    "    # likely related to technology, while also checking for climate trends.\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plotting average yield on the primary (left) y-axis\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Average Yield (tonnes per hectare)', color=color)\n",
    "    ax1.plot(yearly_data['year'], yearly_data['yield_wheat_spring'], color=color, marker='o', label='Average Yield')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Create a second y-axis that shares the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plotting average temperature on the secondary (right) y-axis\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Average June Temperature (C)', color=color)\n",
    "    ax2.plot(yearly_data['year'], yearly_data['temperature_Jun'], color=color, linestyle='--', marker='x', label='Avg. July Temp')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('Long-Term Trend of Spring Wheat Yield and June Temperature (1982-2016)', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    # Adding a single legend for both lines\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"Time-series plot generated. Note the clear upward trend in yield, confirming the need for a 'year' control variable.\\n\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: A required column was not found in the dataset: {e}. Please check the CSV file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44b618",
   "metadata": {},
   "source": [
    "# regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_spring = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Define the Full Model Formula ---\n",
    "    # Programmatically get all monthly stressor column names\n",
    "    monthly_stressors = [col for col in df_wheat_spring.columns if '_' in col and 'yield' not in col]\n",
    "    \n",
    "    # Join them with '+' to create the predictor part of the formula\n",
    "    stressor_formula_part = ' + '.join(monthly_stressors)\n",
    "    \n",
    "    # Construct the complete R-style formula string.\n",
    "    # We include our controls (year, spatial splines) and all potential predictors.\n",
    "    # Note: patsy's bs() function creates the basis spline columns.\n",
    "    formula = f\"yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + {stressor_formula_part}\"\n",
    "    \n",
    "    print(\"\\nGenerated model formula for patsy:\")\n",
    "    print(formula) # Uncomment to see the full, very long formula string\n",
    "\n",
    "    # --- 3. Create the Design Matrix (X) and Response Vector (y) ---\n",
    "    # patsy processes the formula and the dataframe to create the matrices needed for modeling.\n",
    "    # 'y' will be our dependent variable, 'X' will be the full set of predictors.\n",
    "    # The intercept is automatically included in 'X' by patsy.\n",
    "    print(\"\\nCreating design matrix (X) and response vector (y) using patsy...\")\n",
    "    y, X = dmatrices(formula, data=df_wheat_spring, return_type='dataframe')\n",
    "    \n",
    "    print(f\"Successfully created response vector y with shape: {y.shape}\")\n",
    "    print(f\"Successfully created design matrix X with shape: {X.shape}\")\n",
    "    print(f\"The {X.shape[1]} columns in X include the intercept, year, 8 spline bases (4 for lat, 4 for lon), and {len(monthly_stressors)} climate stressors.\")\n",
    "\n",
    "    # --- 4. Standardize the Predictor Matrix (X) ---\n",
    "    # We scale ALL predictors to have a mean of 0 and a standard deviation of 1.\n",
    "    # This ensures the regularization penalty is applied fairly to all variables.\n",
    "    # We do NOT scale the response variable y.\n",
    "    print(\"\\nStandardizing the design matrix X...\")\n",
    "    \n",
    "    # We remove the Intercept column before scaling, as it should not be regularized or scaled.\n",
    "    # We will add it back later if needed, but scikit-learn's models handle it by default.\n",
    "    X_no_intercept = X.drop('Intercept', axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_values = scaler.fit_transform(X_no_intercept)\n",
    "    \n",
    "    # Convert the scaled array back to a pandas DataFrame with the original column names\n",
    "    X_scaled = pd.DataFrame(X_scaled_values, columns=X_no_intercept.columns, index=X.index)\n",
    "    \n",
    "    print(\"Standardization complete.\")\n",
    "    \n",
    "    # Verification: Check the mean and standard deviation of a few scaled columns\n",
    "    print(\"\\n--- Verification of Standardization ---\")\n",
    "    verification_cols = ['year', 'bs(lat, df=4)[0]', 'temperature_Jul']\n",
    "    for col in verification_cols:\n",
    "        mean_val = X_scaled[col].mean()\n",
    "        std_val = X_scaled[col].std()\n",
    "        print(f\"Column '{col}': Mean = {mean_val:.4f}, Std Dev = {std_val:.4f}\")\n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume 'y' and 'X_scaled' are already in memory from the previous step.\n",
    "# If not, you would need to re-run the data preparation script.\n",
    "\n",
    "try:\n",
    "    # --- 1. Define the GLM Model ---\n",
    "    # We specify our model family (Gamma) and the link function (log) as per our project plan.\n",
    "    # We pass the prepared y and the fully scaled X matrix.\n",
    "    # Note: statsmodels requires the intercept to be in the X matrix, which patsy provided.\n",
    "    \n",
    "    # We need to add the intercept back to the scaled data for statsmodels GLM\n",
    "    X_scaled_with_intercept = X.copy() # Start with the original X to preserve intercept and structure\n",
    "    X_scaled_with_intercept[X_no_intercept.columns] = X_scaled # Replace non-intercept columns with scaled versions\n",
    "\n",
    "    gl_gamma = sm.GLM(y, X_scaled_with_intercept, family=sm.families.Gamma(link=sm.families.links.log()))\n",
    "    print(\"Successfully initialized Gamma GLM with a log link.\")\n",
    "\n",
    "    # --- 2. Set up the Regularization Path ---\n",
    "    # We need to test a series of alpha values (penalty strengths).\n",
    "    # A logarithmic scale is best for this, from a weak penalty to a strong one.\n",
    "    n_alphas = 100\n",
    "    alphas = np.logspace(-3, 0.5, n_alphas) # From 0.001 to ~3.16\n",
    "\n",
    "    # The L1_wt parameter controls the Elastic Net mix (0=Ridge, 1=Lasso). \n",
    "    # 0.5 is a balanced choice.\n",
    "    elastic_net_l1_wt = 0.5 \n",
    "    \n",
    "    print(f\"Will fit the model for {n_alphas} alpha values with L1_wt (l1_ratio) = {elastic_net_l1_wt}\")\n",
    "\n",
    "    # --- 3. Fit the Model for Each Alpha and Store Coefficients ---\n",
    "    # We will loop through our alphas and save the coefficients from each model fit.\n",
    "    coefficients = []\n",
    "    \n",
    "    for alpha_val in alphas:\n",
    "        # The fit_regularized method performs the Elastic Net estimation.\n",
    "        # We set refit=False because we want to see the shrunken coefficients for this analysis.\n",
    "        results = gl_gamma.fit_regularized(\n",
    "            method='elastic_net', \n",
    "            alpha=alpha_val, \n",
    "            L1_wt=elastic_net_l1_wt,\n",
    "            refit=False \n",
    "        )\n",
    "        coefficients.append(results.params)\n",
    "    \n",
    "    # Convert the list of coefficient series into a DataFrame for easy plotting\n",
    "    coef_df = pd.DataFrame(coefficients, index=alphas)\n",
    "    coef_df.index.name = \"alpha\"\n",
    "    \n",
    "    # Exclude the Intercept for plotting, as it's not regularized and has a different scale.\n",
    "    coef_df_no_intercept = coef_df.drop('Intercept', axis=1)\n",
    "    \n",
    "    print(\"\\nCompleted fitting models along the regularization path.\")\n",
    "\n",
    "    # --- 4. Visualize the Regularization Path ---\n",
    "    print(\"Generating the regularization path plot...\")\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    ax.plot(coef_df_no_intercept)\n",
    "    ax.set_xscale('log') # The alpha path is best viewed on a log scale\n",
    "    \n",
    "    # Add a vertical line at zero\n",
    "    ax.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    \n",
    "    ax.set_title('Regularization Path for spring Wheat Model Coefficients (Elastic Net)', fontsize=18)\n",
    "    ax.set_xlabel('Alpha (Penalty Strength)', fontsize=14)\n",
    "    ax.set_ylabel('Standardized Coefficient Value', fontsize=14)\n",
    "    \n",
    "    # To avoid a cluttered legend, we don't add one here. The goal is to see the general pattern.\n",
    "    # Alternatively, for fewer variables, a legend could be useful:\n",
    "    # ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"ERROR: Make sure that 'y' and 'X_scaled' DataFrames from the previous step are available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ddb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Corrected code to identify the most robust variables ---\n",
    "# We will inspect the coefficients at a moderately high alpha value\n",
    "# This tells us which variables \"survived\" the penalty the longest.\n",
    "alpha_to_inspect = 0.03\n",
    "\n",
    "try:\n",
    "    # Find the alpha in our index that is closest to our target\n",
    "    # CORRECTED LINE: The operation works directly on the index without .flat\n",
    "    closest_alpha = coef_df.index[np.abs(coef_df.index - alpha_to_inspect).argmin()]\n",
    "\n",
    "    print(f\"--- Coefficients at alpha  {closest_alpha:.4f} ---\")\n",
    "\n",
    "    # Get the coefficients at this alpha and sort them by absolute value\n",
    "    robust_coeffs = coef_df.loc[closest_alpha].copy()\n",
    "    robust_coeffs_sorted = robust_coeffs.abs().sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nVariables sorted by the absolute magnitude of their shrunken coefficient:\")\n",
    "    # We display more variables to get a fuller picture\n",
    "    print(robust_coeffs_sorted.head(15))\n",
    "\n",
    "    # Let's also see their actual values (positive or negative) for the top variables\n",
    "    print(\"\\n--- Actual coefficient values for the most robust variables ---\")\n",
    "    print(coef_df.loc[closest_alpha, robust_coeffs_sorted.index].head(15))\n",
    "\n",
    "except NameError:\n",
    "     print(\"ERROR: Make sure that 'coef_df' DataFrame from the previous step is available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dec827",
   "metadata": {},
   "source": [
    "# modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac93dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Base Champion Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_spring = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the base Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    base_formula = \"yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + temperature_Jul + temperature_May + temperature_Mar + potential_evaporation_Apr + precipitation_Apr\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    base_model = smf.glm(\n",
    "        formula=base_formula,\n",
    "        data=df_wheat_spring,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    base_model_results = base_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(base_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Future Comparison ---\n",
    "    # The AIC is a key metric for comparing different model formulations. Lower is better.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Base Champion Model: {base_model_results.aic:.2f}\")\n",
    "    print(\"This will be our benchmark for comparison.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38304cae",
   "metadata": {},
   "source": [
    "# quadratic terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Quadratic Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_spring = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the base Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    quadratic_formula = \"yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + temperature_Jul + I(temperature_Jul**2) + temperature_May + I(temperature_May**2) + temperature_Mar + potential_evaporation_Apr + precipitation_Apr + I(precipitation_Apr**2)\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    quadratic_model = smf.glm(\n",
    "        formula=quadratic_formula,\n",
    "        data=df_wheat_spring,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    quadratic_model_results = quadratic_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(quadratic_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion's AIC (3516.86).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Quadratic Model: {quadratic_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the Base Model's AIC (3518.87).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d333b",
   "metadata": {},
   "source": [
    "# interractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Quadratic Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_spring = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the base Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    interaction_formula = \"yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + temperature_May:temperature_Jul + temperature_Jul + I(temperature_Jul**2) + temperature_May + I(temperature_May**2) + temperature_Mar + potential_evaporation_Apr + precipitation_Apr + I(precipitation_Apr**2)\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    interaction_model = smf.glm(\n",
    "        formula=interaction_formula,\n",
    "        data=df_wheat_spring,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    interaction_model_results = interaction_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the interaction Champion Model ---\")\n",
    "    print(interaction_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion's AIC (3516.86).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Interaction Model: {interaction_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the quadratic Model's AIC (3481.75).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277612df",
   "metadata": {},
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Generating Final Yield Response Curves for Spring Wheat ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "df_wheat = pd.read_csv(file_path)\n",
    "df_wheat = df_wheat[df_wheat['yield_wheat_spring'] > 0].copy()\n",
    "print(\"Data prepared successfully.\")\n",
    "\n",
    "# --- 2. Fit Our Final Champion Model ---\n",
    "final_champion_formula = \"\"\"\n",
    "    yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                         temperature_Jul + I(temperature_Jul**2) + \n",
    "                         temperature_May + I(temperature_May**2) + \n",
    "                         temperature_Mar + potential_evaporation_Apr + \n",
    "                         precipitation_Apr + I(precipitation_Apr**2) +\n",
    "                         temperature_May:temperature_Jul\n",
    "\"\"\"\n",
    "print(\"Fitting Final Champion model for Spring Wheat...\")\n",
    "final_model = smf.glm(\n",
    "    formula=final_champion_formula,\n",
    "    data=df_wheat,\n",
    "    family=sm.families.Gamma(link=sm.families.links.log())\n",
    ").fit()\n",
    "print(f\"Model fitted successfully. AIC: {final_model.aic:.2f}\")\n",
    "\n",
    "\n",
    "# --- 3. Prepare a Base Prediction Dictionary with Median Values ---\n",
    "median_values = {\n",
    "    'year': df_wheat['year'].median(),\n",
    "    'lat': df_wheat['lat'].median(),\n",
    "    'lon': df_wheat['lon'].median(),\n",
    "    'temperature_Jul': df_wheat['temperature_Jul'].median(),\n",
    "    'temperature_May': df_wheat['temperature_May'].median(),\n",
    "    'temperature_Mar': df_wheat['temperature_Mar'].median(),\n",
    "    'potential_evaporation_Apr': df_wheat['potential_evaporation_Apr'].median(),\n",
    "    'precipitation_Apr': df_wheat['precipitation_Apr'].median()\n",
    "}\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "# --- PLOTS A, B, C: The Three Non-Linear Main Effects ---\n",
    "# We create a dictionary to loop through, making the code cleaner\n",
    "plots_to_make = {\n",
    "    'temperature_Jul': {'color': 'red', 'xlabel': 'Average July Temperature (C)'},\n",
    "    'temperature_May': {'color': 'orange', 'xlabel': 'Average May Temperature (C)'},\n",
    "    'precipitation_Apr': {'color': 'blue', 'xlabel': 'April Precipitation (mm)'}\n",
    "}\n",
    "\n",
    "for var, props in plots_to_make.items():\n",
    "    print(f\"\\nGenerating plot: Non-Linear Yield Response to {var}...\")\n",
    "    \n",
    "    x_range = np.linspace(df_wheat[var].min(), df_wheat[var].max(), 100)\n",
    "    pred_df = pd.DataFrame(median_values, index=range(100))\n",
    "    pred_df[var] = x_range\n",
    "    \n",
    "    preds = final_model.get_prediction(pred_df).summary_frame(alpha=0.05)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(pred_df[var], preds['mean'], color=props['color'], linewidth=3, label='Predicted Yield')\n",
    "    plt.fill_between(pred_df[var], preds['mean_ci_lower'], preds['mean_ci_upper'], color=props['color'], alpha=0.2, label='95% Confidence Interval')\n",
    "    \n",
    "    plt.title(f'Non-Linear Yield Response of Spring Wheat to {var}', fontsize=18)\n",
    "    plt.xlabel(props['xlabel'], fontsize=14)\n",
    "    plt.ylabel('Predicted Spring Wheat Yield (t/ha)', fontsize=14)\n",
    "    plt.legend(); plt.ylim(bottom=0); plt.show()\n",
    "\n",
    "\n",
    "# --- PLOT D: The \"Money Plot\" - Compounding Heat Stress Interaction ---\n",
    "print('\\nGenerating Plot D: The \"Compounding Heat Stress\" Interaction Effect...')\n",
    "\n",
    "# Define the scenarios for May temperature\n",
    "temp_may_quantiles = df_wheat['temperature_May'].quantile([0.25, 0.5, 0.75])\n",
    "scenarios = {\n",
    "    'Cool May (25th Pct)': {'value': temp_may_quantiles[0.25], 'color': 'blue'},\n",
    "    'Average May (50th Pct)': {'value': temp_may_quantiles[0.50], 'color': 'purple'},\n",
    "    'Hot May (75th Pct)': {'value': temp_may_quantiles[0.75], 'color': 'red'}\n",
    "}\n",
    "\n",
    "x_range_jul = np.linspace(df_wheat['temperature_Jul'].min(), df_wheat['temperature_Jul'].max(), 100)\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "for scenario_name, props in scenarios.items():\n",
    "    pred_df_interact = pd.DataFrame(median_values, index=range(100))\n",
    "    pred_df_interact['temperature_Jul'] = x_range_jul  # Vary July temp on x-axis\n",
    "    pred_df_interact['temperature_May'] = props['value'] # Set the May temp for this scenario\n",
    "    \n",
    "    preds_interact = final_model.get_prediction(pred_df_interact).summary_frame(alpha=0.05)\n",
    "    \n",
    "    plt.plot(pred_df_interact['temperature_Jul'], preds_interact['mean'], color=props['color'], linewidth=3, label=scenario_name)\n",
    "    plt.fill_between(pred_df_interact['temperature_Jul'], preds_interact['mean_ci_lower'], preds_interact['mean_ci_upper'], color=props['color'], alpha=0.15)\n",
    "\n",
    "plt.title('Vulnerability to July Heat is Compounded by a Hot May', fontsize=18)\n",
    "plt.xlabel('Average July Temperature (C)', fontsize=14)\n",
    "plt.ylabel('Predicted Spring Wheat Yield (t/ha)', fontsize=14)\n",
    "plt.legend(title='May Temperature Scenario'); plt.ylim(bottom=0); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af7205",
   "metadata": {},
   "source": [
    "### **Analysis of Spring Wheat Yield Response Curves**\n",
    "\n",
    "These plots visualize the key relationships from our final, complex statistical model. They show how predicted spring wheat yield responds to specific climate factors, holding all others at their typical values.\n",
    "\n",
    "#### **Plots 1 & 2: The Dominance of Heat Stress (`temperature_Jul` & `temperature_May`)**\n",
    "\n",
    "*   **Primary Finding:** Higher temperatures in both May and July are consistently and significantly associated with lower yields.\n",
    "*   **Interpretation:** Both plots show a similar pattern: a steep decline in yield as temperatures rise. The slight curve reveals a subtle inverted U-shape, but the peak occurs at very cool temperatures. This indicates that for the vast majority of observed conditions, the primary effect of temperature in May and July is **heat stress**.\n",
    "\n",
    "#### **Plot 3: The Optimal Level of Early-Season Rain (`precipitation_Apr`)**\n",
    "\n",
    "*   **Primary Finding:** The relationship with April precipitation is a distinct **inverted U-shape**.\n",
    "*   **Interpretation:** The model shows that yield is maximized at a moderate level of spring rainfall (approx. 200-250 mm). This demonstrates a \"too much of a good thing\" effect, where both drought conditions (too little rain) and waterlogging/oversaturation (too much rain) are detrimental to crop establishment.\n",
    "\n",
    "#### **Plot 4: The Core Insight - Compounding Heat Stress**\n",
    "\n",
    "*   **Primary Finding:** The plant's vulnerability to heat stress in July is **compounded by heat stress experienced in May.**\n",
    "*   **Interpretation:** This plot visualizes our most sophisticated finding. It shows the yield response to July heat under three different May temperature scenarios.\n",
    "    *   The **red line (\"Hot May\")** has the steepest downward slope. This is the visual proof that a plant \"pre-stressed\" by a hot May suffers the most severe yield losses when it also faces a hot July.\n",
    "    *   Conversely, the **blue line (\"Cool May\")** is the flattest. This demonstrates that a plant that enjoys a mild May is more **resilient**, suffering a much smaller yield penalty when faced with the same level of July heat. This quantifies the critical, compounding nature of heat stress across different growth stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcaa785",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Generating Final Vulnerability Curves for Spring Wheat ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/wheat_spring_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat = pd.read_csv(file_path)\n",
    "    df_wheat = df_wheat[df_wheat['yield_wheat_spring'] > 0].copy()\n",
    "    print(\"Data prepared successfully.\")\n",
    "\n",
    "    # --- 2. Fit Our Final Champion Model ---\n",
    "    final_champion_formula = \"\"\"\n",
    "        yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                             temperature_Jul + I(temperature_Jul**2) + \n",
    "                             temperature_May + I(temperature_May**2) + \n",
    "                             temperature_Mar + potential_evaporation_Apr + \n",
    "                             precipitation_Apr + I(precipitation_Apr**2) +\n",
    "                             temperature_May:temperature_Jul\n",
    "    \"\"\"\n",
    "    final_model = smf.glm(\n",
    "        formula=final_champion_formula,\n",
    "        data=df_wheat,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    ).fit()\n",
    "    print(f\"Final champion model for Spring Wheat fitted successfully. AIC: {final_model.aic:.2f}\\n\")\n",
    "\n",
    "    # --- 3. Define Scenarios and the Baseline ---\n",
    "    # We define our \"compounding heat stress\" scenarios based on May's temperature\n",
    "    temp_may_quantiles = df_wheat['temperature_May'].quantile([0.25, 0.5, 0.75])\n",
    "    scenarios = {\n",
    "        'Cool May (25th Pct)': {'value': temp_may_quantiles[0.25], 'color': 'blue'},\n",
    "        'Average May (50th Pct)': {'value': temp_may_quantiles[0.50], 'color': 'purple'},\n",
    "        'Hot May (75th Pct)': {'value': temp_may_quantiles[0.75], 'color': 'red'}\n",
    "    }\n",
    "    \n",
    "    # Our \"typical year\" is a year with average conditions in both May and July\n",
    "    median_values = {\n",
    "        'year': df_wheat['year'].median(),\n",
    "        'lat': df_wheat['lat'].median(),\n",
    "        'lon': df_wheat['lon'].median(),\n",
    "        'temperature_Jul': df_wheat['temperature_Jul'].median(),  # Median July heat\n",
    "        'temperature_May': df_wheat['temperature_May'].median(),  # Median May heat\n",
    "        'temperature_Mar': df_wheat['temperature_Mar'].median(),\n",
    "        'potential_evaporation_Apr': df_wheat['potential_evaporation_Apr'].median(),\n",
    "        'precipitation_Apr': df_wheat['precipitation_Apr'].median()\n",
    "    }\n",
    "    X_baseline = pd.DataFrame(median_values, index=[0])\n",
    "    yield_baseline = final_model.get_prediction(X_baseline).summary_frame()['mean'].iloc[0]\n",
    "    print(f\"Predicted baseline yield for a typical case (median May, median July): {yield_baseline:.2f} t/ha\")\n",
    "\n",
    "    # --- 4. Generate and Plot Vulnerability for Each Scenario ---\n",
    "    print(\"Generating the vulnerability curve plot...\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(14, 9))\n",
    "    \n",
    "    temp_jul_range = np.linspace(df_wheat['temperature_Jul'].min(), df_wheat['temperature_Jul'].max(), 100)\n",
    "\n",
    "    for scenario_name, props in scenarios.items():\n",
    "        # Create the prediction grid for this scenario\n",
    "        pred_df_scenario = pd.DataFrame(median_values, index=range(100))\n",
    "        pred_df_scenario['temperature_Jul'] = temp_jul_range # Vary July heat on x-axis\n",
    "        pred_df_scenario['temperature_May'] = props['value'] # Set the May heat level for this scenario\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = final_model.get_prediction(pred_df_scenario).summary_frame(alpha=0.05)\n",
    "        yield_predicted = preds['mean']\n",
    "        \n",
    "        # Calculate percentage change from the single baseline\n",
    "        yield_change_pct = ((yield_predicted - yield_baseline) / yield_baseline) * 100\n",
    "        \n",
    "        # Plot the vulnerability curve for this scenario\n",
    "        ax.plot(temp_jul_range, yield_change_pct, color=props['color'], linewidth=3, label=scenario_name)\n",
    "\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_title('Vulnerability of Spring Wheat to July Heat, Compounded by May Temperature', fontsize=18)\n",
    "    ax.set_xlabel('Average July Temperature (C)', fontsize=14)\n",
    "    ax.set_ylabel('Yield Change vs. Typical Year (%)', fontsize=14)\n",
    "    ax.legend(title='May Temperature Scenario')\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842908cd",
   "metadata": {},
   "source": [
    "### **Analysis of the Final Spring Wheat Vulnerability Curve**\n",
    "\n",
    "This vulnerability curve is the ultimate synthesis of our model's most important finding: the compounding nature of heat stress. It translates the complex statistical interaction into a clear, quantitative measure of risk and opportunity.\n",
    "\n",
    "#### **How to Read This Plot**\n",
    "\n",
    "*   The **X-axis** represents the range of average July temperatures, from cool to extremely hot.\n",
    "*   The **Y-axis** shows the predicted percentage change in yield compared to a baseline \"typical year\" (which has both average May and average July temperatures).\n",
    "*   The **three colored lines** represent the vulnerability to July's temperature under three different May temperature scenarios: Cool, Average, and Hot.\n",
    "\n",
    "#### **Primary Findings**\n",
    "\n",
    "1.  **Compounding Vulnerability to Heat:**\n",
    "    *   The most critical finding is the difference in the slopes of the curves on the right side of the plot. The **red line (\"Hot May\")** is the steepest, showing the most dramatic yield loss as July gets hotter. For example, at 25C in July, a plant that also experienced a hot May suffers a yield loss of approximately **-15%**.\n",
    "    *   In contrast, the **blue line (\"Cool May\")** is the flattest. A plant that had a mild start to the season is more resilient; it suffers a much smaller yield loss of only about **-5%** at the same 25C July temperature.\n",
    "\n",
    "2.  **A Complex \"Head Start\" Effect:**\n",
    "    *   Interestingly, the plot reveals a crossover effect. In the rare scenario of a **very cool July** (below ~18C), a \"Hot May\" (red line) actually leads to the highest yield *gain*. This suggests that an initial burst of warmth can be beneficial if it is not followed by subsequent heat stress, possibly by accelerating early development.\n",
    "\n",
    "3.  **The Dominance of Heat Stress:**\n",
    "    *   For any July temperature above ~21C, all scenarios result in a **yield loss** compared to a typical year. This confirms that late-season heat stress is the dominant threat to spring wheat in this region, and a \"Hot May\" consistently and significantly worsens this vulnerability.\n",
    "\n",
    "#### **Overall Conclusion**\n",
    "\n",
    "This plot powerfully visualizes the model's core insight: **vulnerability is not static.** A hot May \"pre-stresses\" the crop, making it substantially more vulnerable to the damaging effects of a hot July. Conversely, a cool May builds resilience, helping to buffer the crop against subsequent heat stress. This quantifies the critical, compounding nature of climate risk for spring wheat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa80d1",
   "metadata": {},
   "source": [
    "### **Final Spring Wheat Model: Interpretation and Conclusions**\n",
    "\n",
    "This section summarizes the final champion model developed to explain the relationship between monthly climate stressors and spring wheat yield in Northern Italy. The model is the result of a multi-step workflow designed to be statistically robust, parsimonious, and highly insightful.\n",
    "\n",
    "#### **The Final Champion Model**\n",
    "\n",
    "After a data-driven process of variable selection and extensive iterative refinement, the final, best-performing model was determined to be a Gamma GLM with a complex interactive and non-linear structure:\n",
    "\n",
    "**Final Model Formula:**\n",
    "```\n",
    "yield_wheat_spring ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                     temperature_Jul + I(temperature_Jul**2) + \n",
    "                     temperature_May + I(temperature_May**2) + \n",
    "                     temperature_Mar + potential_evaporation_Apr + \n",
    "                     precipitation_Apr + I(precipitation_Apr**2) +\n",
    "                     temperature_May:temperature_Jul\n",
    "```\n",
    "\n",
    "**Key Performance Metrics:**\n",
    "*   **Akaike Information Criterion (AIC):** `3467.84` (The lowest of all tested models)\n",
    "*   **Pseudo R-squared (CS):** `0.5762` (Explains approx. **58%** of the variation in yield)\n",
    "\n",
    "#### **The Modeling Journey: How We Arrived Here**\n",
    "\n",
    "The final model was the product of a systematic, evidence-based process:\n",
    "\n",
    "1.  **Variable Selection:** An **Elastic Net regularization** revealed that spring wheat yield is influenced by a complex mix of factors throughout its earlier growing season, identifying a broad set of robust predictors.\n",
    "\n",
    "2.  **Model Refinement (Parsimony):** An initial base model was fitted. Unlike other crops, all selected climate predictors proved to be statistically significant, indicating a more complex set of initial drivers.\n",
    "\n",
    "3.  **Testing for Non-Linearity:** Guided by strong evidence from our EDA, we systematically tested for non-linear effects. Adding quadratic terms for `temperature_Jul`, `temperature_May`, and `precipitation_Apr` all resulted in **massive, successive drops in the AIC**, confirming that multiple, distinct \"optimal\" conditions are critical for spring wheat.\n",
    "\n",
    "4.  **Testing for Interactions:** We tested the most plausible interaction hypotheses. While a `Heat x Water` interaction was not significant, the **`temperature_May:temperature_Jul`** (compounding heat stress) interaction proved **highly significant and dramatically improved the AIC**, becoming the core insight of the final model.\n",
    "\n",
    "This structured process ensures our final model is not overfit and that its complexity is justified by strong statistical evidence.\n",
    "\n",
    "#### **Detailed Interpretation of the Final Model**\n",
    "\n",
    "*   **Control Variables:**\n",
    "    *   `year`: The positive, significant coefficient confirms a strong **technological trend**, with yields consistently increasing over time.\n",
    "    *   `bs(lat, df=4)` & `bs(lon, df=4)`: The high significance of the spatial splines confirms that **geography is a dominant driver** of yield.\n",
    "\n",
    "*   **Key Climate Drivers:**\n",
    "    *   **Multiple Non-Linear Effects:** The model identified three distinct \"optimal\" conditions. `temperature_Jul`, `temperature_May`, and `precipitation_Apr` all have significant **inverted U-shaped effects**. This is a major finding, indicating that spring wheat is highly sensitive to \"just right\" conditions, and yields suffer if it is too hot/cold or too wet/dry during these critical months.\n",
    "    *   **The Compounding Heat Stress Interaction:** This is the model's most important finding. The significant `temperature_May:temperature_Jul` interaction reveals that heat stress is not simply additive; it's compounding. A hot May \"pre-stresses\" the crop, making it significantly more vulnerable to the damaging effects of a subsequent hot July.\n",
    "\n",
    "#### **Insights from Visualization**\n",
    "\n",
    "*   **The Non-Linear Response Curves:** The yield response plots for the three temperature and precipitation variables visually confirmed the **inverted U-shapes**, allowing us to see the optimal conditions for each month and the penalty for deviating from them.\n",
    "\n",
    "*   **The Final Vulnerability Curve (Interaction Plot):** This plot synthesizes the core finding into a quantitative measure of risk. By showing the vulnerability to July heat under different May temperature scenarios, it tells a clear story:\n",
    "    *   **Resilience:** A crop that experiences a \"Cool May\" (blue line) is more resilient, suffering a much smaller percentage yield loss from a hot July.\n",
    "    *   **Vulnerability:** A crop \"pre-stressed\" by a \"Hot May\" (red line) is far more vulnerable, suffering a dramatically more severe yield loss when faced with the same July heat.\n",
    "\n",
    "#### **Overall Conclusion**\n",
    "\n",
    "The model provides a powerful and nuanced explanation of spring wheat vulnerability. Unlike the other summer crops, spring wheat's success is not tied to one or two dominant factors but is highly dependent on achieving **\"optimal\" conditions across multiple, distinct phases** of its growth (April establishment, May growth, July grain-fill). The model's core insight is the discovery of **compounding heat stress**, where early-season heat in May dramatically amplifies the crop's vulnerability to the primary threat of a hot July. This highlights a complex, time-dependent climate risk profile that is unique to spring wheat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````

## File: 43-wheat-winter-gamma-models.ipynb/43-wheat-winter-gamma-models.ipynb
````
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59fc82f3",
   "metadata": {},
   "source": [
    "# EDA for Winter Wheat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c22e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import scipy.stats as stats\n",
    "from patsy import dmatrices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c840638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis - Full Correlation Matrix\n",
    "\n",
    "print(\"--- EDA: Correlation Analysis of All Monthly Stressors ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# As confirmed, this file is already specific to wheat_winter and its growing season.\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_winter = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Select Only the Monthly Stressor Variables ---\n",
    "    # We will select all columns that have a month name in them, which is a robust\n",
    "    # way to grab all the monthly predictors we want to investigate.\n",
    "    monthly_stressors = [col for col in df_wheat_winter.columns if '_' in col and 'yield' not in col]\n",
    "    df_corr = df_wheat_winter[monthly_stressors]\n",
    "    \n",
    "    print(f\"\\nSelected {len(df_corr.columns)} monthly stressor variables for correlation analysis.\")\n",
    "\n",
    "    # --- 3. Calculate and Print the Correlation Matrix ---\n",
    "    correlation_matrix = df_corr.corr()\n",
    "    \n",
    "    # Optional: If you want to see the full numerical matrix, uncomment the next line\n",
    "    # print(\"\\n--- Full Pairwise Correlation Matrix ---\")\n",
    "    # print(correlation_matrix)\n",
    "\n",
    "    # --- 4. Visualize the Matrix with a Heatmap ---\n",
    "    # A heatmap is the best way to see the broad patterns of collinearity.\n",
    "    print(\"\\nGenerating correlation heatmap...\")\n",
    "    \n",
    "    plt.figure(figsize=(18, 15))\n",
    "    heatmap = sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        cmap='coolwarm',  # Use a diverging colormap (red=positive, blue=negative)\n",
    "        center=0,         # Center the colormap at zero\n",
    "        vmin=-1,          # Set the color scale limits to the theoretical min/max\n",
    "        vmax=1,\n",
    "        linewidths=.5,\n",
    "        annot=False       # Annotations are turned off as the matrix is too large to be readable\n",
    "    )\n",
    "    \n",
    "    plt.title('Pairwise Correlation Matrix of All Monthly Stressors for wheat_winter', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Extended EDA for wheat_winter Yield Analysis ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_winter = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- Task 1: Examine the Distribution of the Dependent Variable (yield_wheat_winter) ---\n",
    "    print(\"--- Task 1: Analyzing the distribution of the dependent variable 'yield_wheat_winter' ---\")\n",
    "    \n",
    "    # Create a figure with two subplots side-by-side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Distribution Analysis for yield_wheat_winter', fontsize=16)\n",
    "\n",
    "    # a) Histogram with a Kernel Density Estimate (KDE)\n",
    "    # This helps us visually assess the shape, center, and spread of the yield data.\n",
    "    # We are checking for positive skewness, which is characteristic of data modeled by a Gamma distribution.\n",
    "    sns.histplot(df_wheat_winter['yield_wheat_winter'], kde=True, ax=axes[0])\n",
    "    axes[0].set_title('Histogram of wheat_winter Yield')\n",
    "    axes[0].set_xlabel('Yield (tonnes per hectare)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    # b) Q-Q (Quantile-Quantile) Plot against a theoretical normal distribution\n",
    "    # This plot helps us assess if the data's distribution follows a specific theoretical distribution.\n",
    "    # Deviations from the red line suggest skewness or heavy tails.\n",
    "    # While our target is a Gamma GLM, a Q-Q plot vs. Normal is a standard first step to detect non-normality.\n",
    "    stats.probplot(df_wheat_winter['yield_wheat_winter'], dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot of wheat_winter Yield vs. Normal Distribution')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Distribution plots generated. Check for positive skew, which supports our choice of a Gamma GLM.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 2: Bivariate Scatter Plots of Yield vs. Key Stressors ---\n",
    "    print(\"--- Task 2: Visualizing relationships between yield and key climate stressors ---\")\n",
    "    \n",
    "    # Select a few key stressors based on agronomic theory for wheat_winter\n",
    "    key_stressors = ['temperature_May','potential_evaporation_Jan','precipitation_Apr','temperature_Nov','temperature_Mar','potential_evaporation_Apr']\n",
    "    \n",
    "    # Create a figure to hold the scatter plots\n",
    "    fig, axes = plt.subplots(1, len(key_stressors), figsize=(21, 6))\n",
    "    fig.suptitle('Bivariate Relationships: wheat_winter Yield vs. Key Stressors', fontsize=16)\n",
    "\n",
    "    for i, stressor in enumerate(key_stressors):\n",
    "        # We use a regression plot with a LOWESS (Locally Weighted Scatterplot Smoothing) curve.\n",
    "        # This is a non-parametric way to see the underlying trend without assuming a linear relationship.\n",
    "        # It's excellent for spotting potential non-linearities (like an inverted 'U' shape).\n",
    "        sns.regplot(\n",
    "            x=stressor,\n",
    "            y='yield_wheat_winter',\n",
    "            data=df_wheat_winter,\n",
    "            ax=axes[i],\n",
    "            lowess=True, # Use LOWESS smoother to detect non-linear patterns\n",
    "            scatter_kws={'alpha': 0.1, 'color': 'gray'}, # De-emphasize individual points\n",
    "            line_kws={'color': 'blue'} # Emphasize the trend line\n",
    "        )\n",
    "        axes[i].set_title(f'Yield vs. {stressor}')\n",
    "        axes[i].set_xlabel(f'{stressor}')\n",
    "        axes[i].set_ylabel('Yield (tonnes per hectare)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    print(\"Scatter plots generated. Look for non-linear patterns that might inform our final model.\\n\")\n",
    "\n",
    "\n",
    "    # --- Task 3: Plot Key Variables Over Time ---\n",
    "    print(\"--- Task 3: Examining long-term trends in yield and a key climate variable ---\")\n",
    "    \n",
    "    # Calculate the mean of yield and a key stressor for each year\n",
    "    yearly_data = df_wheat_winter.groupby('year')[['yield_wheat_winter', 'temperature_May']].mean().reset_index()\n",
    "\n",
    "    # Create a plot with a primary and secondary y-axis to show both trends together.\n",
    "    # This confirms the necessity of including 'year' as a control variable to capture trends\n",
    "    # likely related to technology, while also checking for climate trends.\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plotting average yield on the primary (left) y-axis\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Average Yield (tonnes per hectare)', color=color)\n",
    "    ax1.plot(yearly_data['year'], yearly_data['yield_wheat_winter'], color=color, marker='o', label='Average Yield')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Create a second y-axis that shares the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plotting average temperature on the secondary (right) y-axis\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Average May Temperature (C)', color=color)\n",
    "    ax2.plot(yearly_data['year'], yearly_data['temperature_May'], color=color, linestyle='--', marker='x', label='Avg. July Temp')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('Long-Term Trend of winter Wheat Yield and May Temperature (1982-2016)', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    # Adding a single legend for both lines\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"Time-series plot generated. Note the clear upward trend in yield, confirming the need for a 'year' control variable.\\n\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: A required column was not found in the dataset: {e}. Please check the CSV file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970bed5",
   "metadata": {},
   "source": [
    "# regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb7f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load the Data ---\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_winter = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\")\n",
    "\n",
    "    # --- 2. Define the Full Model Formula ---\n",
    "    # Programmatically get all monthly stressor column names\n",
    "    monthly_stressors = [col for col in df_wheat_winter.columns if '_' in col and 'yield' not in col]\n",
    "    \n",
    "    # Join them with '+' to create the predictor part of the formula\n",
    "    stressor_formula_part = ' + '.join(monthly_stressors)\n",
    "    \n",
    "    # Construct the complete R-style formula string.\n",
    "    # We include our controls (year, spatial splines) and all potential predictors.\n",
    "    # Note: patsy's bs() function creates the basis spline columns.\n",
    "    formula = f\"yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + {stressor_formula_part}\"\n",
    "    \n",
    "    print(\"\\nGenerated model formula for patsy:\")\n",
    "    print(formula) # Uncomment to see the full, very long formula string\n",
    "\n",
    "    # --- 3. Create the Design Matrix (X) and Response Vector (y) ---\n",
    "    # patsy processes the formula and the dataframe to create the matrices needed for modeling.\n",
    "    # 'y' will be our dependent variable, 'X' will be the full set of predictors.\n",
    "    # The intercept is automatically included in 'X' by patsy.\n",
    "    print(\"\\nCreating design matrix (X) and response vector (y) using patsy...\")\n",
    "    y, X = dmatrices(formula, data=df_wheat_winter, return_type='dataframe')\n",
    "    \n",
    "    print(f\"Successfully created response vector y with shape: {y.shape}\")\n",
    "    print(f\"Successfully created design matrix X with shape: {X.shape}\")\n",
    "    print(f\"The {X.shape[1]} columns in X include the intercept, year, 8 spline bases (4 for lat, 4 for lon), and {len(monthly_stressors)} climate stressors.\")\n",
    "\n",
    "    # --- 4. Standardize the Predictor Matrix (X) ---\n",
    "    # We scale ALL predictors to have a mean of 0 and a standard deviation of 1.\n",
    "    # This ensures the regularization penalty is applied fairly to all variables.\n",
    "    # We do NOT scale the response variable y.\n",
    "    print(\"\\nStandardizing the design matrix X...\")\n",
    "    \n",
    "    # We remove the Intercept column before scaling, as it should not be regularized or scaled.\n",
    "    # We will add it back later if needed, but scikit-learn's models handle it by default.\n",
    "    X_no_intercept = X.drop('Intercept', axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_values = scaler.fit_transform(X_no_intercept)\n",
    "    \n",
    "    # Convert the scaled array back to a pandas DataFrame with the original column names\n",
    "    X_scaled = pd.DataFrame(X_scaled_values, columns=X_no_intercept.columns, index=X.index)\n",
    "    \n",
    "    print(\"Standardization complete.\")\n",
    "    \n",
    "    # Verification: Check the mean and standard deviation of a few scaled columns\n",
    "    print(\"\\n--- Verification of Standardization ---\")\n",
    "    verification_cols = ['year', 'bs(lat, df=4)[0]', 'temperature_Jul']\n",
    "    for col in verification_cols:\n",
    "        mean_val = X_scaled[col].mean()\n",
    "        std_val = X_scaled[col].std()\n",
    "        print(f\"Column '{col}': Mean = {mean_val:.4f}, Std Dev = {std_val:.4f}\")\n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc267372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume 'y' and 'X_scaled' are already in memory from the previous step.\n",
    "# If not, you would need to re-run the data preparation script.\n",
    "\n",
    "try:\n",
    "    # --- 1. Define the GLM Model ---\n",
    "    # We specify our model family (Gamma) and the link function (log) as per our project plan.\n",
    "    # We pass the prepared y and the fully scaled X matrix.\n",
    "    # Note: statsmodels requires the intercept to be in the X matrix, which patsy provided.\n",
    "    \n",
    "    # We need to add the intercept back to the scaled data for statsmodels GLM\n",
    "    X_scaled_with_intercept = X.copy() # Start with the original X to preserve intercept and structure\n",
    "    X_scaled_with_intercept[X_no_intercept.columns] = X_scaled # Replace non-intercept columns with scaled versions\n",
    "\n",
    "    gl_gamma = sm.GLM(y, X_scaled_with_intercept, family=sm.families.Gamma(link=sm.families.links.log()))\n",
    "    print(\"Successfully initialized Gamma GLM with a log link.\")\n",
    "\n",
    "    # --- 2. Set up the Regularization Path ---\n",
    "    # We need to test a series of alpha values (penalty strengths).\n",
    "    # A logarithmic scale is best for this, from a weak penalty to a strong one.\n",
    "    n_alphas = 100\n",
    "    alphas = np.logspace(-3, 0.5, n_alphas) # From 0.001 to ~3.16\n",
    "\n",
    "    # The L1_wt parameter controls the Elastic Net mix (0=Ridge, 1=Lasso). \n",
    "    # 0.5 is a balanced choice.\n",
    "    elastic_net_l1_wt = 0.5 \n",
    "    \n",
    "    print(f\"Will fit the model for {n_alphas} alpha values with L1_wt (l1_ratio) = {elastic_net_l1_wt}\")\n",
    "\n",
    "    # --- 3. Fit the Model for Each Alpha and Store Coefficients ---\n",
    "    # We will loop through our alphas and save the coefficients from each model fit.\n",
    "    coefficients = []\n",
    "    \n",
    "    for alpha_val in alphas:\n",
    "        # The fit_regularized method performs the Elastic Net estimation.\n",
    "        # We set refit=False because we want to see the shrunken coefficients for this analysis.\n",
    "        results = gl_gamma.fit_regularized(\n",
    "            method='elastic_net', \n",
    "            alpha=alpha_val, \n",
    "            L1_wt=elastic_net_l1_wt,\n",
    "            refit=False \n",
    "        )\n",
    "        coefficients.append(results.params)\n",
    "    \n",
    "    # Convert the list of coefficient series into a DataFrame for easy plotting\n",
    "    coef_df = pd.DataFrame(coefficients, index=alphas)\n",
    "    coef_df.index.name = \"alpha\"\n",
    "    \n",
    "    # Exclude the Intercept for plotting, as it's not regularized and has a different scale.\n",
    "    coef_df_no_intercept = coef_df.drop('Intercept', axis=1)\n",
    "    \n",
    "    print(\"\\nCompleted fitting models along the regularization path.\")\n",
    "\n",
    "    # --- 4. Visualize the Regularization Path ---\n",
    "    print(\"Generating the regularization path plot...\")\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "    ax.plot(coef_df_no_intercept)\n",
    "    ax.set_xscale('log') # The alpha path is best viewed on a log scale\n",
    "    \n",
    "    # Add a vertical line at zero\n",
    "    ax.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    \n",
    "    ax.set_title('Regularization Path for sprwintering Wheat Model Coefficients (Elastic Net)', fontsize=18)\n",
    "    ax.set_xlabel('Alpha (Penalty Strength)', fontsize=14)\n",
    "    ax.set_ylabel('Standardized Coefficient Value', fontsize=14)\n",
    "    \n",
    "    # To avoid a cluttered legend, we don't add one here. The goal is to see the general pattern.\n",
    "    # Alternatively, for fewer variables, a legend could be useful:\n",
    "    # ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"ERROR: Make sure that 'y' and 'X_scaled' DataFrames from the previous step are available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e79f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Corrected code to identify the most robust variables ---\n",
    "# We will inspect the coefficients at a moderately high alpha value\n",
    "# This tells us which variables \"survived\" the penalty the longest.\n",
    "alpha_to_inspect = 0.03\n",
    "\n",
    "try:\n",
    "    # Find the alpha in our index that is closest to our target\n",
    "    # CORRECTED LINE: The operation works directly on the index without .flat\n",
    "    closest_alpha = coef_df.index[np.abs(coef_df.index - alpha_to_inspect).argmin()]\n",
    "\n",
    "    print(f\"--- Coefficients at alpha  {closest_alpha:.4f} ---\")\n",
    "\n",
    "    # Get the coefficients at this alpha and sort them by absolute value\n",
    "    robust_coeffs = coef_df.loc[closest_alpha].copy()\n",
    "    robust_coeffs_sorted = robust_coeffs.abs().sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nVariables sorted by the absolute magnitude of their shrunken coefficient:\")\n",
    "    # We display more variables to get a fuller picture\n",
    "    print(robust_coeffs_sorted.head(15))\n",
    "\n",
    "    # Let's also see their actual values (positive or negative) for the top variables\n",
    "    print(\"\\n--- Actual coefficient values for the most robust variables ---\")\n",
    "    print(coef_df.loc[closest_alpha, robust_coeffs_sorted.index].head(15))\n",
    "\n",
    "except NameError:\n",
    "     print(\"ERROR: Make sure that 'coef_df' DataFrame from the previous step is available in memory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa280721",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Base Champion Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_winter = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the base Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    base_formula = \"yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + temperature_May + temperature_Nov + temperature_Mar + potential_evaporation_Jan + potential_evaporation_Apr\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    base_model = smf.glm(\n",
    "        formula=base_formula,\n",
    "        data=df_wheat_winter,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    base_model_results = base_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(base_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Future Comparison ---\n",
    "    # The AIC is a key metric for comparing different model formulations. Lower is better.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Base Champion Model: {base_model_results.aic:.2f}\")\n",
    "    print(\"This will be our benchmark for comparison.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83716a6",
   "metadata": {},
   "source": [
    "# quadratic terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823212f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the Quadratic Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_winter = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the base Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    quadratic_formula = \"yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + temperature_May + I(temperature_May**2) + temperature_Nov + temperature_Mar + I(temperature_Mar**2) + potential_evaporation_Jan + potential_evaporation_Apr\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    quadratic_model = smf.glm(\n",
    "        formula=quadratic_formula,\n",
    "        data=df_wheat_winter,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    quadratic_model_results = quadratic_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(quadratic_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion's AIC (3516.86).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Quadratic Model: {quadratic_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the Base Model's AIC (2681.90).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60dd65",
   "metadata": {},
   "source": [
    "AIC when temp may is made quadratic: 2566.27\n",
    "\n",
    "AIC when temp now is also made quadratic: 2565.54\n",
    "\n",
    "AIC when temp now is also made quadratic: 2563.08\n",
    "\n",
    "The model for AIC = 2563.08:\n",
    "quadratic_formula = \"yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + temperature_May + I(temperature_May**2) + temperature_Nov + I(temperature_Nov**2) + temperature_Mar + I(temperature_Mar**2) + potential_evaporation_Jan + potential_evaporation_Apr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Fitting the interaction Model ---\")\n",
    "\n",
    "# --- 1. Load the Data ---\n",
    "# We use the original dataframe for this step.\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat_winter = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded dataset from: {file_path}\\n\")\n",
    "\n",
    "    # --- 2. Define and Fit the base Model ---\n",
    "    # This formula contains only the variables that proved robust in the regularization step.\n",
    "    # We use statsmodels.formula.api which simplifies fitting models from a formula string.\n",
    "    interaction_formula = \"yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + \" \\\n",
    "    \"temperature_May + I(temperature_May**2) + temperature_Nov + temperature_Mar + \" \\\n",
    "    \"I(temperature_Mar**2) + potential_evaporation_Jan + potential_evaporation_Apr +\" \\\n",
    "    \"temperature_Mar*temperature_May +\" \\\n",
    "    \"potential_evaporation_Jan*temperature_May\"\n",
    "\n",
    "    # Initialize the GLM model using the formula and the dataframe.\n",
    "    # Specify the Gamma family with a log link as planned.\n",
    "    interaction_model = smf.glm(\n",
    "        formula=interaction_formula,\n",
    "        data=df_wheat_winter,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    )\n",
    "\n",
    "    # Fit the model. This is the standard, un-penalized fit.\n",
    "    interaction_model_results = interaction_model.fit()\n",
    "\n",
    "    # --- 3. Print the Full Summary for Interpretation ---\n",
    "    # This summary is now statistically valid and is the basis for our interpretation.\n",
    "    print(\"--- Summary of the Base Champion Model ---\")\n",
    "    print(interaction_model_results.summary())\n",
    "\n",
    "    # --- 4. Print AIC for Comparison ---\n",
    "    # We will compare this AIC to our current champion's AIC (3516.86).\n",
    "    # A lower AIC will indicate that capturing the non-linear effect is an improvement.\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AIC for Quadratic Model: {interaction_model_results.aic:.2f}\")\n",
    "    print(\"Compare this to the quadratic best Model's AIC (2563.84).\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773f777",
   "metadata": {},
   "source": [
    "# visalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"--- Generating Final, Refined Yield Response Curves for Winter Wheat ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "df_wheat = pd.read_csv(file_path)\n",
    "df_wheat = df_wheat[df_wheat['yield_wheat_winter'] > 0].copy()\n",
    "print(\"Data prepared successfully.\")\n",
    "\n",
    "# --- 2. Fit Our Final Champion Model ---\n",
    "final_champion_formula = \"\"\"\n",
    "    yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                         temperature_May + I(temperature_May**2) + \n",
    "                         temperature_Nov + temperature_Mar + I(temperature_Mar**2) + \n",
    "                         potential_evaporation_Jan + potential_evaporation_Apr +\n",
    "                         temperature_Mar:temperature_May +\n",
    "                         potential_evaporation_Jan:temperature_May\n",
    "\"\"\"\n",
    "print(\"Fitting Final Champion model for Winter Wheat...\")\n",
    "final_model = smf.glm(\n",
    "    formula=final_champion_formula,\n",
    "    data=df_wheat,\n",
    "    family=sm.families.Gamma(link=sm.families.links.log())\n",
    ").fit()\n",
    "print(f\"Model fitted successfully. AIC: {final_model.aic:.2f}\")\n",
    "\n",
    "# --- 3. Prepare a Base Prediction Dictionary with Median Values ---\n",
    "median_values = {\n",
    "    'year': df_wheat['year'].median(),\n",
    "    'lat': df_wheat['lat'].median(),\n",
    "    'lon': df_wheat['lon'].median(),\n",
    "    'temperature_May': df_wheat['temperature_May'].median(),\n",
    "    'temperature_Nov': df_wheat['temperature_Nov'].median(),\n",
    "    'temperature_Mar': df_wheat['temperature_Mar'].median(),\n",
    "    'potential_evaporation_Jan': df_wheat['potential_evaporation_Jan'].median(),\n",
    "    'potential_evaporation_Apr': df_wheat['potential_evaporation_Apr'].median()\n",
    "}\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "# --- PLOT B (Now Plot 1): \"Money Plot\" #1 - Compounding Spring Heat Stress ---\n",
    "print('\\nGenerating Plot 1: The \"Compounding Spring Heat\" Interaction...')\n",
    "temp_mar_quantiles = df_wheat['temperature_Mar'].quantile([0.25, 0.5, 0.75])\n",
    "scenarios_mar = {\n",
    "    'Cold March (25th Pct)': {'value': temp_mar_quantiles[0.25], 'color': 'blue'},\n",
    "    'Average March (50th Pct)': {'value': temp_mar_quantiles[0.50], 'color': 'purple'},\n",
    "    'Warm March (75th Pct)': {'value': temp_mar_quantiles[0.75], 'color': 'red'}\n",
    "}\n",
    "\n",
    "temp_may_range = np.linspace(df_wheat['temperature_May'].min(), df_wheat['temperature_May'].max(), 100)\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "for scenario_name, props in scenarios_mar.items():\n",
    "    pred_df_interact = pd.DataFrame(median_values, index=range(100))\n",
    "    pred_df_interact['temperature_May'] = temp_may_range \n",
    "    pred_df_interact['temperature_Mar'] = props['value'] \n",
    "    \n",
    "    preds_interact = final_model.get_prediction(pred_df_interact).summary_frame(alpha=0.05)\n",
    "    \n",
    "    plt.plot(pred_df_interact['temperature_May'], preds_interact['mean'], color=props['color'], linewidth=3, label=scenario_name)\n",
    "    plt.fill_between(pred_df_interact['temperature_May'], preds_interact['mean_ci_lower'], preds_interact['mean_ci_upper'], color=props['color'], alpha=0.15)\n",
    "\n",
    "plt.title('Vulnerability to May Heat is Compounded by March Temperature', fontsize=18)\n",
    "plt.xlabel('Average May Temperature (C)', fontsize=14)\n",
    "plt.ylabel('Predicted Winter Wheat Yield (t/ha)', fontsize=14)\n",
    "plt.legend(title='March Temperature Scenario')\n",
    "# --- MODIFIED LINE ---\n",
    "plt.ylim(2, 14) # Zooming in on the y-axis\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- PLOT C (Now Plot 2): \"Money Plot\" #2 - Winter Conditions x Spring Stress ---\n",
    "print('\\nGenerating Plot 2: The \"Winter x Spring\" Interaction...')\n",
    "pe_jan_quantiles = df_wheat['potential_evaporation_Jan'].quantile([0.25, 0.5, 0.75])\n",
    "scenarios_jan = {\n",
    "    'Cloudy/Cold Winter (25th Pct PE)': {'value': pe_jan_quantiles[0.25], 'color': 'gray'},\n",
    "    'Average Winter (50th Pct PE)': {'value': pe_jan_quantiles[0.50], 'color': 'purple'},\n",
    "    'Sunny/Mild Winter (75th Pct PE)': {'value': pe_jan_quantiles[0.75], 'color': 'orange'}\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "for scenario_name, props in scenarios_jan.items():\n",
    "    pred_df_interact2 = pd.DataFrame(median_values, index=range(100))\n",
    "    pred_df_interact2['temperature_May'] = temp_may_range \n",
    "    pred_df_interact2['potential_evaporation_Jan'] = props['value'] \n",
    "    \n",
    "    preds_interact2 = final_model.get_prediction(pred_df_interact2).summary_frame(alpha=0.05)\n",
    "    \n",
    "    plt.plot(pred_df_interact2['temperature_May'], preds_interact2['mean'], color=props['color'], linewidth=3, label=scenario_name)\n",
    "    plt.fill_between(pred_df_interact2['temperature_May'], preds_interact2['mean_ci_lower'], preds_interact2['mean_ci_upper'], color=props['color'], alpha=0.15)\n",
    "\n",
    "plt.title('Vulnerability to May Heat is Influenced by Winter Conditions', fontsize=18)\n",
    "plt.xlabel('Average May Temperature (C)', fontsize=14)\n",
    "plt.ylabel('Predicted Winter Wheat Yield (t/ha)', fontsize=14)\n",
    "plt.legend(title='January PE Scenario')\n",
    "# --- MODIFIED LINE ---\n",
    "plt.ylim(2, 10) # Zooming in on the y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e5dfb6",
   "metadata": {},
   "source": [
    "### **Analysis of Winter Wheat Interaction Plots**\n",
    "\n",
    "These plots visualize the two critical interaction effects discovered by our final model. They reveal a complex story where the vulnerability of winter wheat to its primary stressorheat during the May flowering periodis significantly altered by the weather conditions from earlier in its life cycle.\n",
    "\n",
    "#### **Plot 1: Vulnerability to May Heat is Compounded by March Temperature**\n",
    "\n",
    "*   **Primary Finding:** A warm March \"pre-stresses\" the plant, making it significantly more vulnerable to subsequent heat stress in May.\n",
    "*   **Interpretation:** This plot perfectly visualizes the \"compounding heat stress\" interaction.\n",
    "    *   The **red line (\"Warm March\")** is the steepest. It shows that while a warm March can lead to very high yields if May is cool, it also leads to the most catastrophic yield decline as May gets hotter. This suggests that a warm March might cause the plant to break dormancy too early or grow too quickly, leaving it exposed and less resilient.\n",
    "    *   The **blue line (\"Cold March\")** is the flattest. This is the key insight: a plant that experiences a proper cold period in March is far more **resilient** to May heat. While its peak yield potential is lower, it suffers a much smaller penalty from a hot May, resulting in a more stable and predictable harvest.\n",
    "\n",
    "#### **Plot 2: Vulnerability to May Heat is Influenced by Winter Conditions**\n",
    "\n",
    "*   **Primary Finding:** The type of winter the crop endurescold and cloudy versus mild and sunnyfundamentally changes its response to spring heat stress.\n",
    "*   **Interpretation:**\n",
    "    *   The **gray line (\"Cloudy/Cold Winter\")** shows the highest peak yield potential. This is strong evidence that winter wheat requires a significant period of cold (vernalization) to maximize its reproductive potential. However, these plants also show a very steep decline in yield when faced with a hot May.\n",
    "    *   Conversely, the **orange line (\"Sunny/Mild Winter\")** shows the lowest yield potential across almost all conditions. This suggests that a mild winter fails to provide a sufficient vernalization signal, which limits the plant's yield ceiling from the very start. These plants are less productive overall but appear slightly less sensitive to extreme May heat because their yield potential was already compromised.\n",
    "\n",
    "#### **Overall Conclusion from Visuals**\n",
    "\n",
    "The key story for winter wheat is one of **sequential stress and resilience**. The highest yields are only possible when a \"proper\" cold winter is followed by a \"proper\" cool spring. A mild winter limits the plant's potential from the start. A warm spring, particularly a warm March, \"pre-stresses\" the plant, making it fragile and extremely vulnerable to the primary threat of a hot May during its critical flowering stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "print(\"--- Generating Final Vulnerability Curves for Winter Wheat ---\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "file_path = '../data-cherry-pick/wheat_winter_ITnorth_core41_1982_2016_allstressors_with_monthly.csv'\n",
    "\n",
    "try:\n",
    "    df_wheat = pd.read_csv(file_path)\n",
    "    df_wheat = df_wheat[df_wheat['yield_wheat_winter'] > 0].copy()\n",
    "    print(\"Data prepared successfully.\")\n",
    "\n",
    "    # --- 2. Fit Our Final Champion Model ---\n",
    "    final_champion_formula = \"\"\"\n",
    "        yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                             temperature_May + I(temperature_May**2) + \n",
    "                             temperature_Nov + temperature_Mar + I(temperature_Mar**2) + \n",
    "                             potential_evaporation_Jan + potential_evaporation_Apr +\n",
    "                             temperature_Mar:temperature_May +\n",
    "                             potential_evaporation_Jan:temperature_May\n",
    "    \"\"\"\n",
    "    final_model = smf.glm(\n",
    "        formula=final_champion_formula,\n",
    "        data=df_wheat,\n",
    "        family=sm.families.Gamma(link=sm.families.links.log())\n",
    "    ).fit()\n",
    "    print(f\"Final champion model for Winter Wheat fitted successfully. AIC: {final_model.aic:.2f}\\n\")\n",
    "\n",
    "    # --- 3. Define a Single, Consistent Baseline for All Plots ---\n",
    "    # Our \"typical year\" has median conditions for all key variables.\n",
    "    median_values = {\n",
    "        'year': df_wheat['year'].median(),\n",
    "        'lat': df_wheat['lat'].median(),\n",
    "        'lon': df_wheat['lon'].median(),\n",
    "        'temperature_May': df_wheat['temperature_May'].median(),\n",
    "        'temperature_Nov': df_wheat['temperature_Nov'].median(),\n",
    "        'temperature_Mar': df_wheat['temperature_Mar'].median(),\n",
    "        'potential_evaporation_Jan': df_wheat['potential_evaporation_Jan'].median(),\n",
    "        'potential_evaporation_Apr': df_wheat['potential_evaporation_Apr'].median()\n",
    "    }\n",
    "    X_baseline = pd.DataFrame(median_values, index=[0])\n",
    "    yield_baseline = final_model.get_prediction(X_baseline).summary_frame()['mean'].iloc[0]\n",
    "    print(f\"Predicted baseline yield for a typical case: {yield_baseline:.2f} t/ha\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # --- 4. Vulnerability Plot 1: Compounding Spring Heat Stress ---\n",
    "    print(\"\\nGenerating Vulnerability Plot 1: Compounding Spring Heat Interaction...\")\n",
    "    temp_mar_quantiles = df_wheat['temperature_Mar'].quantile([0.25, 0.5, 0.75])\n",
    "    scenarios_mar = {\n",
    "        'Cold March (25th Pct)': {'value': temp_mar_quantiles[0.25], 'color': 'blue'},\n",
    "        'Average March (50th Pct)': {'value': temp_mar_quantiles[0.50], 'color': 'purple'},\n",
    "        'Warm March (75th Pct)': {'value': temp_mar_quantiles[0.75], 'color': 'red'}\n",
    "    }\n",
    "    \n",
    "    temp_may_range = np.linspace(df_wheat['temperature_May'].min(), df_wheat['temperature_May'].max(), 100)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 9))\n",
    "    for scenario_name, props in scenarios_mar.items():\n",
    "        pred_df_scenario = pd.DataFrame(median_values, index=range(100))\n",
    "        pred_df_scenario['temperature_May'] = temp_may_range \n",
    "        pred_df_scenario['temperature_Mar'] = props['value']\n",
    "        \n",
    "        preds = final_model.get_prediction(pred_df_scenario).summary_frame(alpha=0.05)\n",
    "        yield_predicted = preds['mean']\n",
    "        yield_change_pct = ((yield_predicted - yield_baseline) / yield_baseline) * 100\n",
    "        \n",
    "        ax.plot(temp_may_range, yield_change_pct, color=props['color'], linewidth=3, label=scenario_name)\n",
    "\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_title('Vulnerability to May Heat is Compounded by March Temperature', fontsize=18)\n",
    "    ax.set_xlabel('Average May Temperature (C)', fontsize=14)\n",
    "    ax.set_ylabel('Yield Change vs. Typical Year (%)', fontsize=14)\n",
    "    ax.legend(title='March Temperature Scenario')\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "    plt.show()\n",
    "\n",
    "    # --- 5. Vulnerability Plot 2: Winter Conditions x Spring Stress ---\n",
    "    print('\\nGenerating Vulnerability Plot 2: The \"Winter x Spring\" Interaction...')\n",
    "    pe_jan_quantiles = df_wheat['potential_evaporation_Jan'].quantile([0.25, 0.5, 0.75])\n",
    "    scenarios_jan = {\n",
    "        'Cloudy/Cold Winter (25th Pct PE)': {'value': pe_jan_quantiles[0.25], 'color': 'gray'},\n",
    "        'Average Winter (50th Pct PE)': {'value': pe_jan_quantiles[0.50], 'color': 'purple'},\n",
    "        'Sunny/Mild Winter (75th Pct PE)': {'value': pe_jan_quantiles[0.75], 'color': 'orange'}\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 9))\n",
    "    for scenario_name, props in scenarios_jan.items():\n",
    "        pred_df_scenario2 = pd.DataFrame(median_values, index=range(100))\n",
    "        pred_df_scenario2['temperature_May'] = temp_may_range\n",
    "        pred_df_scenario2['potential_evaporation_Jan'] = props['value']\n",
    "        \n",
    "        preds2 = final_model.get_prediction(pred_df_scenario2).summary_frame(alpha=0.05)\n",
    "        yield_predicted2 = preds2['mean']\n",
    "        yield_change_pct2 = ((yield_predicted2 - yield_baseline) / yield_baseline) * 100\n",
    "        \n",
    "        ax.plot(temp_may_range, yield_change_pct2, color=props['color'], linewidth=3, label=scenario_name)\n",
    "\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_title('Vulnerability to May Heat is Influenced by Winter Conditions', fontsize=18)\n",
    "    ax.set_xlabel('Average May Temperature (C)', fontsize=14)\n",
    "    ax.set_ylabel('Yield Change vs. Typical Year (%)', fontsize=14)\n",
    "    ax.legend(title='January PE Scenario')\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93506a4",
   "metadata": {},
   "source": [
    "### **Analysis of the Final Winter Wheat Vulnerability Curves**\n",
    "\n",
    "These two vulnerability curves are the ultimate output of our winter wheat analysis. They translate the model's complex statistical interactions into clear, quantitative measures of compounding climate risk.\n",
    "\n",
    "#### **Plot 1: Vulnerability to May Heat is Compounded by March Temperature**\n",
    "\n",
    "*   **Primary Finding:** A warm March \"pre-stresses\" the plant, making it significantly more vulnerable to subsequent heat stress in May.\n",
    "*   **Interpretation:** This plot visualizes the powerful \"compounding heat stress\" interaction.\n",
    "    *   The **red line (\"Warm March\")** shows a high-risk, high-reward scenario. While it predicts the highest potential yield gains if May is very cool, it also shows the most catastrophic yield decline as May gets hotter, with losses exceeding **-20%** in a hot May. This suggests a warm March forces the plant into a fragile state.\n",
    "    *   The **blue line (\"Cold March\")** represents a more resilient plant. While its peak yield potential is lower, the curve is much flatter. This visually proves that a proper cold start to spring helps the plant withstand heat stress during the critical flowering period in May, resulting in more stable yields.\n",
    "\n",
    "#### **Plot 2: Vulnerability to May Heat is Influenced by Winter Conditions**\n",
    "\n",
    "*   **Primary Finding:** A \"proper\" cold and cloudy winter is essential for maximizing yield potential, but a mild, sunny winter leads to chronically lower yields.\n",
    "*   **Interpretation:** This plot reveals the critical role of winter conditions in setting the stage for the entire season.\n",
    "    *   The **gray line (\"Cloudy/Cold Winter\")** shows the highest yield potential, with gains of over **+125%** possible in a very cool May. This is strong evidence for the importance of **vernalization** (a cold period) for winter wheat's reproductive success.\n",
    "    *   Conversely, the **orange line (\"Sunny/Mild Winter\")** is the lowest curve across almost all conditions. This suggests that an insufficient cold signal during winter compromises the plant's yield potential from the very beginning, leading to consistently below-average outcomes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1badcdaa",
   "metadata": {},
   "source": [
    "### **Final Winter Wheat Model: Interpretation and Conclusions**\n",
    "\n",
    "This section summarizes the final champion model developed to explain the relationship between monthly climate stressors and winter wheat yield in Northern Italy. The model is the result of a multi-step workflow designed to be statistically robust, parsimonious, and highly insightful.\n",
    "\n",
    "#### **The Final Champion Model**\n",
    "\n",
    "After a data-driven process of variable selection and extensive iterative refinement, the final, best-performing model was determined to be a Gamma GLM with a complex structure of multiple non-linearities and interactions:\n",
    "\n",
    "**Final Model Formula:**\n",
    "```\n",
    "yield_wheat_winter ~ year + bs(lat, df=4) + bs(lon, df=4) + \n",
    "                     temperature_May + I(temperature_May**2) + \n",
    "                     temperature_Nov + temperature_Mar + I(temperature_Mar**2) + \n",
    "                     potential_evaporation_Jan + potential_evaporation_Apr +\n",
    "                     temperature_Mar:temperature_May +\n",
    "                     potential_evaporation_Jan:temperature_May\n",
    "```\n",
    "\n",
    "**Key Performance Metrics:**\n",
    "*   **Akaike Information Criterion (AIC):** `2476.85` (The lowest of all tested models)\n",
    "*   **Pseudo R-squared (CS):** `0.9101` (Explains approx. **91%** of the variation in yield)\n",
    "\n",
    "#### **The Modeling Journey: How We Arrived Here**\n",
    "\n",
    "The final model was the product of a systematic, evidence-based process:\n",
    "\n",
    "1.  **Variable Selection:** An **Elastic Net regularization** identified a broad set of robust predictors spanning the entire November-June growing season, suggesting a more complex set of drivers than for summer crops.\n",
    "\n",
    "2.  **Model Refinement (Parsimony):** A strategic base model was built, and non-significant spline components were correctly handled according to the Rule of Hierarchy.\n",
    "\n",
    "3.  **Testing for Non-Linearity:** Guided by strong evidence from our EDA, we systematically tested for non-linear effects. Adding quadratic terms for `temperature_May`, `temperature_Nov`, and `temperature_Mar` all resulted in **massive, successive drops in the AIC**, confirming that multiple, distinct \"optimal\" temperature conditions are critical for winter wheat.\n",
    "\n",
    "4.  **Testing for Interactions:** We tested our most plausible, theory-driven interaction hypotheses. This was the key to unlocking the model's full power. The data revealed two hugely significant interactions: **`temperature_Mar:temperature_May`** (compounding spring heat stress) and **`potential_evaporation_Jan:temperature_May`** (winter conditions interacting with spring stress).\n",
    "\n",
    "This structured process ensures our final model is not overfit and that its high complexity is justified by exceptionally strong statistical evidence.\n",
    "\n",
    "#### **Detailed Interpretation of the Final Model**\n",
    "\n",
    "*   **Control Variables:**\n",
    "    *   `year`: The positive, significant coefficient confirms a strong **technological trend**.\n",
    "    *   `bs(lat, df=4)` & `bs(lon, df=4)`: The high significance of the spatial splines confirms that **geography is a dominant driver** of yield.\n",
    "\n",
    "*   **Key Climate Drivers:**\n",
    "    *   **Multiple Non-Linear Temperature Effects:** The model successfully identified distinct non-linear responses to temperature in November, March, and May, indicating that the concept of an \"optimal\" temperature is critical at multiple, separate life-cycle stages.\n",
    "    *   **Interaction 1 (Compounding Spring Heat):** The `temperature_Mar:temperature_May` interaction is a core finding. It proves that heat stress is not simply additive; it's compounding. A warm March \"pre-stresses\" the crop, making it significantly more vulnerable to the damaging effects of a subsequent hot May.\n",
    "    *   **Interaction 2 (Winter's Legacy):** The `potential_evaporation_Jan:temperature_May` interaction reveals that the plant's response to spring heat is conditional on the winter it endured. A proper cold, cloudy winter (low PE) is essential for maximizing yield potential, likely due to vernalization.\n",
    "\n",
    "#### **Insights from Visualization**\n",
    "\n",
    "*   **The Interaction Plots:** The two multi-line yield response curves were essential for understanding the model. They visually confirmed that the impact of the primary stressor (May heat) is not a fixed curve but is dynamically altered by the conditions in both March and the preceding winter.\n",
    "\n",
    "*   **The Final Vulnerability Curves:** The vulnerability plots translate these complex interactions into quantitative risk. They show that:\n",
    "    *   **Resilience vs. Vulnerability:** A \"Cold March\" builds resilience, flattening the May vulnerability curve. A \"Warm March\" induces fragility, dramatically steepening the curve and increasing yield losses from May heat.\n",
    "    *   **The Importance of Vernalization:** A \"Cloudy/Cold Winter\" is the only scenario that allows for large potential yield gains, while a \"Sunny/Mild Winter\" leads to chronically below-average yields, confirming the crop's fundamental need for a proper cold period.\n",
    "\n",
    "#### **Overall Conclusion**\n",
    "\n",
    "The model provides a powerful and deeply nuanced explanation of winter wheat vulnerability. The dominant story is one of **sequential and compounding stress**. Unlike the summer crops, winter wheat's success is not tied to a single critical month but depends on a fragile sequence of conditions: a sufficiently cold winter for vernalization, followed by a mild but not-too-warm spring to build resilience. The model's core insight is that **deviations from this sequence create compounding vulnerability**, where an overly warm March can dramatically amplify the plant's susceptibility to the primary threat of a hot May during its critical flowering stage. This highlights a complex, time-dependent climate risk profile that is unique to winter wheat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climarisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
````
