{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65258f3-67b4-4f58-a2bd-ef185805eaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 42\n",
      "Unique (lat,lon) pairs: 42\n",
      "Missing values total: 0\n",
      "Inferred grid offsets: lat 0.25°, lon 0.25° (step=0.5°)\n",
      "Off-grid counts: lat 0, lon 0\n",
      "BBox: lat [43.75, 46.75], lon [6.75, 12.25]\n",
      "Saved canonical copies to: ..\\italy_core_data\\derived\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Point to your original file (read-only)\n",
    "CSV = Path(r\"../italy_core_data/n_italy_core_42_cells.csv\")\n",
    "\n",
    "# 2) Load and standardize column names in memory\n",
    "df0 = pd.read_csv(CSV)\n",
    "df = df0.copy()\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "rename = {}\n",
    "if \"latitude\" in df.columns and \"lat\" not in df.columns: rename[\"latitude\"] = \"lat\"\n",
    "if \"longitude\" in df.columns and \"lon\" not in df.columns: rename[\"longitude\"] = \"lon\"\n",
    "if \"long\" in df.columns and \"lon\" not in df.columns: rename[\"long\"] = \"lon\"\n",
    "df = df.rename(columns=rename)\n",
    "\n",
    "# Keep only lat/lon and ensure numeric\n",
    "assert {\"lat\",\"lon\"} <= set(df.columns), f\"Expected columns lat, lon. Got: {list(df.columns)}\"\n",
    "df = df[[\"lat\",\"lon\"]].copy()\n",
    "df[\"lat\"] = pd.to_numeric(df[\"lat\"], errors=\"coerce\")\n",
    "df[\"lon\"] = pd.to_numeric(df[\"lon\"], errors=\"coerce\")\n",
    "\n",
    "# 3) Validations (handles 0.25°-offset 0.5° grids)\n",
    "\n",
    "def infer_offset_deg(series, step=0.5):\n",
    "    \"\"\"Infer whether coords are aligned to n*step (+ 0) or n*step (+ step/2).\"\"\"\n",
    "    frac = (series / step) % 1.0  # in [0,1)\n",
    "    m = np.median(frac)\n",
    "    # decide between 0.00 and 0.50\n",
    "    return 0.0 if abs(m - 0.0) < 0.25 else step/2  # ~0.25 deg\n",
    "\n",
    "def off_grid_with_offset(series, offset_deg, step=0.5, tol=5e-4):\n",
    "    resid = ((series - offset_deg) / step) - np.round((series - offset_deg) / step)\n",
    "    return (np.abs(resid) > tol).sum()\n",
    "\n",
    "lat_off = infer_offset_deg(df[\"lat\"])\n",
    "lon_off = infer_offset_deg(df[\"lon\"])\n",
    "\n",
    "n_raw = len(df)\n",
    "n_nan = int(df.isna().sum().sum())\n",
    "n_unique = len(df.drop_duplicates([\"lat\",\"lon\"]))\n",
    "\n",
    "off_lat = off_grid_with_offset(df[\"lat\"], lat_off)\n",
    "off_lon = off_grid_with_offset(df[\"lon\"], lon_off)\n",
    "\n",
    "latmin, latmax = df[\"lat\"].min(), df[\"lat\"].max()\n",
    "lonmin, lonmax = df[\"lon\"].min(), df[\"lon\"].max()\n",
    "\n",
    "print(f\"Rows: {n_raw}\")\n",
    "print(f\"Unique (lat,lon) pairs: {n_unique}\")\n",
    "print(f\"Missing values total: {n_nan}\")\n",
    "print(f\"Inferred grid offsets: lat {lat_off:.2f}°, lon {lon_off:.2f}° (step=0.5°)\")\n",
    "print(f\"Off-grid counts: lat {off_lat}, lon {off_lon}\")\n",
    "print(f\"BBox: lat [{latmin}, {latmax}], lon [{lonmin}, {lonmax}]\")\n",
    "\n",
    "# Hard checks\n",
    "assert n_nan == 0, \"Found missing lat/lon values.\"\n",
    "assert n_unique == 42, f\"Expected 42 unique cells; found {n_unique}.\"\n",
    "assert off_lat == 0 and off_lon == 0, \"Some coordinates are not aligned to the inferred 0.5° grid.\"\n",
    "\n",
    "# 4) Save canonical copies (original CSV untouched)\n",
    "OUTDIR = CSV.parent / \"derived\"\n",
    "OUTDIR.mkdir(exist_ok=True)\n",
    "\n",
    "df_unique = (\n",
    "    df.drop_duplicates([\"lat\",\"lon\"])\n",
    "      .sort_values([\"lat\",\"lon\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# (a) Verbatim copy: exactly as in the source\n",
    "# df_unique.to_parquet(OUTDIR / \"mask_core_42.parquet\", index=False)\n",
    "df_unique.to_csv(OUTDIR / \"mask_core_42.csv\", index=False)\n",
    "\n",
    "# (b) Normalized copy: snapped to the inferred 0.5° grid (useful for exact joins later)\n",
    "def snap_to_grid(series, offset_deg, step=0.5):\n",
    "    return offset_deg + step * np.round((series - offset_deg) / step)\n",
    "\n",
    "df_norm = df_unique.copy()\n",
    "df_norm[\"lat\"] = snap_to_grid(df_norm[\"lat\"], lat_off)\n",
    "df_norm[\"lon\"] = snap_to_grid(df_norm[\"lon\"], lon_off)\n",
    "df_norm.to_csv(OUTDIR / \"mask_core_42_normalized.csv\", index=False)\n",
    "\n",
    "print(f\"Saved canonical copies to: {OUTDIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "043f1eb9-3031-48fd-aa9d-a7474f6cda6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDHY grid: lat n=360, lon n=720\n",
      "lat range [-89.75, 89.75]\n",
      "lon range [0.25, 359.75]\n",
      "Inferred: step lat=0.5°, lon=0.5°; offsets lat=0.25°, lon=0.25°\n",
      "Saved grid meta & coords to: ..\\italy_core_data\\derived\n",
      "Index mapping checks → lat mismatches: 0, lon mismatches: 0\n",
      "Wrote 42-cell index mapping: ..\\italy_core_data\\derived\\mask_core_42_on_gdhy.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# --- Update these two paths ---\n",
    "GDHY_DIR = Path(r\"..\\data\\maize\")  # folder with yield_YYYY.nc4\n",
    "CORE_DIR = Path(r\"..\\italy_core_data\")  # folder with your CSV\n",
    "# ------------------------------\n",
    "\n",
    "# choose a representative file (use 1982 since 1981 is excluded downstream)\n",
    "gdhyds = xr.open_dataset(GDHY_DIR / \"yield_1982.nc4\")\n",
    "\n",
    "# extract grid coordinates (names should be 'lat'/'lon' in GDHY)\n",
    "lat = gdhyds[\"lat\"].values\n",
    "lon = gdhyds[\"lon\"].values\n",
    "\n",
    "# Quick grid report\n",
    "print(f\"GDHY grid: lat n={lat.size}, lon n={lon.size}\")\n",
    "print(f\"lat range [{lat.min()}, {lat.max()}]\")\n",
    "print(f\"lon range [{lon.min()}, {lon.max()}]\")\n",
    "\n",
    "# detect 0.5° step & 0.25° (or 0.00°) offset\n",
    "def infer_step_and_offset(arr, guess_step=0.5):\n",
    "    step = float(np.round(np.median(np.diff(arr)), 3))\n",
    "    # infer offset relative to multiples of step\n",
    "    frac = (arr / guess_step) % 1.0\n",
    "    m = float(np.median(frac))\n",
    "    offset = 0.0 if abs(m - 0.0) < 0.25 else guess_step/2\n",
    "    return step, offset\n",
    "\n",
    "lat_step, lat_off = infer_step_and_offset(lat)\n",
    "lon_step, lon_off = infer_step_and_offset(lon)\n",
    "\n",
    "print(f\"Inferred: step lat={lat_step}°, lon={lon_step}°; offsets lat={lat_off}°, lon={lon_off}°\")\n",
    "\n",
    "# Save a tiny \"target grid\" artifact we’ll reuse later (coords only)\n",
    "DERIVED = CORE_DIR / \"derived\"\n",
    "DERIVED.mkdir(exist_ok=True)\n",
    "\n",
    "grid_meta = {\n",
    "    \"source\": \"GDHY maize\",\n",
    "    \"file_used\": str((GDHY_DIR / \"yield_1982.nc4\").resolve()),\n",
    "    \"lat_len\": int(lat.size),\n",
    "    \"lon_len\": int(lon.size),\n",
    "    \"lat_min\": float(lat.min()),\n",
    "    \"lat_max\": float(lat.max()),\n",
    "    \"lon_min\": float(lon.min()),\n",
    "    \"lon_max\": float(lon.max()),\n",
    "    \"step_lat\": lat_step,\n",
    "    \"step_lon\": lon_step,\n",
    "    \"offset_lat\": lat_off,\n",
    "    \"offset_lon\": lon_off,\n",
    "}\n",
    "with open(DERIVED / \"grid_gdhy_0p5.json\", \"w\") as f:\n",
    "    json.dump(grid_meta, f, indent=2)\n",
    "\n",
    "# also save a NetCDF with just the 1D coords (handy for regridders)\n",
    "xr.Dataset(coords={\"lat\": ([\"lat\"], lat), \"lon\": ([\"lon\"], lon)}).to_netcdf(DERIVED / \"grid_gdhy_0p5.nc\")\n",
    "\n",
    "print(f\"Saved grid meta & coords to: {DERIVED}\")\n",
    "\n",
    "# ---- Verify the 42 cells sit on this grid and get their indices ----\n",
    "mask42 = pd.read_csv(DERIVED / \"mask_core_42_normalized.csv\")  # from Step 0\n",
    "# helper to convert lat/lon to integer indices using the inferred step/offset\n",
    "def coord_to_index(val, arr, offset, step):\n",
    "    # compute expected index analytically, then clip to bounds\n",
    "    idx = int(round((val - offset) / step))\n",
    "    # safety: ensure it maps to the actual array position\n",
    "    idx = int(np.clip(idx, 0, len(arr)-1))\n",
    "    # final sanity: if exact match fails due to tiny float drift, snap to nearest\n",
    "    if not np.isclose(arr[idx], val, atol=5e-4):\n",
    "        idx = int(np.argmin(np.abs(arr - val)))\n",
    "    return idx\n",
    "\n",
    "mask42[\"ilat\"] = mask42[\"lat\"].apply(lambda v: coord_to_index(v, lat, lat_off, lat_step))\n",
    "mask42[\"ilon\"] = mask42[\"lon\"].apply(lambda v: coord_to_index(v, lon, lon_off, lon_step))\n",
    "\n",
    "# check round-trip accuracy\n",
    "lat_miss = (~np.isclose(lat[mask42[\"ilat\"]], mask42[\"lat\"], atol=5e-4)).sum()\n",
    "lon_miss = (~np.isclose(lon[mask42[\"ilon\"]], mask42[\"lon\"], atol=5e-4)).sum()\n",
    "\n",
    "print(f\"Index mapping checks → lat mismatches: {lat_miss}, lon mismatches: {lon_miss}\")\n",
    "assert lat_miss == 0 and lon_miss == 0, \"Some 42 cells do not map cleanly onto the GDHY grid.\"\n",
    "\n",
    "# save the index mapping for later quick subsetting (new file; originals untouched)\n",
    "mask42[[\"lat\",\"lon\",\"ilat\",\"ilon\"]].to_csv(DERIVED / \"mask_core_42_on_gdhy.csv\", index=False)\n",
    "print(f\"Wrote 42-cell index mapping: {DERIVED/'mask_core_42_on_gdhy.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aedec5c9-02a3-476f-ae10-3a5faeeff929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Update these two paths (same as you used in Step 1) ---\n",
    "GDHY_DIR = Path(r\"..\\data\\maize\")   # folder with yield_YYYY.nc4\n",
    "CORE_DIR = Path(r\"..\\italy_core_data\")\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "DERIVED = CORE_DIR / \"derived\"\n",
    "DERIVED.mkdir(exist_ok=True)\n",
    "\n",
    "# 1) Load the 42-cell mapping (created in Step 1). We’ll also sort for deterministic order.\n",
    "m42 = (\n",
    "    pd.read_csv(DERIVED / \"mask_core_42_on_gdhy.csv\")\n",
    "      .sort_values([\"lat\",\"lon\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "ilat = m42[\"ilat\"].to_numpy()\n",
    "ilon = m42[\"ilon\"].to_numpy()\n",
    "latc = m42[\"lat\"].to_numpy()\n",
    "lonc = m42[\"lon\"].to_numpy()\n",
    "\n",
    "# 2) Helper to open one year's GDHY file and return a 1x42 vector\n",
    "def load_yield_vector(year: int) -> np.ndarray:\n",
    "    f = GDHY_DIR / f\"yield_{year}.nc4\"\n",
    "    ds = xr.open_dataset(f)\n",
    "    # find the yield variable (often 'var')\n",
    "    varname = [v for v in ds.data_vars][0]\n",
    "    da = ds[varname].squeeze()  # -> (lat, lon) or (lat, lon) after dropping singleton time\n",
    "    # ensure we index in (lat, lon) order\n",
    "    if set((\"lat\",\"lon\")).issubset(da.dims):\n",
    "        da = da.transpose(\"lat\",\"lon\")\n",
    "    else:\n",
    "        raise ValueError(f\"{f} does not have expected lat/lon dims; got {da.dims}\")\n",
    "    arr = da.values  # shape [nlat, nlon]\n",
    "    # advanced indexing: pairs of (ilat[k], ilon[k]) -> (42,)\n",
    "    vals = arr[ilat, ilon]\n",
    "    return vals.astype(\"float32\")\n",
    "\n",
    "# 3) Loop years 1982–2016 (exclude 1981)\n",
    "years = list(range(1982, 2017))\n",
    "rows = []\n",
    "for y in years:\n",
    "    rows.append(load_yield_vector(y))\n",
    "data = np.vstack(rows)  # shape (35, 42)\n",
    "\n",
    "# 4) Build xarray object with coords and save\n",
    "yield_da = xr.DataArray(\n",
    "    data,\n",
    "    dims=(\"year\", \"cell\"),\n",
    "    coords={\n",
    "        \"year\": years,\n",
    "        \"cell\": np.arange(len(lonc)),\n",
    "        \"lat\": (\"cell\", latc),\n",
    "        \"lon\": (\"cell\", lonc),\n",
    "    },\n",
    "    name=\"yield_maize\",  # tonnes per hectare (per GDHY)\n",
    ")\n",
    "yield_ds = yield_da.to_dataset()\n",
    "\n",
    "# Write compact NetCDF (compressed)\n",
    "encoding = {\"yield_maize\": {\"zlib\": True, \"complevel\": 4}}\n",
    "out_nc = DERIVED / \"yield_maize_core42_1982_2016.nc\"\n",
    "yield_ds.to_netcdf(out_nc, encoding=encoding)\n",
    "\n",
    "# Also write a tidy CSV (rows = 42*35) for quick inspection\n",
    "out_csv = DERIVED / \"yield_maize_core42_1982_2016.csv\"\n",
    "\n",
    "# Build a tidy/long table; xarray includes 'lat' and 'lon' already\n",
    "df_long = yield_da.to_dataframe(name=\"yield_maize\").reset_index()\n",
    "\n",
    "# Keep columns and sort\n",
    "df_long = df_long[[\"lat\", \"lon\", \"year\", \"yield_maize\"]].sort_values([\"lat\", \"lon\", \"year\"])\n",
    "\n",
    "df_long.to_csv(out_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aad7dc75-453d-4268-8f9a-8825b4913c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: t2m_MJJAS_core42_1982_2016.nc, t2m_MJJAS_core42_1982_2016.csv  →  ..\\italy_core_data\\derived\n",
      "Shape: years=35 (expect 35), cells=42 (expect 42)\n",
      "NANs: 0\n",
      "Year range: 1982–2016\n",
      "Lat range: 43.75–46.75 | Lon range: 6.75–12.25\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- paths (edit ERA5_DIR to your location) ----\n",
    "ERA5_DIR = Path(r\"..\\data\\climate_monthly_full\")        # <-- folder with your .grib files\n",
    "CORE_DIR = Path(r\"..\\italy_core_data\")          # same as before\n",
    "DERIVED  = CORE_DIR / \"derived\"\n",
    "DERIVED.mkdir(exist_ok=True)\n",
    "\n",
    "# ---- load artifacts from prior steps ----\n",
    "m42 = pd.read_csv(DERIVED / \"mask_core_42_on_gdhy.csv\").sort_values([\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "ilat, ilon = m42[\"ilat\"].to_numpy(), m42[\"ilon\"].to_numpy()\n",
    "latc, lonc = m42[\"lat\"].to_numpy(), m42[\"lon\"].to_numpy()\n",
    "\n",
    "grid_ds = xr.open_dataset(DERIVED / \"grid_gdhy_0p5.nc\")  # has 1D lat/lon for GDHY\n",
    "lat_g, lon_g = grid_ds[\"lat\"].values, grid_ds[\"lon\"].values\n",
    "\n",
    "# ---- helpers ----\n",
    "def find_year_file(year: int, folder: Path) -> Path:\n",
    "    cand = folder / f\"era5_land_{year}.grib\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "    # fallback: any file that contains the year in the name\n",
    "    matches = list(folder.glob(f\"*{year}*.grib\"))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No GRIB for year {year} in {folder}\")\n",
    "    return matches[0]\n",
    "\n",
    "def open_era5_t2m_year(year: int) -> xr.DataArray:\n",
    "    f = find_year_file(year, ERA5_DIR)\n",
    "    ds = xr.open_dataset(\n",
    "        f,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs=dict(indexpath=\"\", filter_by_keys={\"shortName\": \"2t\"}),\n",
    "        decode_timedelta=True,  # <- silences the FutureWarning\n",
    "    )\n",
    "    ds = ds.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "    if ds.lat[0] > ds.lat[-1]:\n",
    "        ds = ds.sortby(\"lat\")\n",
    "    if ds.lon.min() < 0:\n",
    "        ds = ds.assign_coords(lon=((ds.lon % 360 + 360) % 360)).sortby(\"lon\")\n",
    "    da = ds[\"t2m\"] - 273.15  # K -> °C\n",
    "    return da\n",
    "\n",
    "def bin_to_half_degree(da: xr.DataArray, step=0.5, offset=0.25) -> xr.DataArray:\n",
    "    # Compute target bin centers as plain numpy arrays (not DataArrays)\n",
    "    lat_bins = (offset + step * np.round((da[\"lat\"].values - offset) / step)).astype(np.float64)\n",
    "    lon_bins = (offset + step * np.round((da[\"lon\"].values - offset) / step)).astype(np.float64)\n",
    "\n",
    "    # Attach as coordinates aligned to the respective dimensions\n",
    "    da = da.assign_coords(lat_bin=(\"lat\", lat_bins), lon_bin=(\"lon\", lon_bins))\n",
    "\n",
    "    # Block-average by those bins\n",
    "    da_c = da.groupby(\"lat_bin\").mean(\"lat\").groupby(\"lon_bin\").mean(\"lon\")\n",
    "    da_c = da_c.rename({\"lat_bin\": \"lat\", \"lon_bin\": \"lon\"})\n",
    "    return da_c\n",
    "\n",
    "def seasonal_mean_mjjas(da_monthly_05: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"Select May–Sep and mean over months (per year).\"\"\"\n",
    "    sel = da_monthly_05.where(da_monthly_05[\"time\"].dt.month.isin([5,6,7,8,9]), drop=True)\n",
    "    out = sel.groupby(\"time.year\").mean(\"time\")\n",
    "    return out.rename({\"year\": \"year\"})\n",
    "\n",
    "# ---- process per year to keep memory low ----\n",
    "years = list(range(1982, 2016 + 1))  # exclude 1981\n",
    "pieces = []\n",
    "for y in years:\n",
    "    # 1) open monthly t2m for the year\n",
    "    da_m = open_era5_t2m_year(y)  # dims: time, lat, lon (0.1°)\n",
    "\n",
    "    # 2) regrid monthly to 0.5° block means\n",
    "    da_m05 = bin_to_half_degree(da_m)  # dims: time, lat(0.5), lon(0.5)\n",
    "\n",
    "    # 3) seasonal (May–Sep) mean for that year\n",
    "    da_y = seasonal_mean_mjjas(da_m05)  # dims: year(=1), lat, lon\n",
    "\n",
    "    # 4) align lat/lon exactly to GDHY grid (nearest) so indices match\n",
    "    da_y = da_y.sel(lat=lat_g, lon=lon_g, method=\"nearest\")\n",
    "\n",
    "    pieces.append(da_y)\n",
    "\n",
    "# ---- stack into one (year, lat, lon) cube on the GDHY grid ----\n",
    "t2m_mjjas = xr.concat(pieces, dim=\"year\")\n",
    "t2m_mjjas = t2m_mjjas.assign_coords(year=(\"year\", years))\n",
    "t2m_mjjas.name = \"t2m_MJJAS_C\"\n",
    "\n",
    "# ---- subset to the 42 cells by GDHY indices (vectorized) ----\n",
    "t2m_42 = t2m_mjjas.isel(\n",
    "    lat=xr.DataArray(ilat, dims=\"cell\"),\n",
    "    lon=xr.DataArray(ilon, dims=\"cell\"),\n",
    ")\n",
    "# add lat/lon as coords on 'cell'\n",
    "t2m_42 = t2m_42.assign_coords(cell=(\"cell\", np.arange(len(ilat))))\n",
    "t2m_42 = t2m_42.assign_coords(lat=(\"cell\", latc), lon=(\"cell\", lonc))\n",
    "\n",
    "# ---- save outputs ----\n",
    "out_nc = DERIVED / \"t2m_MJJAS_core42_1982_2016.nc\"\n",
    "enc = {\"t2m_MJJAS_C\": {\"zlib\": True, \"complevel\": 4}}\n",
    "t2m_42.to_dataset(name=\"t2m_MJJAS_C\").to_netcdf(out_nc, encoding=enc)\n",
    "\n",
    "out_csv = DERIVED / \"t2m_MJJAS_core42_1982_2016.csv\"\n",
    "df_t = t2m_42.to_dataframe(name=\"temperature\").reset_index()\n",
    "df_t = df_t[[\"lat\",\"lon\",\"year\",\"temperature\"]].sort_values([\"lat\",\"lon\",\"year\"])\n",
    "df_t.to_csv(out_csv, index=False)\n",
    "\n",
    "# ---- QA ----\n",
    "print(f\"Saved: {out_nc.name}, {out_csv.name}  →  {DERIVED}\")\n",
    "print(f\"Shape: years={t2m_42.sizes['year']} (expect 35), cells={t2m_42.sizes['cell']} (expect 42)\")\n",
    "print(f\"NANs: {int(np.isnan(t2m_42.values).sum())}\")\n",
    "print(f\"Year range: {t2m_42.year.values.min()}–{t2m_42.year.values.max()}\")\n",
    "print(f\"Lat range: {t2m_42.lat.values.min()}–{t2m_42.lat.values.max()} | Lon range: {t2m_42.lon.values.min()}–{t2m_42.lon.values.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e238e69-388a-4c3f-878d-ae062757aa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yield: rows=1470, unique_keys=1470, NaNs=0\n",
      "Temperature: rows=1470, unique_keys=1470, NaNs=0\n",
      "Merged rows: 1470 (expected 1470)\n",
      "Saved combined dataset → ..\\italy_core_data\\derived\\maize_ITnorth_core42_1982_2016.csv\n",
      "Columns: ['lat', 'lon', 'year', 'yield_maize', 'temperature']\n",
      "     lat    lon  year  yield_maize  temperature\n",
      "0  43.75  11.75  1982     9.251022    18.613775\n",
      "1  43.75  11.75  1983     9.876991    18.254950\n",
      "2  43.75  11.75  1984    10.552830    16.382141\n",
      "3  43.75  11.75  1985    10.486564    18.763770\n",
      "4  43.75  11.75  1986    12.218849    18.099987\n",
      "5  43.75  11.75  1987    11.250535    18.253310\n",
      "6  43.75  11.75  1988    11.657367    18.235193\n",
      "7  43.75  11.75  1989    10.706320    17.116800\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- paths ---\n",
    "DERIVED = Path(r\"..\\italy_core_data\\derived\")\n",
    "yield_csv = DERIVED / \"yield_maize_core42_1982_2016.csv\"\n",
    "temp_csv  = DERIVED / \"t2m_MJJAS_core42_1982_2016.csv\"\n",
    "out_csv   = DERIVED / \"maize_ITnorth_core42_1982_2016.csv\"\n",
    "\n",
    "# --- load ---\n",
    "y = pd.read_csv(yield_csv)   # cols: lat, lon, year, yield_maize\n",
    "t = pd.read_csv(temp_csv)    # cols: lat, lon, year, temperature\n",
    "\n",
    "# --- normalize dtypes ---\n",
    "for df, name in [(y,\"yield\"), (t,\"temp\")]:\n",
    "    # ensure keys exist\n",
    "    assert {\"lat\",\"lon\",\"year\"}.issubset(df.columns), f\"{name}: missing join keys\"\n",
    "    # consistent dtypes\n",
    "    df[\"year\"] = df[\"year\"].astype(int)\n",
    "    df[\"lat\"]  = df[\"lat\"].astype(float)\n",
    "    df[\"lon\"]  = df[\"lon\"].astype(float)\n",
    "\n",
    "# --- quick sanity on each input ---\n",
    "def quick_report(df, label):\n",
    "    n = len(df)\n",
    "    nunique = df[[\"lat\",\"lon\",\"year\"]].drop_duplicates().shape[0]\n",
    "    n_nans = int(df.isna().sum().sum())\n",
    "    print(f\"{label}: rows={n}, unique_keys={nunique}, NaNs={n_nans}\")\n",
    "    assert n == 42*35, f\"{label}: expected 1470 rows, got {n}\"\n",
    "    assert nunique == n, f\"{label}: duplicate (lat,lon,year) rows found\"\n",
    "    assert n_nans == 0, f\"{label}: contains missing values\"\n",
    "\n",
    "quick_report(y, \"Yield\")\n",
    "quick_report(t, \"Temperature\")\n",
    "\n",
    "# --- merge on exact keys ---\n",
    "df = y.merge(t, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "\n",
    "# --- final checks ---\n",
    "print(f\"Merged rows: {len(df)} (expected 1470)\")\n",
    "assert len(df) == 42*35, \"Merge did not produce 1470 rows—check key alignment\"\n",
    "assert int(df.isna().sum().sum()) == 0, \"Merged table has missing values\"\n",
    "\n",
    "# order & save\n",
    "df = df[[\"lat\",\"lon\",\"year\",\"yield_maize\",\"temperature\"]].sort_values([\"lat\",\"lon\",\"year\"]).reset_index(drop=True)\n",
    "df.to_csv(out_csv, index=False)\n",
    "\n",
    "print(f\"Saved combined dataset → {out_csv}\")\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(df.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "525042fc-9313-4c2b-b7ab-1c641600761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precipitation: processed 1982\n",
      "precipitation: processed 1987\n",
      "precipitation: processed 1992\n",
      "precipitation: processed 1997\n",
      "precipitation: processed 2002\n",
      "precipitation: processed 2007\n",
      "precipitation: processed 2012\n",
      "Saved precipitation: precipitation_MJJAS_core42_1982_2016.csv  (rows=1470)\n",
      "soil_water: processed 1982\n",
      "soil_water: processed 1987\n",
      "soil_water: processed 1992\n",
      "soil_water: processed 1997\n",
      "soil_water: processed 2002\n",
      "soil_water: processed 2007\n",
      "soil_water: processed 2012\n",
      "Saved soil_water: soil_water_MJJAS_core42_1982_2016.csv  (rows=1470)\n",
      "solar_radiation: processed 1982\n",
      "solar_radiation: processed 1987\n",
      "solar_radiation: processed 1992\n",
      "solar_radiation: processed 1997\n",
      "solar_radiation: processed 2002\n",
      "solar_radiation: processed 2007\n",
      "solar_radiation: processed 2012\n",
      "Saved solar_radiation: solar_radiation_MJJAS_core42_1982_2016.csv  (rows=1470)\n",
      "potential_evaporation: processed 1982\n",
      "potential_evaporation: processed 1987\n",
      "potential_evaporation: processed 1992\n",
      "potential_evaporation: processed 1997\n",
      "potential_evaporation: processed 2002\n",
      "potential_evaporation: processed 2007\n",
      "potential_evaporation: processed 2012\n",
      "Saved potential_evaporation: potential_evaporation_MJJAS_core42_1982_2016.csv  (rows=1470)\n",
      "\n",
      "Final table saved → ..\\italy_core_data\\derived\\maize_ITnorth_core42_1982_2016_allstressors.csv\n",
      "Columns: ['lat', 'lon', 'year', 'yield_maize', 'temperature', 'precipitation', 'soil_water', 'solar_radiation', 'potential_evaporation']\n",
      "     lat    lon  year  yield_maize  temperature  precipitation  soil_water  \\\n",
      "0  43.75  11.75  1982     9.251022    18.613775     365.897682    0.292107   \n",
      "1  43.75  11.75  1983     9.876991    18.254950     327.526063    0.283785   \n",
      "2  43.75  11.75  1984    10.552830    16.382141     551.174189    0.320709   \n",
      "3  43.75  11.75  1985    10.486564    18.763770     229.241146    0.253280   \n",
      "4  43.75  11.75  1986    12.218849    18.099987     375.639853    0.302944   \n",
      "5  43.75  11.75  1987    11.250535    18.253310     331.152387    0.285093   \n",
      "6  43.75  11.75  1988    11.657367    18.235193     317.863780    0.281345   \n",
      "7  43.75  11.75  1989    10.706320    17.116800     529.639760    0.337776   \n",
      "\n",
      "   solar_radiation  potential_evaporation  \n",
      "0     2.624705e+09             964.837317  \n",
      "1     2.620409e+09             996.756408  \n",
      "2     2.514478e+09             910.182317  \n",
      "3     2.678356e+09            1056.200879  \n",
      "4     2.683026e+09             984.848034  \n",
      "5     2.697742e+09            1032.086607  \n",
      "6     2.649595e+09            1007.750931  \n",
      "7     2.608387e+09             912.023481  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------ paths (edit if needed) ------------\n",
    "ERA5_DIR = Path(r\"..\\data\\climate_monthly_full\")   # contains era5_land_monthly_YYYY.grib\n",
    "CORE_DIR = Path(r\"..\\italy_core_data\")\n",
    "DERIVED  = CORE_DIR / \"derived\"\n",
    "DERIVED.mkdir(exist_ok=True)\n",
    "\n",
    "base_csv = DERIVED / \"maize_ITnorth_core42_1982_2016.csv\"  # yield + temperature (from Step 4)\n",
    "\n",
    "# ------------ prior artifacts ------------\n",
    "m42 = pd.read_csv(DERIVED / \"mask_core_42_on_gdhy.csv\").sort_values([\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "latc, lonc = m42[\"lat\"].to_numpy(), m42[\"lon\"].to_numpy()\n",
    "\n",
    "# Small safety margin so binning catches the edge cells\n",
    "BBOX = dict(\n",
    "    lat_min=float(latc.min() - 0.5),\n",
    "    lat_max=float(latc.max() + 0.5),\n",
    "    lon_min=float(lonc.min() - 0.5),\n",
    "    lon_max=float(lonc.max() + 0.5),\n",
    ")\n",
    "\n",
    "years = list(range(1982, 2016 + 1))\n",
    "season_months = [5,6,7,8,9]  # MJJAS\n",
    "\n",
    "# ------------ helpers ------------\n",
    "def find_year_file(year: int) -> Path:\n",
    "    cand = ERA5_DIR / f\"era5_land_monthly_{year}.grib\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "    matches = list(ERA5_DIR.glob(f\"*{year}*.grib\"))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No GRIB for year {year} in {ERA5_DIR}\")\n",
    "    return matches[0]\n",
    "\n",
    "def open_era5_var_year(year: int, short: str, varname: str, bbox: dict) -> xr.DataArray:\n",
    "    \"\"\"Open one ERA5-Land variable for a year, standardize coords, crop to Italy bbox, return DA.\"\"\"\n",
    "    f = find_year_file(year)\n",
    "    ds = xr.open_dataset(\n",
    "        f,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs=dict(indexpath=\"\", filter_by_keys={\"shortName\": short}),\n",
    "        decode_timedelta=True,\n",
    "    ).rename({\"latitude\":\"lat\",\"longitude\":\"lon\"})\n",
    "\n",
    "    # Ensure ascending latitude and 0..360 longitudes\n",
    "    if ds.lat[0] > ds.lat[-1]:\n",
    "        ds = ds.sortby(\"lat\")\n",
    "    if ds.lon.min() < 0:\n",
    "        ds = ds.assign_coords(lon=((ds.lon % 360 + 360) % 360)).sortby(\"lon\")\n",
    "\n",
    "    # Crop to Italy bbox BEFORE regridding (massive speed-up)\n",
    "    ds = ds.sel(lat=slice(bbox[\"lat_min\"], bbox[\"lat_max\"]),\n",
    "                lon=slice(bbox[\"lon_min\"], bbox[\"lon_max\"]))\n",
    "\n",
    "    da = ds[varname]\n",
    "    if \"expver\" in da.dims:\n",
    "        da = da.isel(expver=-1)  # pick analysis member if present\n",
    "\n",
    "    return da  # dims: time, lat, lon (regional)\n",
    "\n",
    "def bin_to_half_degree(da: xr.DataArray, step=0.5, offset=0.25) -> xr.DataArray:\n",
    "    \"\"\"Block-average 0.1° to 0.5° by labeling to nearest 0.25 + 0.5*k centers, then mean.\"\"\"\n",
    "    lat_bins = (offset + step * np.round((da[\"lat\"].values - offset) / step)).astype(np.float64)\n",
    "    lon_bins = (offset + step * np.round((da[\"lon\"].values - offset) / step)).astype(np.float64)\n",
    "    da = da.assign_coords(lat_bin=(\"lat\", lat_bins), lon_bin=(\"lon\", lon_bins))\n",
    "    da_c = da.groupby(\"lat_bin\").mean(\"lat\").groupby(\"lon_bin\").mean(\"lon\")\n",
    "    return da_c.rename({\"lat_bin\":\"lat\",\"lon_bin\":\"lon\"})  # dims: time, lat(0.5), lon(0.5)\n",
    "\n",
    "def seasonal_mean(da_m05: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"Mean over MJJAS months (for state variables).\"\"\"\n",
    "    sel = da_m05.where(da_m05[\"time\"].dt.month.isin(season_months), drop=True)\n",
    "    return sel.groupby(\"time.year\").mean(\"time\").rename({\"year\":\"year\"})\n",
    "\n",
    "def seasonal_total_from_daily_means(da_m05: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    ERA5 Monthly Averaged fluxes are DAILY means.\n",
    "    Convert to monthly totals by multiplying by days_in_month, then sum MJJAS.\n",
    "    \"\"\"\n",
    "    sel = da_m05.where(da_m05[\"time\"].dt.month.isin(season_months), drop=True)\n",
    "    days = xr.DataArray(\n",
    "        sel[\"time\"].dt.days_in_month,\n",
    "        coords={\"time\": sel[\"time\"]},\n",
    "        dims=[\"time\"]\n",
    "    ).astype(np.float64)\n",
    "    return (sel * days).groupby(\"time.year\").sum(\"time\").rename({\"year\":\"year\"})\n",
    "\n",
    "def process_var(short: str, xr_name: str, out_col: str, how: str, unit_conv=None, post_conv=None):\n",
    "    \"\"\"\n",
    "    short/xr_name : GRIB key & xarray variable name (often the same)\n",
    "    out_col       : output column name\n",
    "    how           : 'mean' (state) or 'sum' (flux) over season\n",
    "    unit_conv     : function applied to the **monthly** field (e.g., K->°C, m->mm, sign flips)\n",
    "    post_conv     : function applied after seasonal reduce (e.g., convert J -> MJ)\n",
    "    \"\"\"\n",
    "    pieces = []\n",
    "    for y in years:\n",
    "        da = open_era5_var_year(y, short, xr_name, BBOX)   # regional 0.1°\n",
    "        if unit_conv is not None:\n",
    "            da = unit_conv(da)                             # per-month conversion\n",
    "        da05 = bin_to_half_degree(da)                      # regional 0.5°\n",
    "\n",
    "        if how == \"mean\":\n",
    "            ya = seasonal_mean(da05)                       # (year, lat, lon)\n",
    "        elif how == \"sum\":\n",
    "            ya = seasonal_total_from_daily_means(da05)    # (year, lat, lon)\n",
    "        else:\n",
    "            raise ValueError(\"how must be 'mean' or 'sum'\")\n",
    "\n",
    "        # Select the exact same 42 cells by value (no global expansion)\n",
    "        ya42 = ya.sel(lat=xr.DataArray(latc, dims=\"cell\"),\n",
    "                      lon=xr.DataArray(lonc, dims=\"cell\"),\n",
    "                      method=\"nearest\")\n",
    "        ya42 = ya42.assign_coords(cell=(\"cell\", np.arange(len(latc))),\n",
    "                                  lat=(\"cell\", latc), lon=(\"cell\", lonc))\n",
    "        pieces.append(ya42)\n",
    "\n",
    "        if (y - years[0]) % 5 == 0:\n",
    "            print(f\"{out_col}: processed {y}\", flush=True)\n",
    "\n",
    "    da_all = xr.concat(pieces, dim=\"year\").assign_coords(year=(\"year\", years))\n",
    "\n",
    "    if post_conv is not None:\n",
    "        da_all = post_conv(da_all)\n",
    "\n",
    "    out_nc  = DERIVED / f\"{out_col}_MJJAS_core42_1982_2016.nc\"\n",
    "    out_csv = DERIVED / f\"{out_col}_MJJAS_core42_1982_2016.csv\"\n",
    "    da_all.to_dataset(name=xr_name).to_netcdf(out_nc, encoding={xr_name: {\"zlib\": True, \"complevel\": 4}})\n",
    "    df = da_all.to_dataframe(name=out_col).reset_index()[[\"lat\",\"lon\",\"year\",out_col]]\n",
    "    df = df.sort_values([\"lat\",\"lon\",\"year\"])\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved {out_col}: {out_csv.name}  (rows={len(df)})\")\n",
    "    return df\n",
    "\n",
    "# ------------ recompute all stressors with correct handling ------------\n",
    "# precipitation: m -> mm (per month), then hours-weighted seasonal total\n",
    "df_tp   = process_var(short=\"tp\",    xr_name=\"tp\",    out_col=\"precipitation\",\n",
    "                      how=\"sum\",\n",
    "                      unit_conv=lambda da: da * 1000.0,   # mm per hour-mean\n",
    "                      post_conv=None)\n",
    "\n",
    "# soil water layer 1: mean MJJAS (m3/m3), no hours weighting\n",
    "df_swvl = process_var(short=\"swvl1\", xr_name=\"swvl1\", out_col=\"soil_water\",\n",
    "                      how=\"mean\",\n",
    "                      unit_conv=None,\n",
    "                      post_conv=None)\n",
    "\n",
    "# solar radiation: J/m² per hour-mean -> hours-weighted seasonal total\n",
    "# (optional: convert to MJ/m² for readability with post_conv=lambda da: da/1e6)\n",
    "df_ssr  = process_var(short=\"ssr\",   xr_name=\"ssr\",   out_col=\"solar_radiation\",\n",
    "                      how=\"sum\",\n",
    "                      unit_conv=None,\n",
    "                      post_conv=None)  # or post_conv=lambda da: da/1e6\n",
    "\n",
    "# potential evaporation: m (negative) -> **positive mm**, then hours-weighted seasonal total\n",
    "df_pev  = process_var(short=\"pev\",   xr_name=\"pev\",   out_col=\"potential_evaporation\",\n",
    "                      how=\"sum\",\n",
    "                      unit_conv=lambda da: -da * 1000.0,  # flip sign & mm per hour-mean\n",
    "                      post_conv=None)\n",
    "\n",
    "# ------------ merge with existing yield+temperature ------------\n",
    "base = pd.read_csv(base_csv)  # lat, lon, year, yield_maize, temperature\n",
    "df = (\n",
    "    base.merge(df_tp,   on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_swvl, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_ssr,  on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_pev,  on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .sort_values([\"lat\",\"lon\",\"year\"])\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Final checks and save\n",
    "assert len(df) == 42*35, f\"Expected 1470 rows, got {len(df)}\"\n",
    "assert int(df.isna().sum().sum()) == 0, \"Merged table has NaNs\"\n",
    "\n",
    "final_csv = DERIVED / \"maize_ITnorth_core42_1982_2016_allstressors.csv\"\n",
    "df.to_csv(final_csv, index=False)\n",
    "\n",
    "print(\"\\nFinal table saved →\", final_csv)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(df.head(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2cafc8-5e00-41a6-9e7b-96dcd1d3244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature: processed 1982\n",
      "temperature: processed 1987\n",
      "temperature: processed 1992\n",
      "temperature: processed 1997\n",
      "temperature: processed 2002\n",
      "temperature: processed 2007\n",
      "temperature: processed 2012\n",
      "soil_water: processed 1982\n",
      "soil_water: processed 1987\n",
      "soil_water: processed 1992\n",
      "soil_water: processed 1997\n",
      "soil_water: processed 2002\n",
      "soil_water: processed 2007\n",
      "soil_water: processed 2012\n",
      "precipitation: processed 1982\n",
      "precipitation: processed 1987\n",
      "precipitation: processed 1992\n",
      "precipitation: processed 1997\n",
      "precipitation: processed 2002\n",
      "precipitation: processed 2007\n",
      "precipitation: processed 2012\n",
      "solar_radiation: processed 1982\n",
      "solar_radiation: processed 1987\n",
      "solar_radiation: processed 1992\n",
      "solar_radiation: processed 1997\n",
      "solar_radiation: processed 2002\n",
      "solar_radiation: processed 2007\n",
      "solar_radiation: processed 2012\n",
      "potential_evaporation: processed 1982\n",
      "potential_evaporation: processed 1987\n",
      "potential_evaporation: processed 1992\n",
      "potential_evaporation: processed 1997\n",
      "potential_evaporation: processed 2002\n",
      "potential_evaporation: processed 2007\n",
      "potential_evaporation: processed 2012\n",
      "Saved enriched dataset with monthly columns → ..\\italy_core_data\\derived\\maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv\n",
      "Columns: ['lat', 'lon', 'year', 'yield_maize', 'temperature', 'precipitation', 'soil_water', 'solar_radiation', 'potential_evaporation', 'temperature_May', 'temperature_Jun', 'temperature_Jul'] ...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- paths ----------\n",
    "ERA5_DIR = Path(r\"..\\data\\climate_monthly_full\")   # folder with era5_land_monthly_YYYY.grib\n",
    "CORE_DIR = Path(r\"..\\italy_core_data\")\n",
    "DERIVED  = CORE_DIR / \"derived\"\n",
    "DERIVED.mkdir(exist_ok=True)\n",
    "\n",
    "base_csv = DERIVED / \"maize_ITnorth_core42_1982_2016_allstressors.csv\"  # seasonal file (already created)\n",
    "out_csv  = DERIVED / \"maize_ITnorth_core42_1982_2016_allstressors_with_monthly.csv\"\n",
    "\n",
    "# ---------- constants ----------\n",
    "years = list(range(1982, 2016 + 1))\n",
    "months = [5, 6, 7, 8, 9]  # May–Sep\n",
    "mon_name = {5: \"May\", 6: \"Jun\", 7: \"Jul\", 8: \"Aug\", 9: \"Sep\"}\n",
    "\n",
    "# ---------- 42-cell mask (values) for exact selection ----------\n",
    "m42 = pd.read_csv(DERIVED / \"mask_core_42_on_gdhy.csv\").sort_values([\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "latc, lonc = m42[\"lat\"].to_numpy(), m42[\"lon\"].to_numpy()\n",
    "\n",
    "# Small bbox so we only process Italy region\n",
    "BBOX = dict(\n",
    "    lat_min=float(latc.min() - 0.5),\n",
    "    lat_max=float(latc.max() + 0.5),\n",
    "    lon_min=float(lonc.min() - 0.5),\n",
    "    lon_max=float(lonc.max() + 0.5),\n",
    ")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_year_file(year: int) -> Path:\n",
    "    cand = ERA5_DIR / f\"era5_land_monthly_{year}.grib\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "    matches = list(ERA5_DIR.glob(f\"*{year}*.grib\"))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No GRIB for year {year} in {ERA5_DIR}\")\n",
    "    return matches[0]\n",
    "\n",
    "def open_era5_var_year(year: int, short: str, varname: str, bbox: dict) -> xr.DataArray:\n",
    "    f = find_year_file(year)\n",
    "    ds = xr.open_dataset(\n",
    "        f,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs=dict(indexpath=\"\", filter_by_keys={\"shortName\": short}),\n",
    "        decode_timedelta=True,\n",
    "    ).rename({\"latitude\":\"lat\",\"longitude\":\"lon\"})\n",
    "    # ensure ascending lat, 0..360 lon, and crop\n",
    "    if ds.lat[0] > ds.lat[-1]:\n",
    "        ds = ds.sortby(\"lat\")\n",
    "    if ds.lon.min() < 0:\n",
    "        ds = ds.assign_coords(lon=((ds.lon % 360 + 360) % 360)).sortby(\"lon\")\n",
    "    ds = ds.sel(lat=slice(bbox[\"lat_min\"], bbox[\"lat_max\"]),\n",
    "                lon=slice(bbox[\"lon_min\"], bbox[\"lon_max\"]))\n",
    "    da = ds[varname]\n",
    "    if \"expver\" in da.dims:\n",
    "        da = da.isel(expver=-1)\n",
    "    return da  # dims: time, lat, lon\n",
    "\n",
    "def bin_to_half_degree(da: xr.DataArray, step=0.5, offset=0.25) -> xr.DataArray:\n",
    "    \"\"\"Block-average 0.1° to 0.5°: label to nearest 0.25+0.5*k centers, then mean.\"\"\"\n",
    "    lat_bins = (offset + step * np.round((da[\"lat\"].values - offset) / step)).astype(np.float64)\n",
    "    lon_bins = (offset + step * np.round((da[\"lon\"].values - offset) / step)).astype(np.float64)\n",
    "    da = da.assign_coords(lat_bin=(\"lat\", lat_bins), lon_bin=(\"lon\", lon_bins))\n",
    "    da_c = da.groupby(\"lat_bin\").mean(\"lat\").groupby(\"lon_bin\").mean(\"lon\")\n",
    "    return da_c.rename({\"lat_bin\":\"lat\",\"lon_bin\":\"lon\"})  # dims: time, lat(0.5), lon(0.5)\n",
    "\n",
    "def monthly_means(da_m05: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"Return monthly means for MJJAS (state variables).\"\"\"\n",
    "    return da_m05.where(da_m05[\"time\"].dt.month.isin(months), drop=True)\n",
    "\n",
    "def monthly_totals_from_daily_means(da_m05: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"For ERA5 Monthly Averaged fluxes: daily mean × days_in_month -> monthly totals.\"\"\"\n",
    "    sel = da_m05.where(da_m05[\"time\"].dt.month.isin(months), drop=True)\n",
    "    days = xr.DataArray(sel[\"time\"].dt.days_in_month, coords={\"time\": sel[\"time\"]}, dims=[\"time\"]).astype(np.float64)\n",
    "    return sel * days\n",
    "\n",
    "def make_monthly_wide(short: str, xr_name: str, out_col: str, how: str, unit_conv=None):\n",
    "    \"\"\"\n",
    "    Build a wide dataframe with columns {out_col}_May … {out_col}_Sep.\n",
    "    how = 'mean' for state vars; 'total' for flux vars.\n",
    "    unit_conv is applied to the **monthly** field prior to means/totals (e.g., K->°C; m->mm and sign flip).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for y in years:\n",
    "        da = open_era5_var_year(y, short, xr_name, BBOX)\n",
    "        if unit_conv is not None:\n",
    "            da = unit_conv(da)\n",
    "        da05 = bin_to_half_degree(da)\n",
    "\n",
    "        if how == \"mean\":\n",
    "            dam = monthly_means(da05)                 # time (MJJAS), lat, lon\n",
    "        elif how == \"total\":\n",
    "            dam = monthly_totals_from_daily_means(da05)\n",
    "        else:\n",
    "            raise ValueError(\"how must be 'mean' or 'total'\")\n",
    "\n",
    "        # select the 42 cells by value (nearest to GDHY centers)\n",
    "        sub = dam.sel(lat=xr.DataArray(latc, dims=\"cell\"),\n",
    "                      lon=xr.DataArray(lonc, dims=\"cell\"),\n",
    "                      method=\"nearest\")\n",
    "        sub = sub.assign_coords(cell=(\"cell\", np.arange(len(latc))),\n",
    "                                lat=(\"cell\", latc), lon=(\"cell\", lonc))\n",
    "\n",
    "        df = sub.to_dataframe(name=out_col).reset_index()\n",
    "        df[\"year\"] = pd.to_datetime(df[\"time\"]).dt.year\n",
    "        df[\"month\"] = pd.to_datetime(df[\"time\"]).dt.month\n",
    "        df = df[df[\"month\"].isin(months)].copy()\n",
    "        df[\"month_name\"] = df[\"month\"].map(mon_name)\n",
    "        rows.append(df[[\"lat\",\"lon\",\"year\",\"month_name\",out_col]])\n",
    "\n",
    "        if (y - years[0]) % 5 == 0:\n",
    "            print(f\"{out_col}: processed {y}\", flush=True)\n",
    "\n",
    "    df_all = pd.concat(rows, ignore_index=True)\n",
    "    # pivot to wide: columns = month names\n",
    "    wide = df_all.pivot_table(index=[\"lat\",\"lon\",\"year\"], columns=\"month_name\", values=out_col, aggfunc=\"first\").reset_index()\n",
    "    # ensure month column order and rename with prefix\n",
    "    month_cols = [\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\"]\n",
    "    for m in month_cols:\n",
    "        if m not in wide.columns:\n",
    "            wide[m] = np.nan\n",
    "    wide = wide[[\"lat\",\"lon\",\"year\"] + month_cols]\n",
    "    wide = wide.rename(columns={m: f\"{out_col}_{m}\" for m in month_cols})\n",
    "    return wide\n",
    "\n",
    "# ---------- build monthly tables ----------\n",
    "# temperature: K -> °C, monthly means\n",
    "df_t2m_mon = make_monthly_wide(short=\"2t\", xr_name=\"t2m\", out_col=\"temperature\",\n",
    "                               how=\"mean\", unit_conv=lambda da: da - 273.15)\n",
    "\n",
    "# soil water: monthly means (m3/m3)\n",
    "df_swvl_mon = make_monthly_wide(short=\"swvl1\", xr_name=\"swvl1\", out_col=\"soil_water\",\n",
    "                                how=\"mean\", unit_conv=None)\n",
    "\n",
    "# precipitation: m -> mm, daily mean × days -> monthly totals\n",
    "df_tp_mon = make_monthly_wide(short=\"tp\", xr_name=\"tp\", out_col=\"precipitation\",\n",
    "                              how=\"total\", unit_conv=lambda da: da * 1000.0)\n",
    "\n",
    "# solar radiation: J/m² daily mean × days -> monthly totals (kept in J/m²)\n",
    "df_ssr_mon = make_monthly_wide(short=\"ssr\", xr_name=\"ssr\", out_col=\"solar_radiation\",\n",
    "                               how=\"total\", unit_conv=None)\n",
    "\n",
    "# potential evaporation: m -> **mm** and flip sign to positive, daily mean × days -> monthly totals\n",
    "df_pev_mon = make_monthly_wide(short=\"pev\", xr_name=\"pev\", out_col=\"potential_evaporation\",\n",
    "                               how=\"total\", unit_conv=lambda da: -da * 1000.0)\n",
    "\n",
    "# ---------- merge with seasonal base and save ----------\n",
    "base = pd.read_csv(base_csv)  # lat, lon, year, seasonal columns already present\n",
    "\n",
    "enriched = (\n",
    "    base.merge(df_t2m_mon, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_tp_mon,   on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_swvl_mon, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_ssr_mon,  on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .merge(df_pev_mon,  on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "        .sort_values([\"lat\",\"lon\",\"year\"])\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# sanity checks\n",
    "assert len(enriched) == 42*35, f\"Expected 1470 rows, got {len(enriched)}\"\n",
    "assert int(enriched.isna().sum().sum()) == 0, \"Found NaNs in enriched dataset\"\n",
    "\n",
    "enriched.to_csv(out_csv, index=False)\n",
    "print(f\"Saved enriched dataset with monthly columns → {out_csv}\")\n",
    "print(\"Columns:\", list(enriched.columns)[:12], \"...\")  # preview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
