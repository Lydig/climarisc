{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7dfde9-8d7a-457e-b995-0201930f645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === USER INPUTS ===\n",
    "\n",
    "# Project/crop tags\n",
    "crop          = \"maize\"                 # e.g., \"maize\", \"wheat_winter\", \"soybean\", ...\n",
    "region_tag    = \"ITnorth_core42\"        # used in filenames (keep as-is if using same 42-core cells)\n",
    "\n",
    "# Years and season\n",
    "start_year    = 1982\n",
    "end_year      = 2016                    # inclusive\n",
    "season_months = [5, 6, 7, 8, 9]         # MJJAS for Po Valley maize\n",
    "\n",
    "# Paths (edit to your structure)\n",
    "BASE_DIR      = r\"..\\italy_core_data\"\n",
    "DERIVED_DIR   = r\"..\\italy_core_data\\derived\"    # outputs will be written here\n",
    "MASK_CSV      = r\"..\\italy_core_data\\derived\\mask_core_42_on_gdhy.csv\"\n",
    "\n",
    "# GDHY yield files (per-year .nc4)\n",
    "GDHY_DIR      = r\"..\\data\\maize\"       # folder containing yield_YYYY.nc4 for the selected crop\n",
    "GDHY_FILE_TMPL= \"yield_{year}.nc4\"     # change if your files differ\n",
    "GDHY_VARNAME  = None                   # None -> auto-detect first data_var, or set e.g. \"var\"\n",
    "\n",
    "# ERA5-Land monthly averaged grib files (per-year)\n",
    "ERA5_DIR      = r\"..\\data\\climate_monthly_full\"\n",
    "ERA5_FILE_TMPL= \"era5_land_monthly_{year}.grib\"  # change if your files differ\n",
    "\n",
    "# Optional: convert SSR to MJ/m² in monthly/seasonal outputs? (keeps J/m² if False)\n",
    "SSR_TO_MJ     = False\n",
    "\n",
    "# === end user inputs ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd034c-0004-49c9-a9d9-5926043d0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Resolve paths\n",
    "DERIVED = Path(DERIVED_DIR); DERIVED.mkdir(exist_ok=True)\n",
    "MASK_CSV = Path(MASK_CSV)\n",
    "GDHY_DIR = Path(GDHY_DIR)\n",
    "ERA5_DIR = Path(ERA5_DIR)\n",
    "\n",
    "years = list(range(int(start_year), int(end_year) + 1))\n",
    "month_names = {1:\"Jan\",2:\"Feb\",3:\"Mar\",4:\"Apr\",5:\"May\",6:\"Jun\",7:\"Jul\",8:\"Aug\",9:\"Sep\",10:\"Oct\",11:\"Nov\",12:\"Dec\"}\n",
    "mon_order = [\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\"]\n",
    "\n",
    "# ---------- mask & bbox ----------\n",
    "m42 = (pd.read_csv(MASK_CSV).sort_values([\"lat\",\"lon\"]).reset_index(drop=True))\n",
    "latc, lonc = m42[\"lat\"].to_numpy(), m42[\"lon\"].to_numpy()\n",
    "ilat = m42[\"ilat\"].to_numpy() if \"ilat\" in m42.columns else None\n",
    "ilon = m42[\"ilon\"].to_numpy() if \"ilon\" in m42.columns else None\n",
    "\n",
    "# Small margin so binning catches edge cells\n",
    "BBOX = dict(\n",
    "    lat_min=float(latc.min() - 0.5),\n",
    "    lat_max=float(latc.max() + 0.5),\n",
    "    lon_min=float(lonc.min() - 0.5),\n",
    "    lon_max=float(lonc.max() + 0.5),\n",
    ")\n",
    "\n",
    "# ---------- file locators ----------\n",
    "def gdhyds_path(y:int)->Path:\n",
    "    p = GDHY_DIR / GDHY_FILE_TMPL.format(year=y)\n",
    "    if not p.exists():\n",
    "        # fallback: any file containing year\n",
    "        cands = list(GDHY_DIR.glob(f\"*{y}*.nc*\"))\n",
    "        if not cands:\n",
    "            raise FileNotFoundError(f\"GDHY file not found for {y} in {GDHY_DIR}\")\n",
    "        return cands[0]\n",
    "    return p\n",
    "\n",
    "def era5_path(y:int)->Path:\n",
    "    p = ERA5_DIR / ERA5_FILE_TMPL.format(year=y)\n",
    "    if not p.exists():\n",
    "        cands = list(ERA5_DIR.glob(f\"*{y}*.grib\"))\n",
    "        if not cands:\n",
    "            raise FileNotFoundError(f\"ERA5 file not found for {y} in {ERA5_DIR}\")\n",
    "        return cands[0]\n",
    "    return p\n",
    "\n",
    "# ---------- ERA5 open + crop ----------\n",
    "def open_era5_var_year(year:int, short:str, xr_name:str)->xr.DataArray:\n",
    "    f = era5_path(year)\n",
    "    ds = xr.open_dataset(\n",
    "        f,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs=dict(indexpath=\"\", filter_by_keys={\"shortName\": short}),\n",
    "        decode_timedelta=True,\n",
    "    ).rename({\"latitude\":\"lat\",\"longitude\":\"lon\"})\n",
    "    # ascending lat\n",
    "    if ds.lat[0] > ds.lat[-1]:\n",
    "        ds = ds.sortby(\"lat\")\n",
    "    # wrap lon to 0..360\n",
    "    if ds.lon.min() < 0:\n",
    "        ds = ds.assign_coords(lon=((ds.lon % 360 + 360) % 360)).sortby(\"lon\")\n",
    "    # crop to bbox\n",
    "    ds = ds.sel(lat=slice(BBOX[\"lat_min\"], BBOX[\"lat_max\"]),\n",
    "                lon=slice(BBOX[\"lon_min\"], BBOX[\"lon_max\"]))\n",
    "    da = ds[xr_name]\n",
    "    if \"expver\" in da.dims:\n",
    "        da = da.isel(expver=-1)\n",
    "    return da  # dims: time, lat, lon\n",
    "\n",
    "# ---------- 0.1° -> 0.5° block-average to GDHY centers (offset 0.25°) ----------\n",
    "def bin_to_half_degree(da: xr.DataArray, step=0.5, offset=0.25) -> xr.DataArray:\n",
    "    lat_bins = (offset + step * np.round((da[\"lat\"].values - offset) / step)).astype(np.float64)\n",
    "    lon_bins = (offset + step * np.round((da[\"lon\"].values - offset) / step)).astype(np.float64)\n",
    "    da = da.assign_coords(lat_bin=(\"lat\", lat_bins), lon_bin=(\"lon\", lon_bins))\n",
    "    da_c = da.groupby(\"lat_bin\").mean(\"lat\").groupby(\"lon_bin\").mean(\"lon\")\n",
    "    return da_c.rename({\"lat_bin\":\"lat\",\"lon_bin\":\"lon\"})\n",
    "\n",
    "# ---------- monthly reductions ----------\n",
    "def monthly_means(da_m05: xr.DataArray, months:list)->xr.DataArray:\n",
    "    return da_m05.where(da_m05[\"time\"].dt.month.isin(months), drop=True)\n",
    "\n",
    "def monthly_totals_from_daily_means(da_m05: xr.DataArray, months:list)->xr.DataArray:\n",
    "    sel = da_m05.where(da_m05[\"time\"].dt.month.isin(months), drop=True)\n",
    "    days = xr.DataArray(sel[\"time\"].dt.days_in_month, coords={\"time\": sel[\"time\"]}, dims=[\"time\"]).astype(np.float64)\n",
    "    return sel * days\n",
    "\n",
    "# ---------- seasonal reductions ----------\n",
    "def seasonal_mean(da_m05: xr.DataArray, months:list)->xr.DataArray:\n",
    "    return monthly_means(da_m05, months).groupby(\"time.year\").mean(\"time\").rename({\"year\":\"year\"})\n",
    "\n",
    "def seasonal_total_from_daily_means(da_m05: xr.DataArray, months:list)->xr.DataArray:\n",
    "    return monthly_totals_from_daily_means(da_m05, months).groupby(\"time.year\").sum(\"time\").rename({\"year\":\"year\"})\n",
    "\n",
    "# ---------- select exact 42 cells by lat/lon value (nearest) ----------\n",
    "def select_core_cells(da: xr.DataArray)->xr.DataArray:\n",
    "    out = da.sel(lat=xr.DataArray(latc, dims=\"cell\"),\n",
    "                 lon=xr.DataArray(lonc, dims=\"cell\"),\n",
    "                 method=\"nearest\")\n",
    "    out = out.assign_coords(cell=(\"cell\", np.arange(len(latc))),\n",
    "                            lat=(\"cell\", latc), lon=(\"cell\", lonc))\n",
    "    return out\n",
    "\n",
    "# ---------- build yield panel ----------\n",
    "def build_yield_panel()->pd.DataFrame:\n",
    "    rows = []\n",
    "    for y in years:\n",
    "        f = gdhyds_path(y)\n",
    "        ds = xr.open_dataset(f)\n",
    "        varname = GDHY_VARNAME or [v for v in ds.data_vars][0]\n",
    "        da = ds[varname].squeeze()\n",
    "        if set((\"lat\",\"lon\")).issubset(da.dims):\n",
    "            da = da.transpose(\"lat\",\"lon\")\n",
    "        else:\n",
    "            raise ValueError(f\"{f} missing lat/lon dims; got {da.dims}\")\n",
    "        arr = da.values\n",
    "        if ilat is None or ilon is None:\n",
    "            # Fallback: map coords to indices\n",
    "            lat_vals = da[\"lat\"].values; lon_vals = da[\"lon\"].values\n",
    "            def idx(val, axis_vals): \n",
    "                i = int(np.argmin(np.abs(axis_vals - val)))\n",
    "                assert np.isclose(axis_vals[i], val, atol=5e-4)\n",
    "                return i\n",
    "            i_lat = np.array([idx(v, lat_vals) for v in latc])\n",
    "            i_lon = np.array([idx(v, lon_vals) for v in lonc])\n",
    "        else:\n",
    "            i_lat, i_lon = ilat, ilon\n",
    "        vals = arr[i_lat, i_lon].astype(\"float32\")\n",
    "        rows.append(pd.DataFrame({\"year\": y, \"lat\": latc, \"lon\": lonc, \"yield_maize\": vals}))\n",
    "    out = pd.concat(rows, ignore_index=True).sort_values([\"lat\",\"lon\",\"year\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# ---------- process a single ERA5 variable to seasonal + monthly ----------\n",
    "def process_era5_var(short:str, xr_name:str, out_col:str, vtype:str):\n",
    "    \"\"\"\n",
    "    short/xr_name : GRIB shortName and xarray variable name (often same)\n",
    "    out_col       : output column base name\n",
    "    vtype         : 'state' (mean) or 'flux' (total from daily means)\n",
    "    Returns: (seasonal_df, monthly_wide_df)\n",
    "    \"\"\"\n",
    "    # collect monthly for all years\n",
    "    monthly_rows = []\n",
    "    seasonal_rows = []\n",
    "    for y in years:\n",
    "        da = open_era5_var_year(y, short, xr_name)       # regional 0.1°\n",
    "        # unit conversions (per variable)\n",
    "        if out_col == \"temperature\":\n",
    "            da = da - 273.15\n",
    "        if out_col == \"precipitation\":\n",
    "            da = da * 1000.0       # m -> mm (daily mean)\n",
    "        if out_col == \"potential_evaporation\":\n",
    "            da = -da * 1000.0      # flip sign, m -> mm (daily mean)\n",
    "        # block-average to 0.5°\n",
    "        da05 = bin_to_half_degree(da)\n",
    "        # monthly slice\n",
    "        if vtype == \"state\":\n",
    "            dam = monthly_means(da05, season_months)\n",
    "        elif vtype == \"flux\":\n",
    "            dam = monthly_totals_from_daily_means(da05, season_months)\n",
    "        else:\n",
    "            raise ValueError(\"vtype must be 'state' or 'flux'\")\n",
    "        # select 42 cells\n",
    "        subm = select_core_cells(dam)\n",
    "        dft = subm.to_dataframe(name=out_col).reset_index()\n",
    "        dft[\"year\"] = pd.to_datetime(dft[\"time\"]).dt.year\n",
    "        dft[\"month\"] = pd.to_datetime(dft[\"time\"]).dt.month\n",
    "        dft[\"month_name\"] = dft[\"month\"].map(month_names)\n",
    "        dft = dft[dft[\"month\"].isin(season_months)]\n",
    "        monthly_rows.append(dft[[\"lat\",\"lon\",\"year\",\"month\",\"month_name\",out_col]])\n",
    "        # seasonal\n",
    "        if vtype == \"state\":\n",
    "            ya = seasonal_mean(da05, season_months)\n",
    "        else:\n",
    "            ya = seasonal_total_from_daily_means(da05, season_months)\n",
    "        suby = select_core_cells(ya)\n",
    "        dfs = suby.to_dataframe(name=out_col).reset_index()[[\"lat\",\"lon\",\"year\",out_col]]\n",
    "        seasonal_rows.append(dfs)\n",
    "        # progress\n",
    "        if (y - years[0]) % 5 == 0:\n",
    "            print(f\"{out_col}: processed {y}\", flush=True)\n",
    "    # concat\n",
    "    monthly_long = pd.concat(monthly_rows, ignore_index=True)\n",
    "    seasonal_df  = pd.concat(seasonal_rows, ignore_index=True).sort_values([\"lat\",\"lon\",\"year\"]).reset_index(drop=True)\n",
    "    # pivot monthly to wide (May..Sep)\n",
    "    wide = monthly_long.pivot_table(index=[\"lat\",\"lon\",\"year\"], columns=\"month_name\", values=out_col, aggfunc=\"first\").reset_index()\n",
    "    for m in mon_order:\n",
    "        if m not in wide.columns: wide[m] = np.nan\n",
    "    wide = wide[[\"lat\",\"lon\",\"year\"] + mon_order]\n",
    "    wide = wide.rename(columns={m: f\"{out_col}_{m}\" for m in mon_order})\n",
    "    # optional SSR MJ/m²\n",
    "    if out_col == \"solar_radiation\" and SSR_TO_MJ:\n",
    "        # add converted copies with _MJm2 suffix (keep originals too)\n",
    "        for m in mon_order:\n",
    "            wide[f\"solar_radiation_{m}_MJm2\"] = wide[f\"solar_radiation_{m}\"] / 1e6\n",
    "        seasonal_df[\"solar_radiation_MJJAS_MJm2\"] = seasonal_df[\"solar_radiation\"] / 1e6\n",
    "    return seasonal_df, wide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf66916-1267-4aea-af2a-47f1c56ebade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) Build yield panel ----------\n",
    "yield_df = build_yield_panel()\n",
    "print(\"Yield panel:\", yield_df.shape, \"| years\", yield_df[\"year\"].min(), \"–\", yield_df[\"year\"].max())\n",
    "\n",
    "# ---------- 2) Process ERA5 variables ----------\n",
    "# Map of variables to process (shortName, xr_name, out_col, type)\n",
    "vars_cfg = [\n",
    "    (\"2t\",   \"t2m\",   \"temperature\",           \"state\"),\n",
    "    (\"tp\",   \"tp\",    \"precipitation\",         \"flux\"),\n",
    "    (\"swvl1\",\"swvl1\", \"soil_water\",            \"state\"),\n",
    "    (\"ssr\",  \"ssr\",   \"solar_radiation\",       \"flux\"),\n",
    "    (\"pev\",  \"pev\",   \"potential_evaporation\", \"flux\"),\n",
    "]\n",
    "\n",
    "seasonal_tables = []\n",
    "monthly_tables  = []\n",
    "\n",
    "for short, xr_name, out_col, vtype in vars_cfg:\n",
    "    seas, mon = process_era5_var(short, xr_name, out_col, vtype)\n",
    "    seasonal_tables.append(seas)\n",
    "    monthly_tables.append(mon)\n",
    "\n",
    "# ---------- 3) Merge seasonal into base ----------\n",
    "base = yield_df.copy()\n",
    "for tbl in seasonal_tables:\n",
    "    base = base.merge(tbl, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "\n",
    "# Basic checks\n",
    "assert len(base) == len(yield_df), \"Row count changed unexpectedly after seasonal merge\"\n",
    "assert base.isna().sum().sum() == 0, \"NaNs found after seasonal merge\"\n",
    "\n",
    "# ---------- 4) Save seasonal CSV ----------\n",
    "base_name = f\"{crop}_{region_tag}_{start_year}_{end_year}_allstressors.csv\"\n",
    "base_csv  = DERIVED / base_name\n",
    "base.to_csv(base_csv, index=False)\n",
    "print(\"Saved seasonal dataset →\", base_csv)\n",
    "\n",
    "# ---------- 5) Merge monthly wide tables ----------\n",
    "enriched = base.copy()\n",
    "for mon in monthly_tables:\n",
    "    enriched = enriched.merge(mon, on=[\"lat\",\"lon\",\"year\"], how=\"inner\")\n",
    "\n",
    "assert len(enriched) == len(base), \"Row count changed unexpectedly after monthly merge\"\n",
    "assert enriched.isna().sum().sum() == 0, \"NaNs found after monthly merge\"\n",
    "\n",
    "# ---------- 6) Save enriched CSV ----------\n",
    "enriched_name = f\"{crop}_{region_tag}_{start_year}_{end_year}_allstressors_with_monthly.csv\"\n",
    "enriched_csv  = DERIVED / enriched_name\n",
    "enriched.to_csv(enriched_csv, index=False)\n",
    "print(\"Saved enriched dataset with monthly columns →\", enriched_csv)\n",
    "\n",
    "# Preview\n",
    "print(\"Columns (first 16):\", list(enriched.columns)[:16], \"…\")\n",
    "print(\"Rows:\", len(enriched))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e108030-e074-4c74-9274-c447c61b9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity checks on ranges (prints only)\n",
    "cols_check = [\"yield_maize\",\"temperature\",\"precipitation\",\"soil_water\",\"solar_radiation\",\"potential_evaporation\"]\n",
    "print(\"Seasonal ranges:\")\n",
    "for c in cols_check:\n",
    "    lo, hi = enriched[c].min(), enriched[c].max()\n",
    "    print(f\"  {c:>22s}: min={lo:.3f}  max={hi:.3f}\")\n",
    "\n",
    "# Confirm May–Sep columns exist\n",
    "must_have = [f\"temperature_{m}\" for m in mon_order] + [f\"precipitation_{m}\" for m in mon_order]\n",
    "missing = [c for c in must_have if c not in enriched.columns]\n",
    "print(\"Monthly cols missing:\", missing)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
